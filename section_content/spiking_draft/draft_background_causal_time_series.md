<details><summary>â†ªExtended time-series causal inference intro:</summary>



<!-- NOTE: SECTION: bivariate time-lagged corr -->
Bivariate cross-correlation methods look at the correlation of time series collected from pairs of nodes at various lags and detect peaks at negative time lags. Such peaks could indicate the presence of a direct causal relationship -- but they could also stem from indirect causal links or hidden confounders [@dean2016dangers]. In these bivariate correlation methods, it is thus necessary to consider patterns of correlation between many pairs of nodes in order to differentiate between direct, indirect, and confounding relationships [@dean2016dangers]. This distinguishes these strategies from some multivariate methods that "control" for the effects of potential confounders. While cross-correlation-based measures are generally limited to detecting linear functional relationships between nodes, their computational feasibility makes them a frequent metric of choice in experimental neuroscience work [@knox1981detection; @salinas2001correlated; @garofalo2009evaluation].

<!-- NOTE:SECTION: info theory, conditional -->
<!-- NOTE: abbreviated, and notation remoded
see /section_content/spiking_background_causal_timeseries.md for original -->
Other techniques detect directional interaction stemming from more general or complex nonlinear relationships. Information-theoretic methods, which use information-based measures to assess the reduction in entropy knowledge of one variable provides about another, are closely related to Granger causality [@schreiber2000measuring; @barnett2009granger]. Transfer entropy extends this notion to potentially nonlinear, non-Gaussian time series by measuring the amount of information present in $Y_t$ that is not contained in the past of either $X$ or $Y$ [@bossomaier2016transfer]. Using transfer entropy as a measure of causal interaction requires accounting for potential confounding variables, which is accomplished through conditional transfer entropy. Conditional transfer entropy quantifies the information present in $Y_t$ that is not contained in the past of $X$, the past of $Y$, or the past of other, potentially confounding, variables $Z$.

To quantify the strength of causal interactions, information-theoretic and transfer-entropy-based methods typically require knowledge of the ground truth causal relationships that exist [@janzing2013quantifying] or an ability to perturb the system [@ay2008information; @lizier2010differentiating]. In practice, these quantities are typically interpreted as "information transfer," and a variety of estimation strategies and methods to automatically select the conditioning set (i.e., the variables and time lags that should be conditioned on) are used (e.g., [@shorten2021estimating]). Multivariate conditional transfer entropy approaches using various variable selection schemes can differentiate between direct interactions, indirect interactions, and common causes, but their results depend on choices such as the binning strategies used to discretize continuous signals, the specific statistical tests used, and the estimator used to compute transfer entropy [@wibral2014directed; @wollstadt2019idtxl].

</details>