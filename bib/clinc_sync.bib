
@misc{2021elephant,
  title = {Elephant - {{Electrophysiology Analysis Toolkit}}},
  year = {2021},
  month = oct,
  abstract = {Elephant is the Electrophysiology Analysis Toolkit},
  copyright = {BSD-3-Clause},
  howpublished = {NeuralEnsemble}
}

@misc{2022hcga,
  title = {Hcga: {{Highly}} Comparative Graph Analysis},
  shorttitle = {Hcga},
  year = {2022},
  month = feb,
  abstract = {Highly Comparative Graph Analysis - Code for network phenotyping},
  copyright = {GPL-3.0},
  howpublished = {Barahona Research - Applied Math - Imperial},
  keywords = {classification-algorithm,feature-extraction,graph-analysis-toolbox,graph-embedding,graph-features,interpretable-features,networks}
}

@article{adolphs2016human,
  title = {Human {{Lesion Studies}} in the 21st {{Century}}},
  author = {Adolphs, Ralph},
  year = {2016},
  month = jun,
  journal = {Neuron},
  volume = {90},
  number = {6},
  pages = {1151--1153},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.05.014},
  abstract = {The study of patients with brain lesions has made major historical contributions to cognitive neuroscience. Here I argue for an increased investment in modern lesion mapping, complementing fMRI studies and laying the conceptual and analytic foundations for future techniques that could experimentally manipulate human brain function.}
}

@article{alstott2009modeling,
  title = {Modeling the {{Impact}} of {{Lesions}} in the {{Human Brain}}},
  author = {Alstott, Jeffrey and Breakspear, Michael and Hagmann, Patric and Cammoun, Leila and Sporns, Olaf},
  year = {2009},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {5},
  number = {6},
  pages = {e1000408},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000408},
  abstract = {Lesions of anatomical brain networks result in functional disturbances of brain systems and behavior which depend sensitively, often unpredictably, on the lesion site. The availability of whole-brain maps of structural connections within the human cerebrum and our increased understanding of the physiology and large-scale dynamics of cortical networks allow us to investigate the functional consequences of focal brain lesions in a computational model. We simulate the dynamic effects of lesions placed in different regions of the cerebral cortex by recording changes in the pattern of endogenous (``resting-state'') neural activity. We find that lesions produce specific patterns of altered functional connectivity among distant regions of cortex, often affecting both cortical hemispheres. The magnitude of these dynamic effects depends on the lesion location and is partly predicted by structural network properties of the lesion site. In the model, lesions along the cortical midline and in the vicinity of the temporo-parietal junction result in large and widely distributed changes in functional connectivity, while lesions of primary sensory or motor regions remain more localized. The model suggests that dynamic lesion effects can be predicted on the basis of specific network measures of structural brain networks and that these effects may be related to known behavioral and cognitive consequences of brain lesions.},
  keywords = {+,Brain damage,Central nervous system,Centrality,Cingulate cortex,Frontal lobe,Lesions,Neural networks,Sensory perception}
}

@article{avery2018inferring,
  title = {Inferring Neural Circuit Properties from Optogenetic Stimulation},
  author = {Avery, Michael and Nassi, Jonathan and Reynolds, John},
  year = {2018},
  month = oct,
  journal = {PLOS ONE},
  volume = {13},
  number = {10},
  pages = {e0205386},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0205386},
  abstract = {Optogenetics has become an important tool for perturbing neural circuitry with unparalleled temporal precision and cell-type specificity. However, direct activation of a specific subpopulation of neurons can rapidly modulate the activity of other neurons within the network and may lead to unexpected and complex downstream effects. Here, we have developed a biologically-constrained computational model that exploits these non-intuitive network responses in order to gain insight into underlying properties of the network. We apply this model to data recorded during optogenetic stimulation in the primary visual cortex of the alert macaque. In these experiments, we found that optogenetic depolarization of excitatory neurons often suppressed neuronal responses, consistent with engagement of normalization circuitry. Our model suggests that the suppression seen in these responses may be mediated by slow excitatory and inhibitory conductance channels. Furthermore, the model predicted that the response of the network to optogenetic perturbation depends critically on the relationship between inherent temporal properties of the network and the temporal properties of the opsin. Consistent with model predictions, stimulation of the C1V1TT opsin, an opsin with a fast time constant (tau = 45 ms), caused faster and stronger suppressive effects after laser offset, as compared to stimulation of the slower C1V1T opsin (tau = 60ms). This work illustrates how the non-intuitive network responses that result from optogenetic stimulation can be exploited to gain insight regarding network properties that underlie fundamental neuronal computations, such as normalization. This novel hybrid opto-theoretical approach can thus enhance the power of optogenetics to dissect complex neural circuits.},
  keywords = {Functional electrical stimulation,Lasers,Macaque,Neural networks,Neural pathways,Neurons,Optogenetics,Synapses}
}

@misc{axionbiosystemsneural,
  title = {Neural {{Data Analysis}}},
  author = {Axion Biosystems}
}

@article{ay2008information,
  title = {Information Flows in Causal Networks},
  author = {Ay, Nihat and Polani, Daniel},
  year = {2008},
  month = feb,
  journal = {Advances in Complex Systems},
  volume = {11},
  number = {01},
  pages = {17--41},
  issn = {0219-5259, 1793-6802},
  doi = {10.1142/S0219525908001465},
  abstract = {We use a notion of causal independence based on intervention, which is a fundamental concept of the theory of causal networks, to define a measure for the strength of a causal effect. We call this measure "information flow" and compare it with known information flow measures such as transfer entropy.}
}

@article{barnett2009granger,
  title = {Granger {{Causality}} and {{Transfer Entropy Are Equivalent}} for {{Gaussian Variables}}},
  author = {Barnett, Lionel and Barrett, Adam B. and Seth, Anil K.},
  year = {2009},
  month = dec,
  journal = {Physical Review Letters},
  volume = {103},
  number = {23},
  pages = {238701},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.103.238701}
}

@article{barnett2014mvgc,
  title = {The {{MVGC}} Multivariate {{Granger}} Causality Toolbox: A New Approach to {{Granger-causal}} Inference},
  shorttitle = {The {{MVGC}} Multivariate {{Granger}} Causality Toolbox},
  author = {Barnett, Lionel and Seth, Anil K.},
  year = {2014},
  month = feb,
  journal = {Journal of Neuroscience Methods},
  volume = {223},
  pages = {50--68},
  issn = {1872-678X},
  doi = {10.1016/j.jneumeth.2013.10.018},
  abstract = {BACKGROUND: Wiener-Granger causality ("G-causality") is a statistical notion of causality applicable to time series data, whereby cause precedes, and helps predict, effect. It is defined in both time and frequency domains, and allows for the conditioning out of common causal influences. Originally developed in the context of econometric theory, it has since achieved broad application in the neurosciences and beyond. Prediction in the G-causality formalism is based on VAR (vector autoregressive) modelling. NEW METHOD: The MVGC Matlab\textcopyright{} Toolbox approach to G-causal inference is based on multiple equivalent representations of a VAR model by (i) regression parameters, (ii) the autocovariance sequence and (iii) the cross-power spectral density of the underlying process. It features a variety of algorithms for moving between these representations, enabling selection of the most suitable algorithms with regard to computational efficiency and numerical accuracy. RESULTS: In this paper we explain the theoretical basis, computational strategy and application to empirical G-causal inference of the MVGC Toolbox. We also show via numerical simulations the advantages of our Toolbox over previous methods in terms of computational accuracy and statistical inference. COMPARISON WITH EXISTING METHOD(S): The standard method of computing G-causality involves estimation of parameters for both a full and a nested (reduced) VAR model. The MVGC approach, by contrast, avoids explicit estimation of the reduced model, thus eliminating a source of estimation error and improving statistical power, and in addition facilitates fast and accurate estimation of the computationally awkward case of conditional G-causality in the frequency domain. CONCLUSIONS: The MVGC Toolbox implements a flexible, powerful and efficient approach to G-causal inference.},
  pmid = {24200508},
  keywords = {Algorithms,Computer Simulation,Granger causality,Humans,Models; Theoretical,Signal Processing; Computer-Assisted,Software,Time Factors,Time series analysis,Vector autoregressive modelling}
}

@article{barnett2018misunderstandings,
  title = {Misunderstandings Regarding the Application of {{Granger}} Causality in Neuroscience},
  author = {Barnett, Lionel and Barrett, Adam B. and Seth, Anil K.},
  year = {2018},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {29},
  pages = {E6676-E6677},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1714497115}
}

@article{bartolomeo2011quest,
  title = {The Quest for the `Critical Lesion Site' in Cognitive Deficits: {{Problems}} and Perspectives},
  shorttitle = {The Quest for the `Critical Lesion Site' in Cognitive Deficits},
  author = {Bartolomeo, Paolo},
  year = {2011},
  month = sep,
  journal = {Cortex},
  volume = {47},
  number = {8},
  pages = {1010--1012},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2010.11.007}
}

@incollection{battaglia2020functional,
  title = {Functional Connectivity and Neuronal Dynamics: Insights from Computational Methods},
  shorttitle = {Functional Connectivity and Neuronal Dynamics},
  booktitle = {The {{Cognitive Neurosciences}}, {{Sixth Edition}}},
  author = {Battaglia, Demian and Brovelli, Andrea},
  editor = {David Poeppel, George R. Mangun and Gazzaniga, Michael S.},
  year = {2020},
  month = may,
  abstract = {Brain functions rely on flexible communication between microcircuits in distinct cortical regions. The mechanisms underlying flexible information routing are still, however, largely unknown. Here, we hypothesize that the emergence of a multiplicity of possible information routing patterns is due to the richness of the complex dynamics that can be supported by an underlying structural network. Analyses of circuit computational models of interacting brain areas suggest that different dynamical states associated with a given connectome mechanistically implement different information routing patterns between system's components. As a result, a fast, network-wide and self-organized reconfiguration of information routing patterns-and Functional Connectivity networks, seen as their proxy-can be achieved by inducing a transition between the available intrinsic dynamical states. We present here a survey of theoretical and modelling results, as well as of sophisticated metrics of Functional Connectivity which are compliant with the daunting task of characterizing dynamic routing from neural data. Theory: Function follows dynamics, rather than structure Neuronal activity conveys information, but which target should this information be-pushed{$\Vert$} to, or which source should new information be-pulled{$\Vert$} from? The problem of dynamic information routing is ubiquitous in a distributed information processing system as the brain. Brain functions in general require the control of distributed networks of interregional communication on fast timescales compliant with behavior, but incompatible with plastic modifications of connectivity tracts (Bressler \& Kelso, 2001; Varela et al., 2001). This argument led to notions of connectivity based on information exchange-or more generically, an-interaction{$\Vert$}-between brain regions or neuronal populations, rather than based on the underlying STRUCTURAL CONNECTIVITY (SC, i.e. anatomic). An entire zoo of data-driven metrics has been introduced in the literature and this chapter will review some of them. Notwithstanding, they track simple correlation, or directed causal influence (Friston, 2011) or information transfer (Wibral et al., 2014) between time-series of activity. These}
}

@article{behrendt2019rtransferentropy,
  title = {{{RTransferEntropy}} \textemdash{} {{Quantifying}} Information Flow between Different Time Series Using Effective Transfer Entropy},
  author = {Behrendt, Simon and Dimpfl, Thomas and Peter, Franziska J. and Zimmermann, David J.},
  year = {2019},
  month = jul,
  journal = {SoftwareX},
  volume = {10},
  pages = {100265},
  issn = {2352-7110},
  doi = {10.1016/j.softx.2019.100265},
  abstract = {This paper shows how to quantify and test for the information flow between two time series with Shannon transfer entropy and R\'enyi transfer entropy using the R package RTransferEntropy. We discuss the methodology, the bias correction applied to calculate effective transfer entropy and outline how to conduct statistical inference. Furthermore, we describe the package in detail and demonstrate its functionality by means of several simulated processes and present an application to financial time series.},
  keywords = {Bootstrap inference,Effective transfer entropy,RÃ©nyi transfer entropy,Shannon transfer entropy}
}

@article{bolus2018design,
  title = {Design Strategies for Dynamic Closed-Loop Optogenetic Neurocontrol in Vivo},
  author = {Bolus, M. F. and Willats, A. A. and Whitmire, C. J. and Rozell, C. J. and Stanley, G. B.},
  year = {2018},
  month = apr,
  journal = {Journal of Neural Engineering},
  volume = {15},
  number = {2},
  pages = {026011},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/aaa506},
  abstract = {OBJECTIVE: Controlling neural activity enables the possibility of manipulating sensory perception, cognitive processes, and body movement, in addition to providing a powerful framework for functionally disentangling the neural circuits that underlie these complex phenomena. Over the last decade, optogenetic stimulation has become an increasingly important and powerful tool for understanding neural circuit function, owing to the ability to target specific cell types and bidirectionally modulate neural activity. To date, most stimulation has been provided in open-loop or in an on/off closed-loop fashion, where previously-determined stimulation is triggered by an event. Here, we describe and demonstrate a design approach for precise optogenetic control of neuronal firing rate modulation using feedback to guide stimulation continuously. APPROACH: Using the rodent somatosensory thalamus as an experimental testbed for realizing desired time-varying patterns of firing rate modulation, we utilized a moving average exponential filter to estimate firing rate online from single-unit spiking measured extracellularly. This estimate of instantaneous rate served as feedback for a proportional integral (PI) controller, which was designed during the experiment based on a linear-nonlinear Poisson (LNP) model of the neuronal response to light. MAIN RESULTS: The LNP model fit during the experiment enabled robust closed-loop control, resulting in good tracking of sinusoidal and non-sinusoidal targets, and rejection of unmeasured disturbances. Closed-loop control also enabled manipulation of trial-to-trial variability. SIGNIFICANCE: Because neuroscientists are faced with the challenge of dissecting the functions of circuit components, the ability to maintain control of a region of interest in spite of changes in ongoing neural activity will be important for disambiguating function within networks. Closed-loop stimulation strategies are ideal for control that is robust to such changes, and the employment of continuous feedback to adjust stimulation in real-time can improve the quality of data collected using optogenetic manipulation.},
  pmcid = {PMC5957547},
  pmid = {29300002},
  keywords = {Action Potentials,Animals,closed-loop,control,Female,firing rate,in vivo,Models; Neurological,optogenetics,Optogenetics,Poisson Distribution,Rats,Rats; Sprague-Dawley,Somatosensory Cortex,thalamus,Thalamus}
}

@article{bolus2021statespace,
  title = {State-Space Optimal Feedback Control of Optogenetically Driven Neural Activity},
  author = {Bolus, M. F. and Willats, A. A. and Rozell, C. J. and Stanley, G. B.},
  year = {2021},
  month = mar,
  journal = {Journal of Neural Engineering},
  volume = {18},
  number = {3},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/abb89c},
  abstract = {Objective.The rapid acceleration of tools for recording neuronal populations and targeted optogenetic manipulation has enabled real-time, feedback control of neuronal circuits in the brain. Continuously-graded control of measured neuronal activity poses a wide range of technical challenges, which we address through a combination of optogenetic stimulation and a state-space optimal control framework implemented in the thalamocortical circuit of the awake mouse.Approach.Closed-loop optogenetic control of neurons was performed in real-time via stimulation of channelrhodopsin-2 expressed in the somatosensory thalamus of the head-fixed mouse. A state-space linear dynamical system model structure was used to approximate the light-to-spiking input-output relationship in both single-neuron as well as multi-neuron scenarios when recording from multielectrode arrays. These models were utilized to design state feedback controller gains by way of linear quadratic optimal control and were also used online for estimation of state feedback, where a parameter-adaptive Kalman filter provided robustness to model-mismatch.Main results.This model-based control scheme proved effective for feedback control of single-neuron firing rate in the thalamus of awake animals. Notably, the graded optical actuation utilized here did not synchronize simultaneously recorded neurons, but heterogeneity across the neuronal population resulted in a varied response to stimulation. Simulated multi-output feedback control provided better control of a heterogeneous population and demonstrated how the approach generalizes beyond single-neuron applications.Significance.To our knowledge, this work represents the first experimental application of state space model-based feedback control for optogenetic stimulation. In combination with linear quadratic optimal control, the approaches laid out and tested here should generalize to future problems involving the control of highly complex neural circuits. More generally, feedback control of neuronal circuits opens the door to adaptively interacting with the dynamics underlying sensory, motor, and cognitive signaling, enabling a deeper understanding of circuit function and ultimately the control of function in the face of injury or disease.},
  pmcid = {PMC8356067},
  pmid = {32932241},
  keywords = {Animals,Channelrhodopsins,closed-loop,control,estimation,Feedback,firing rate,in vivo,Mice,Neurons,optogenetics,Optogenetics,state space,thalamus,Thalamus}
}

@inbook{bossomaier2016transfer,
  title = {Transfer {{Entropy}}},
  booktitle = {An {{Introduction}} to {{Transfer Entropy}}},
  author = {Bossomaier, Terry and Barnett, Lionel and Harr{\'e}, Michael and Lizier, Joseph T.},
  year = {2016},
  pages = {65--95},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-43222-9_4},
  collaborator = {Bossomaier, Terry and Barnett, Lionel and Harr{\'e}, Michael and Lizier, Joseph T.},
  isbn = {978-3-319-43221-2 978-3-319-43222-9}
}

@article{braganza2018circuit,
  title = {The {{Circuit Motif}} as a {{Conceptual Tool}} for {{Multilevel Neuroscience}}},
  author = {Braganza, Oliver and Beck, Heinz},
  year = {2018},
  month = mar,
  journal = {Trends in Neurosciences},
  volume = {41},
  number = {3},
  pages = {128--136},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2018.01.002},
  abstract = {Modern neuroscientific techniques that specifically manipulate and measure neuronal activity in behaving animals now allow bridging of the gap from the cellular to the behavioral level. However, in doing so, they also pose new challenges. Research using incompletely defined manipulations in a high-dimensional space without clear hypotheses is likely to suffer from multiple well-known conceptual and statistical problems. In this context it is essential to develop hypotheses with testable implications across levels. Here we propose that a focus on circuit motifs can help achieve this goal. Viewing neural structures as an assembly of circuit motif building blocks is not new. However, recent tool advances have made it possible to extensively map, specifically manipulate, and quantitatively investigate circuit motifs and thereby reexamine their relevance to brain function.},
  keywords = {behavior,circuit motif,high-dimensional research,inhibition,multilevel neuroscience,optogenetics}
}

@article{cadotte2008causal,
  title = {Causal {{Measures}} of {{Structure}} and {{Plasticity}} in {{Simulated}} and {{Living Neural Networks}}},
  author = {Cadotte, Alex J. and DeMarse, Thomas B. and He, Ping and Ding, Mingzhou},
  year = {2008},
  month = oct,
  journal = {PLOS ONE},
  volume = {3},
  number = {10},
  pages = {e3355},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0003355},
  abstract = {A major goal of neuroscience is to understand the relationship between neural structures and their function. Recording of neural activity with arrays of electrodes is a primary tool employed toward this goal. However, the relationships among the neural activity recorded by these arrays are often highly complex making it problematic to accurately quantify a network's structural information and then relate that structure to its function. Current statistical methods including cross correlation and coherence have achieved only modest success in characterizing the structural connectivity. Over the last decade an alternative technique known as Granger causality is emerging within neuroscience. This technique, borrowed from the field of economics, provides a strong mathematical foundation based on linear auto-regression to detect and quantify ``causal'' relationships among different time series. This paper presents a combination of three Granger based analytical methods that can quickly provide a relatively complete representation of the causal structure within a neural network. These are a simple pairwise Granger causality metric, a conditional metric, and a little known computationally inexpensive subtractive conditional method. Each causal metric is first described and evaluated in a series of biologically plausible neural simulations. We then demonstrate how Granger causality can detect and quantify changes in the strength of those relationships during plasticity using 60 channel spike train data from an in vitro cortical network measured on a microelectrode array. We show that these metrics can not only detect the presence of causal relationships, they also provide crucial information about the strength and direction of that relationship, particularly when that relationship maybe changing during plasticity. Although we focus on the analysis of multichannel spike train data the metrics we describe are applicable to any stationary time series in which causal relationships among multiple measures is desired. These techniques can be especially useful when the interactions among those measures are highly complex, difficult to untangle, and maybe changing over time.},
  keywords = {Action potentials,Electrode potentials,Functional electrical stimulation,Monte Carlo method,Neural networks,Neuronal plasticity,Neurons,Synapses}
}

@article{candadai2020infotheory,
  title = {Infotheory: {{A C}}++/{{Python}} Package for Multivariate Information Theoretic Analysis},
  shorttitle = {Infotheory},
  author = {Candadai, Madhavun and Izquierdo, Eduardo J.},
  year = {2020},
  month = mar,
  journal = {Journal of Open Source Software},
  volume = {5},
  number = {47},
  eprint = {1907.02339},
  eprinttype = {arxiv},
  pages = {1609},
  issn = {2475-9066},
  doi = {10.21105/joss.01609},
  abstract = {This paper introduces \textbackslash texttt\{infotheory\}: a package written in C++ and usable from Python and C++, for multivariate information theoretic analyses of discrete and continuous data. This package allows the user to study the relationship between components of a complex system simply from the data recorded during its operation, using the tools of information theory. It implements widely used measures such as entropy and mutual information, as well as more recent measures that arise from multivariate extensions to information theory, specifically Partial Information Decomposition. It provides an easy-to-use and flexible tool for use in research as well as pedgogical purposes to introduce students to information theory. Website: http://mcandadai.com/infotheory/ Source: https://git.io/infot},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Quantitative Biology - Neurons and Cognition}
}

@article{candadai2021information,
  title = {Information Theoretic Analysis of Computational Models as a Tool to Understand the Neural Basis of Behaviors},
  author = {Candadai, Madhavun},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.05186 [cs, math, q-bio]},
  eprint = {2106.05186},
  eprinttype = {arxiv},
  primaryclass = {cs, math, q-bio},
  abstract = {One of the greatest research challenges of this century is to understand the neural basis for how behavior emerges in brain-body-environment systems. To this end, research has flourished along several directions but have predominantly focused on the brain. While there is in an increasing acceptance and focus on including the body and environment in studying the neural basis of behavior, animal researchers are often limited by technology or tools. Computational models provide an alternative framework within which one can study model systems where ground-truth can be measured and interfered with. These models act as a hypothesis generation framework that would in turn guide experimentation. Furthermore, the ability to intervene as we please, allows us to conduct in-depth analysis of these models in a way that cannot be performed in natural systems. For this purpose, information theory is emerging as a powerful tool that can provide insights into the operation of these brain-body-environment models. In this work, I provide an introduction, a review and discussion to make a case for how information theoretic analysis of computational models is a potent research methodology to help us better understand the neural basis of behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition}
}

@misc{causal,
  title = {Causal {{Discovery Toolbox Documentation}} \textemdash{} {{Causal Discovery Toolbox}} 0.5.23 Documentation},
  howpublished = {https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html}
}

@article{chicharro2012when,
  title = {When {{Two Become One}}: {{The Limits}} of {{Causality Analysis}} of {{Brain Dynamics}}},
  shorttitle = {When {{Two Become One}}},
  author = {Chicharro, Daniel and Ledberg, Anders},
  editor = {Wennekers, Thomas},
  year = {2012},
  month = mar,
  journal = {PLoS ONE},
  volume = {7},
  number = {3},
  pages = {e32466},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0032466}
}

@article{chis2011structural,
  title = {Structural {{Identifiability}} of {{Systems Biology Models}}: {{A Critical Comparison}} of {{Methods}}},
  shorttitle = {Structural {{Identifiability}} of {{Systems Biology Models}}},
  author = {Chis, Oana-Teodora and Banga, Julio R. and {Balsa-Canto}, Eva},
  editor = {Jaeger, Johannes},
  year = {2011},
  month = nov,
  journal = {PLoS ONE},
  volume = {6},
  number = {11},
  pages = {e27755},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0027755}
}

@article{churchland2010stimulus,
  title = {Stimulus Onset Quenches Neural Variability: A Widespread Cortical Phenomenon},
  shorttitle = {Stimulus Onset Quenches Neural Variability},
  author = {Churchland, Mark M. and Yu, Byron M. and Cunningham, John P. and Sugrue, Leo P. and Cohen, Marlene R. and Corrado, Greg S. and Newsome, William T. and Clark, Andrew M. and Hosseini, Paymon and Scott, Benjamin B. and Bradley, David C. and Smith, Matthew A. and Kohn, Adam and Movshon, J. Anthony and Armstrong, Katherine M. and Moore, Tirin and Chang, Steve W. and Snyder, Lawrence H. and Lisberger, Stephen G. and Priebe, Nicholas J. and Finn, Ian M. and Ferster, David and Ryu, Stephen I. and Santhanam, Gopal and Sahani, Maneesh and Shenoy, Krishna V.},
  year = {2010},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {13},
  number = {3},
  pages = {369--378},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2501},
  abstract = {The authors measured the variability of neuronal responses across a large number of datasets and cortical areas. They found that variability decreased in response to all stimuli tested, whether the animal was awake, behaving or anesthetized, suggesting that the stabilization of cortex in response to an input is a general cortical property.},
  copyright = {2010 Nature Publishing Group},
  keywords = {Network models,Neuronal physiology}
}

@article{ciba2018spikecontrast,
  title = {Spike-Contrast: {{A}} Novel Time Scale Independent and Multivariate Measure of Spike Train Synchrony},
  shorttitle = {Spike-Contrast},
  author = {Ciba, Manuel and Isomura, Takuya and Jimbo, Yasuhiko and Bahmer, Andreas and Thielemann, Christiane},
  year = {2018},
  month = jan,
  journal = {Journal of Neuroscience Methods},
  volume = {293},
  pages = {136--143},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2017.09.008}
}

@article{cliff2021assessing,
  title = {Assessing the {{Significance}} of {{Directed}} and {{Multivariate Measures}} of {{Linear Dependence Between Time Series}}},
  author = {Cliff, Oliver M. and Novelli, Leonardo and Fulcher, Ben D. and Shine, James M. and Lizier, Joseph T.},
  year = {2021},
  month = feb,
  journal = {Physical Review Research},
  volume = {3},
  number = {1},
  eprint = {2003.03887},
  eprinttype = {arxiv},
  pages = {013145},
  issn = {2643-1564},
  doi = {10.1103/PhysRevResearch.3.013145},
  abstract = {Inferring linear dependence between time series is central to our understanding of natural and artificial systems. Unfortunately, the hypothesis tests that are used to determine statistically significant directed or multivariate relationships from time-series data often yield spurious associations (Type I errors) or omit causal relationships (Type II errors). This is due to the autocorrelation present in the analysed time series -- a property that is ubiquitous across diverse applications, from brain dynamics to climate change. Here we show that, for limited data, this issue cannot be mediated by fitting a time-series model alone (e.g., in Granger causality or prewhitening approaches), and instead that the degrees of freedom in statistical tests should be altered to account for the effective sample size induced by cross-correlations in the observations. This insight enabled us to derive modified hypothesis tests for any multivariate correlation-based measures of linear dependence between covariance-stationary time series, including Granger causality and mutual information with Gaussian marginals. We use both numerical simulations (generated by autoregressive models and digital filtering) as well as recorded fMRI-neuroimaging data to show that our tests are unbiased for a variety of stationary time series. Our experiments demonstrate that the commonly used \$F\$- and \$\textbackslash chi\^2\$-tests can induce significant false-positive rates of up to \$100\textbackslash\%\$ for both measures, with and without prewhitening of the signals. These findings suggest that many dependencies reported in the scientific literature may have been, and may continue to be, spuriously reported or missed if modified hypothesis tests are not used when analysing time series.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Mathematics - Statistics Theory,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Neurons and Cognition,Statistics - Applications,Statistics - Methodology}
}

@misc{cliff2022assessing,
  title = {Assessing the Significance of Directed and Multivariate Dependence Measures},
  author = {Cliff, Oliver},
  year = {2022},
  month = feb,
  abstract = {A package to compute and test the significance of linear dependence between multiple autocorrelated time series.},
  copyright = {GPL-3.0},
  keywords = {autocorrelation,granger-causality,mutual-information,time-series-analysis}
}

@misc{cliff2022python,
  title = {Python {{Toolkit}} of {{Statistics}} for {{Pairwise Interactions}} (Pyspi)},
  author = {Cliff, Oliver},
  year = {2022},
  month = feb,
  abstract = {Comparative analysis of pairwise interactions in multivariate time series.},
  copyright = {GPL-3.0},
  keywords = {comparative-analysis,pairwise-interactions,time-series}
}

@article{cliff2022unifying,
  title = {Unifying {{Pairwise Interactions}} in {{Complex Dynamics}}},
  author = {Cliff, Oliver M. and Lizier, Joseph T. and Tsuchiya, Naotsugu and Fulcher, Ben D.},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.11941 [physics]},
  eprint = {2201.11941},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Scientists have developed hundreds of techniques to measure the interactions between pairs of processes in complex systems. But these computational methods -- from correlation coefficients to causal inference -- rely on distinct quantitative theories that remain largely disconnected. Here we introduce a library of 249 statistics for pairwise interactions and assess their behavior on 1053 multivariate time series from a wide range of real-world and model-generated systems. Our analysis highlights new commonalities between different mathematical formulations, providing a unified picture of a rich, interdisciplinary literature. We then show that leveraging many methods from across science can uncover those most suitable for addressing a given problem, yielding high accuracy and interpretable understanding. Our framework is provided in extendable open software, enabling comprehensive data-driven analysis by integrating decades of methodological advances.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability}
}

@article{cole1949dynamic,
  title = {Dynamic Electrical Characteristics of the Squid Axon Membrane},
  author = {Cole, K.S},
  year = {1949},
  journal = {Annual review of physiology},
  number = {3},
  pages = {253--258},
  biburl = {https://www.bibsonomy.org/bibtex/2803ebab4cfc02db3a7cf4aa7b5c2ae28/fdiehl},
  rating = {0},
  uri = {papers://7B65697B-E216-4648-8A41-C67830C0DC73/Paper/p19865}
}

@article{curto2019relating,
  title = {Relating Network Connectivity to Dynamics: Opportunities and Challenges for Theoretical Neuroscience},
  shorttitle = {Relating Network Connectivity to Dynamics},
  author = {Curto, Carina and Morrison, Katherine},
  year = {2019},
  month = oct,
  journal = {Current Opinion in Neurobiology},
  series = {Computational {{Neuroscience}}},
  volume = {58},
  pages = {11--20},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2019.06.003},
  abstract = {We review recent work relating network connectivity to the dynamics of neural activity. While concepts stemming from network science provide a valuable starting point, the interpretation of graph-theoretic structures and measures can be highly dependent on the dynamics associated to the network. Properties that are quite meaningful for linear dynamics, such as random walk and network flow models, may be of limited relevance in the neuroscience setting. Theoretical and computational neuroscience are playing a vital role in understanding the relationship between network connectivity and the nonlinear dynamics associated to neural networks.}
}

@article{cutts2014detecting,
  title = {Detecting {{Pairwise Correlations}} in {{Spike Trains}}: {{An Objective Comparison}} of {{Methods}} and {{Application}} to the {{Study}} of {{Retinal Waves}}},
  shorttitle = {Detecting {{Pairwise Correlations}} in {{Spike Trains}}},
  author = {Cutts, Catherine S. and Eglen, Stephen J.},
  year = {2014},
  month = oct,
  journal = {Journal of Neuroscience},
  volume = {34},
  number = {43},
  pages = {14288--14303},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2767-14.2014},
  abstract = {Correlations in neuronal spike times are thought to be key to processing in many neural systems. Many measures have been proposed to summarize these correlations and of these the correlation index is widely used and is the standard in studies of spontaneous retinal activity. We show that this measure has two undesirable properties: it is unbounded above and confounded by firing rate. We list properties needed for a measure to fairly quantify and compare correlations and we propose a novel measure of correlation\textemdash the spike time tiling coefficient. This coefficient, the correlation index, and 33 other measures of correlation of spike times are blindly tested for the required properties on synthetic and experimental data. Based on this, we propose a measure (the spike time tiling coefficient) to replace the correlation index. To demonstrate the benefits of this measure, we reanalyze data from seven key studies, which previously used the correlation index to investigate the nature of spontaneous activity. We reanalyze data from {$\beta$}2(KO) and {$\beta$}2(TG) mutants, mutants lacking connexin isoforms, and also the age-dependent changes in wild-type and {$\beta$}2(KO) correlations. Reanalysis of the data using the proposed measure can significantly change the conclusions. It leads to better quantification of correlations and therefore better inference from the data. We hope that the proposed measure will have wide applications, and will help clarify the role of activity in retinotopic map formation.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2014 Cutts and Eglen. This article is freely available online through the J Neurosci Author Open Choice option.},
  pmid = {25339742},
  keywords = {activity,correlations,development,retina,retinotopic map,spike times}
}

@article{damasio1989lesion,
  title = {Lesion {{Analysis}}},
  author = {DAMASIO, H.},
  year = {1989},
  journal = {Neuropsychology.},
  publisher = {{Oxford University Press}}
}

@article{das2020systematic,
  title = {Systematic Errors in Connectivity Inferred from Activity in Strongly Recurrent Networks},
  author = {Das, Abhranil and Fiete, Ila R.},
  year = {2020},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {23},
  number = {10},
  pages = {1286--1296},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-020-0699-2},
  abstract = {Understanding the mechanisms of neural computation and learning will require knowledge of the underlying circuitry. Because it is difficult to directly measure the wiring diagrams of neural circuits, there has long been an interest in estimating them algorithmically from multicell activity recordings. We show that even sophisticated methods, applied to unlimited data from every cell in the circuit, are biased toward inferring connections between unconnected but highly correlated neurons. This failure to `explain away' connections occurs when there is a mismatch between the true network dynamics and the model used for inference, which is inevitable when modeling the real world. Thus, causal inference suffers when variables are highly correlated, and activity-based estimates of connectivity should be treated with special caution in strongly connected networks. Finally, performing inference on the activity of circuits pushed far out of equilibrium by a simple low-dimensional suppressive drive might ameliorate inference bias.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  keywords = {++,Computational neuroscience,cross-correlation,Network models,Neural circuits,parameter sweep,xcorr},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Network models;Neural circuits Subject\_term\_id: computational-neuroscience;network-models;neural-circuit}
}

@article{davies-thompson2019hierarchical,
  title = {Hierarchical {{Brain Network}} for {{Face}} and {{Voice Integration}} of {{Emotion Expression}}},
  author = {{Davies-Thompson}, Jodie and Elli, Giulia V and Rezk, Mohamed and Benetti, Stefania and {van Ackeren}, Markus and Collignon, Olivier},
  year = {2019},
  month = aug,
  journal = {Cerebral Cortex},
  volume = {29},
  number = {9},
  pages = {3590--3605},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhy240},
  abstract = {The brain has separate specialized computational units to process faces and voices located in occipital and temporal cortices. However, humans seamlessly integrate signals from the faces and voices of others for optimal social interaction. How are emotional expressions, when delivered by different sensory modalities (faces and voices), integrated in the brain? In this study, we characterized the brains' response to faces, voices, and combined face\textendash voice information (congruent, incongruent), which varied in expression (neutral, fearful). Using a whole-brain approach, we found that only the right posterior superior temporal sulcus (rpSTS) responded more to bimodal stimuli than to face or voice alone but only when the stimuli contained emotional expression. Face- and voice-selective regions of interest, extracted from independent functional localizers, similarly revealed multisensory integration in the face-selective rpSTS only; further, this was the only face-selective region that also responded significantly to voices. Dynamic causal modeling revealed that the rpSTS receives unidirectional information from the face-selective fusiform face area, and voice-selective temporal voice area, with emotional expression affecting the connection strength. Our study promotes a hierarchical model of face and voice integration, with convergence in the rpSTS, and that such integration depends on the (emotional) salience of the stimuli.}
}

@article{dean2016dangers,
  title = {Dangers and Uses of Cross-Correlation in Analyzing Time Series in Perception, Performance, Movement, and Neuroscience: {{The}} Importance of Constructing Transfer Function Autoregressive Models},
  shorttitle = {Dangers and Uses of Cross-Correlation in Analyzing Time Series in Perception, Performance, Movement, and Neuroscience},
  author = {Dean, Roger T. and Dunsmuir, William T. M.},
  year = {2016},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {48},
  number = {2},
  pages = {783--802},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0611-2},
  abstract = {Many articles on perception, performance, psychophysiology, and neuroscience seek to relate pairs of time series through assessments of their cross-correlations. Most such series are individually autocorrelated: they do not comprise independent values. Given this situation, an unfounded reliance is often placed on cross-correlation as an indicator of relationships (e.g., referent vs. response, leading vs. following). Such cross-correlations can indicate spurious relationships, because of autocorrelation. Given these dangers, we here simulated how and why such spurious conclusions can arise, to provide an approach to resolving them. We show that when multiple pairs of series are aggregated in several different ways for a cross-correlation analysis, problems remain. Finally, even a genuine cross-correlation function does not answer key motivating questions, such as whether there are likely causal relationships between the series. Thus, we illustrate how to obtain a transfer function describing such relationships, informed by any genuine cross-correlations. We illustrate the confounds and the meaningful transfer functions by two concrete examples, one each in perception and performance, together with key elements of the R software code needed. The approach involves autocorrelation functions, the establishment of stationarity, prewhitening, the determination of cross-correlation functions, the assessment of Granger causality, and autoregressive model development. Autocorrelation also limits the interpretability of other measures of possible relationships between pairs of time series, such as mutual information. We emphasize that further complexity may be required as the appropriate analysis is pursued fully, and that causal intervention experiments will likely also be needed.}
}

@article{deblasi2019total,
  title = {Total Spiking Probability Edges: {{A}} Cross-Correlation Based Method for Effective Connectivity Estimation of Cortical Spiking Neurons},
  shorttitle = {Total Spiking Probability Edges},
  author = {De Blasi, Stefano and Ciba, Manuel and Bahmer, Andreas and Thielemann, Christiane},
  year = {2019},
  month = jan,
  journal = {Journal of Neuroscience Methods},
  volume = {312},
  pages = {169--181},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2018.11.013},
  abstract = {Background Connectivity is a relevant parameter for the information flow within neuronal networks. Network connectivity can be reconstructed from recorded spike train data. Various methods have been developed to estimate connectivity from spike trains. New method In this work, a novel effective connectivity estimation algorithm called Total Spiking Probability Edges (TSPE) is proposed and evaluated. First, a cross-correlation between pairs of spike trains is calculated. Second, to distinguish between excitatory and inhibitory connections, edge filters are applied on the resulting cross-correlogram. Results TSPE was evaluated with large scale in silico networks and enables almost perfect reconstructions (true positive rate of approx. 99\% at a false positive rate of 1\% for low density random networks) depending on the network topology and the spike train duration. A distinction between excitatory and inhibitory connections was possible. TSPE is computational effective and takes less than 3\,min on a high-performance computer to estimate the connectivity of an 1\,h dataset of 1000 spike trains. Comparison of existing methods TSPE was compared with connectivity estimation algorithms like Transfer Entropy based methods, Filtered and Normalized Cross-Correlation Histogram and Normalized Cross-Correlation. In all test cases, TSPE outperformed the compared methods in the connectivity reconstruction accuracy. Conclusions The results show that the accuracy of functional connectivity estimation of large scale neuronal networks has been enhanced by TSPE compared to state of the art methods. Furthermore, TSPE enables the classification of excitatory and inhibitory synaptic effects.},
  keywords = {Classification,Connectivity estimation,Inhibitory and excitatory,MATLAB,Neuronal networks,Parallel spike trains,Toolbox}
}

@article{delapavapanche2019datadriven,
  title = {A {{Data-Driven Measure}} of {{Effective Connectivity Based}} on {{Renyi}}'s {$\alpha$}-{{Entropy}}},
  author = {De La Pava Panche, Ivan and {Alvarez-Meza}, Andres M. and {Orozco-Gutierrez}, Alvaro},
  year = {2019},
  journal = {Frontiers in Neuroscience},
  volume = {13},
  issn = {1662-453X},
  abstract = {Transfer entropy (TE) is a model-free effective connectivity measure based on information theory. It has been increasingly used in neuroscience because of its ability to detect unknown non-linear interactions, which makes it well suited for exploratory brain effective connectivity analyses. Like all information theoretic quantities, TE is defined regarding the probability distributions of the system under study, which in practice are unknown and must be estimated from data. Commonly used methods for TE estimation rely on a local approximation of the probability distributions from nearest neighbor distances, or on symbolization schemes that then allow the probabilities to be estimated from the symbols' relative frequencies. However, probability estimation is a challenging problem, and avoiding this intermediate step in TE computation is desirable. In this work, we propose a novel TE estimator using functionals defined on positive definite and infinitely divisible kernels matrices that approximate Renyi's entropy measures of order {$\alpha$}. Our data-driven approach estimates TE directly from data, sidestepping the need for probability distribution estimation. Also, the proposed estimator encompasses the well-known definition of TE as a sum of Shannon entropies in the limiting case when {$\alpha$} \textrightarrow{} 1. We tested our proposal on a simulation framework consisting of two linear models, based on autoregressive approaches and a linear coupling function, respectively, and on the public electroencephalogram (EEG) database BCI Competition IV, obtained under a motor imagery paradigm. For the synthetic data, the proposed kernel-based TE estimation method satisfactorily identifies the causal interactions present in the data. Also, it displays robustness to varying noise levels and data sizes, and to the presence of multiple interaction delays in the same connected network. Obtained results for the motor imagery task show that our approach codes discriminant spatiotemporal patterns for the left and right-hand motor imagination tasks, with classification performances that compare favorably to the state-of-the-art.}
}

@article{deweese2004shared,
  title = {Shared and {{Private Variability}} in the {{Auditory Cortex}}},
  author = {Deweese, Michael R. and Zador, Anthony M.},
  year = {2004},
  month = sep,
  journal = {Journal of Neurophysiology},
  volume = {92},
  number = {3},
  pages = {1840--1855},
  publisher = {{American Physiological Society}},
  issn = {0022-3077},
  doi = {10.1152/jn.00197.2004},
  abstract = {The high variability of cortical sensory responses is often assumed to impose a major constraint on efficient computation. In the auditory cortex, however, response variability can be very low. We have used in vivo whole cell patch-clamp methods to study the trial-to-trial variability of the subthreshold fluctuations in membrane potential underlying tone-evoked responses in the auditory cortex of anesthetized rats. Using methods adapted from classical quantal analysis, we partitioned this subthreshold variability into a private component (which includes synaptic, thermal, and other sources local to the recorded cell) and a shared component arising from network interactions. Here we report that this private component is remarkably small, usually about 1\textendash 3 mV, as quantified by the variance divided by the mean of the ensemble of tone-evoked response heights. The shared component can be much larger, and shows more heterogeneity across the population, ranging from about 0 to 10 mV. The remarkable fact that, at least 5 synapses from the auditory periphery, this variability remains so small raises the possibility that the intervening neural circuitry is organized so as to prevent private noise from accumulating as neural signals propagate to the cortex.}
}

@book{durbin1998biological,
  title = {Biological {{Sequence Analysis}}: {{Probabilistic Models}} of {{Proteins}} and {{Nucleic Acids}}},
  shorttitle = {Biological {{Sequence Analysis}}},
  author = {Durbin, Richard and Eddy, Sean R. and Krogh, Anders and Mitchison, Graeme},
  year = {1998},
  month = apr,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511790492},
  isbn = {978-0-521-62041-3 978-0-521-62971-3 978-0-511-79049-2}
}

@article{eberhardt2007interventions,
  title = {Interventions and {{Causal Inference}}},
  author = {Eberhardt, Frederick and Scheines, Richard},
  year = {2007},
  month = dec,
  journal = {Philosophy of Science},
  volume = {74},
  number = {5},
  pages = {981--995},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/525638},
  abstract = {The literature on causal discovery has focused on interventions that involve randomly assigning values to a single variable. But such a randomized intervention is not the only possibility, nor is it always optimal. In some cases it is impossible or it would be unethical to perform such an intervention. We provide an account of `hard' and `soft' interventions and discuss what they can contribute to causal discovery. We also describe how the choice of the optimal intervention(s) depends heavily on the particular experimental setup and the assumptions that can be made.}
}

@inproceedings{eberhardt2008almost,
  title = {Almost Optimal Intervention Sets for Causal Discovery},
  booktitle = {Proc. {{Twenty-Forth Conf}}. on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Eberhardt, Frederick},
  year = {2008},
  month = jul,
  address = {{Helsinki, Finland}}
}

@article{faes2008mutual,
  title = {Mutual Nonlinear Prediction as a Tool to Evaluate Coupling Strength and Directionality in Bivariate Time Series: {{Comparison}} among Different Strategies Based on k Nearest Neighbors},
  shorttitle = {Mutual Nonlinear Prediction as a Tool to Evaluate Coupling Strength and Directionality in Bivariate Time Series},
  author = {Faes, Luca and Porta, Alberto and Nollo, Giandomenico},
  year = {2008},
  month = aug,
  journal = {Physical Review E},
  volume = {78},
  number = {2},
  pages = {026201},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.78.026201}
}

@article{faes2011informationbased,
  title = {Information-Based Detection of Nonlinear {{Granger}} Causality in Multivariate Processes via a Nonuniform Embedding Technique},
  author = {Faes, Luca and Nollo, Giandomenico and Porta, Alberto},
  year = {2011},
  month = may,
  journal = {Physical Review E},
  volume = {83},
  number = {5},
  pages = {051112},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.83.051112}
}

@techreport{fakhar2019neuronal,
  type = {Preprint},
  title = {Neuronal {{Causes}} and {{Behavioural Effects}}: A {{Review}} on {{Logical}}, {{Methodological}}, and {{Technical Issues With Respect}} to {{Causal Explanations}} of {{Behaviour}} in {{Neuroscience}}},
  shorttitle = {Neuronal {{Causes}} and {{Behavioural Effects}}},
  author = {Fakhar, Kayson and Gonschorek, Dominic and Schmors, Lisa and Bielczyk, Natalia Z},
  year = {2019},
  month = aug,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/zk2dy},
  abstract = {Elucidating causal, neurobiological underpinnings of behaviour is an essential goal of neuroscientific studies. However, due to the complexity of the brain as well as the complexity of the organism's environment, finding a causal architecture underlying observed behaviour remains a formidable challenge. In this piece, we review the logical, conceptual, and methodological issues concerning causal research in neuroscience.}
}

@book{fakhar2020causal,
  title = {Causal {{Brain Mapping}} of {{Artificial Neural Networks}} by {{Game Theory}}: {{A Case}} for in-Silico {{Multi-Perturbation Experiments}}},
  shorttitle = {Causal {{Brain Mapping}} of {{Artificial Neural Networks}} by {{Game Theory}}},
  author = {Fakhar, Kayson and Hilgetag, Claus},
  year = {2020},
  month = sep,
  doi = {10.13140/RG.2.2.29018.77769},
  abstract = {Elucidating causal relations between the brain and its produced behaviour is a fundamental neuroscientific goal. Traditional approaches for revealing such relations have relied on univariate single-site lesion experiments in which an individual brain region is perturbed and consequent behavioural effects are quantified. More recent studies, however, have pointed towards the inefficiency of single-site perturbations to reveal reliable neural contributions, arguing for more extensive perturbational regimes and analyses, e.g., multi-site perturbation approaches. This study aims to compare uni- and multi-site perturbation approaches using an Artificial Neural Network model to elucidate their inferential capabilities.}
}

@misc{fakhar2021systematic,
  type = {Preprint},
  title = {Systematic {{Perturbation}} of an {{Artificial Neural Network}}: {{A Step Towards Quantifying Causal Contributions}} in {{The Brain}}},
  shorttitle = {Systematic {{Perturbation}} of an {{Artificial Neural Network}}},
  author = {Fakhar, Kayson and Hilgetag, Claus Christian},
  year = {2021},
  month = nov,
  publisher = {{Neuroscience}},
  doi = {10.1101/2021.11.04.467251},
  abstract = {Lesion inference analysis is a fundamental approach for characterizing the causal contributions of neural elements to brain function. Historically, it has helped to localize specialized functions in the brain after brain damage, and it has gained new prominence through the arrival of modern optogenetic perturbation techniques that allow probing the functional contributions of neural circuit elements at unprecedented levels of detail.}
}

@incollection{fornito2016connectivity,
  title = {Connectivity {{Matrices}} and {{Brain Graphs}} - {{Chapter}} 3},
  booktitle = {Fundamentals of {{Brain Network Analysis}}},
  editor = {Fornito, Alex and Zalesky, Andrew and Bullmore, Edward T.},
  year = {2016},
  month = jan,
  pages = {89--113},
  publisher = {{Academic Press}},
  address = {{San Diego}},
  doi = {10.1016/B978-0-12-407908-3.00003-0},
  abstract = {Connectomes can be represented equivalently in either matrix or graph form. These representations form the basis for all subsequent analyses of network organization. In this chapter, we overview the major types of matrices and graphs studied in connectomics, namely, those describing binary and weighted networks as well as directed and undirected networks. We consider basic properties of networks, such as connection density and weight, and review different methods for visualizing a network, either by reordering the rows and columns of the connectivity matrix, or by projecting network graphs into anatomical or topological space. We then discuss, in detail, the properties of an ideal graph-based model of a connectome, and the extent to which this ideal has been realized with current approaches.},
  isbn = {978-0-12-407908-3},
  keywords = {Adjacency,Connectogram,Connectome,Digraph,Edge,Force-directed,Graph,Matrix,Node}
}

@book{fornito2016fundamentals,
  title = {Fundamentals of Brain Network Analysis},
  author = {Fornito, Alex and Zalesky, Andrew and Bullmore, Edward},
  year = {2016},
  publisher = {{Academic Press}}
}

@article{freund2001behavioral,
  title = {Behavioral Stochastic Resonance: {{How}} a Noisy Army Betrays Its Outpost},
  shorttitle = {Behavioral Stochastic Resonance},
  author = {Freund, Jan and Kienert, Jochen and {Schimansky-Geier}, Lutz and Beisner, Beatrix and Neiman, Alexander and Russell, David and Yakusheva, Tatyana and Moss, Frank},
  year = {2001},
  month = apr,
  journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},
  volume = {63},
  pages = {031910},
  doi = {10.1103/PhysRevE.63.031910},
  abstract = {Juvenile paddlefish prey upon single zooplankton by detecting a weak electric signature resulting from its feeding and swimming motions. Moreover, it has recently been shown that paddlefish make use of stochastic resonance near the threshold for prey detection: a process termed behavioral stochastic resonance. But this process depends upon an external source of electric noise. A swarm of plankton, for example, Daphnia, can provide this noise. Assuming that juvenile paddlefish attack single Daphnia as outliers in the vicinity of the swarm, making use of noise from the swarm, we calculate the spatial distribution of the average phase locking period for the subthreshold signals acting at the paddlefish rostrum. Numeric evaluation of analytic formulas supports the notion of a noise-induced widening of the capture area quantitatively.}
}

@misc{ft,
  title = {Ft\_spike\_xcorr - {{FieldTrip}} Toolbox},
  howpublished = {https://www.fieldtriptoolbox.org/reference/ft\_spike\_xcorr/}
}

@article{fulcher2013highly,
  title = {Highly Comparative Time-Series Analysis: The Empirical Structure of Time Series and Their Methods},
  shorttitle = {Highly Comparative Time-Series Analysis},
  author = {Fulcher, Ben D. and Little, Max A. and Jones, Nick S.},
  year = {2013},
  month = jun,
  journal = {Journal of The Royal Society Interface},
  volume = {10},
  number = {83},
  pages = {20130048},
  issn = {1742-5689, 1742-5662},
  doi = {10.1098/rsif.2013.0048},
  abstract = {The process of collecting and organizing sets of observations represents a common theme throughout the history of science. However, despite the ubiquity of scientists measuring, recording and analysing the dynamics of different processes, an extensive organization of scientific time-series data and analysis methods has never been performed. Addressing this, annotated collections of over 35 000 real-world and model-generated time series, and over 9000 time-series analysis algorithms are analysed in this work. We introduce reduced representations of both time series, in terms of their properties measured by diverse scientific methods, and of time-series analysis methods, in terms of their behaviour on empirical time series, and use them to organize these interdisciplinary resources. This new approach to comparing across diverse scientific data and methods allows us to organize time-series datasets automatically according to their properties, retrieve alternatives to particular analysis methods developed in other scientific disciplines and automate the selection of useful methods for time-series classification and regression tasks. The broad scientific utility of these tools is demonstrated on datasets of electroencephalograms, self-affine time series, heartbeat intervals, speech signals and others, in each case contributing novel analysis techniques to the existing literature. Highly comparative techniques that compare across an interdisciplinary literature can thus be used to guide more focused research in time-series analysis for applications across the scientific disciplines.}
}

@misc{fulcher2022hctsa,
  title = {Hctsa: Highly Comparative Time-Series Analysis},
  shorttitle = {{$\hzigzag$}ï¸ Hctsa {$\hzigzag$}ï¸},
  author = {Fulcher, Ben},
  year = {2022},
  month = feb,
  abstract = {Highly comparative time-series analysis},
  keywords = {feature-extraction,matlab,time-series,time-series-analysis}
}

@article{gal2017rich,
  title = {Rich Cell-Type-Specific Network Topology in Neocortical Microcircuitry},
  author = {Gal, Eyal and London, Michael and Globerson, Amir and Ramaswamy, Srikanth and Reimann, Michael W. and Muller, Eilif and Markram, Henry and Segev, Idan},
  year = {2017},
  month = jul,
  journal = {Nature Neuroscience},
  volume = {20},
  number = {7},
  pages = {1004--1013},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.4576},
  abstract = {To unravel structural regularities in neocortical networks, Gal et al. analyzed a biologically constrained model of a neocortical microcircuit. Using extended graph theory, they found multiple cell-type-specific wiring features, including small-word and rich-club topologies that might contribute to the large repertoire of computations performed by the neocortex.},
  copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  keywords = {+++,Network models,Neural circuits,Research data},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Network models;Neural circuits;Research data Subject\_term\_id: network-models;neural-circuit;research-data}
}

@misc{gal2020neuron,
  title = {Neuron {{Geometry Underlies Universal Network Features}} in {{Cortical Microcircuits}}},
  author = {Gal, Eyal and Perin, Rodrigo and Markram, Henry and London, Michael and Segev, Idan},
  year = {2020},
  month = may,
  pages = {656058},
  institution = {{bioRxiv}},
  doi = {10.1101/656058},
  abstract = {Why do cortical microcircuits in a variety of brain regions express similar, highly nonrandom, network motifs? To what extent this structure is innate and how much of it is molded by plasticity and learning processes? To address these questions, we developed a general network science framework to quantify the contribution of neurons' geometry and their embedding in cortical volume to the emergence of three-neuron network motifs. Applying this framework to a dense in silico reconstructed cortical microcircuits showed that the innate asymmetric neuron's geometry underlies the universally recurring motif architecture. It also predicted the spatial alignment of cells composing the different triplets-motifs. These predictions were directly validated via in vitro 12-patch whole-cell recordings (7,309 triplets) from rat somatosensory cortex. We conclude that the local geometry of neurons imposes an innate, already structured, global network architecture, which serves as a skeleton upon which fine-grained structural and functional plasticity processes take place.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/}
}

@article{garewal1983lack,
  title = {Lack of Inhibition by Oxipurinol of 5-{{FU}} Toxicity against Human Tumor Cell Lines},
  author = {Garewal, H S and Ahmann, F R and Alberts, D S},
  year = {1983},
  month = may,
  journal = {Cancer treatment reports},
  volume = {67},
  number = {5},
  pages = {495--498},
  issn = {0361-5960},
  abstract = {Allopurinol has been reported to decrease the gastrointestinal and bone marrow toxicity of 5-FU when administered in a high dose by continuous infusion. The effect of oxipurinol, the major metabolite of allopurinol, on 5-FU cytotoxicity against three human tumor cell lines was studied using a soft agar clonogenic assay. For both WiDR (colon) and T-47 (breast), 5-FU cytotoxicity was greater in the presence of oxipurinol than in its absence. Oxipurinol did not significantly affect 5-FU cytotoxicity against Hec-1A (endometrial). In none of the three lines was a protective effect of oxipurinol noted.},
  pmid = {6682703}
}

@article{garofalo2009evaluation,
  title = {Evaluation of the {{Performance}} of {{Information Theory-Based Methods}} and {{Cross-Correlation}} to {{Estimate}} the {{Functional Connectivity}} in {{Cortical Networks}}},
  author = {Garofalo, Matteo and Nieus, Thierry and Massobrio, Paolo and Martinoia, Sergio},
  year = {2009},
  month = aug,
  journal = {PLOS ONE},
  volume = {4},
  number = {8},
  pages = {e6482},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0006482},
  abstract = {Functional connectivity of in vitro neuronal networks was estimated by applying different statistical algorithms on data collected by Micro-Electrode Arrays (MEAs). First we tested these ``connectivity methods'' on neuronal network models at an increasing level of complexity and evaluated the performance in terms of ROC (Receiver Operating Characteristic) and PPC (Positive Precision Curve), a new defined complementary method specifically developed for functional links identification. Then, the algorithms better estimated the actual connectivity of the network models, were used to extract functional connectivity from cultured cortical networks coupled to MEAs. Among the proposed approaches, Transfer Entropy and Joint-Entropy showed the best results suggesting those methods as good candidates to extract functional links in actual neuronal networks from multi-site recordings.},
  keywords = {++,+++,Action potentials,Behavior,bin size,Decision making,Electrophysiology,Network analysis,Neural networks,Neurons,Peak values}
}

@article{gerhard2013successful,
  title = {Successful {{Reconstruction}} of a {{Physiological Circuit}} with {{Known Connectivity}} from {{Spiking Activity Alone}}},
  author = {Gerhard, Felipe and Kispersky, Tilman and Gutierrez, Gabrielle J. and Marder, Eve and Kramer, Mark and Eden, Uri},
  year = {2013},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {9},
  number = {7},
  pages = {e1003138},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003138},
  abstract = {Identifying the structure and dynamics of synaptic interactions between neurons is the first step to understanding neural network dynamics. The presence of synaptic connections is traditionally inferred through the use of targeted stimulation and paired recordings or by post-hoc histology. More recently, causal network inference algorithms have been proposed to deduce connectivity directly from electrophysiological signals, such as extracellularly recorded spiking activity. Usually, these algorithms have not been validated on a neurophysiological data set for which the actual circuitry is known. Recent work has shown that traditional network inference algorithms based on linear models typically fail to identify the correct coupling of a small central pattern generating circuit in the stomatogastric ganglion of the crab Cancer borealis. In this work, we show that point process models of observed spike trains can guide inference of relative connectivity estimates that match the known physiological connectivity of the central pattern generator up to a choice of threshold. We elucidate the necessary steps to derive faithful connectivity estimates from a model that incorporates the spike train nature of the data. We then apply the model to measure changes in the effective connectivity pattern in response to two pharmacological interventions, which affect both intrinsic neural dynamics and synaptic transmission. Our results provide the first successful application of a network inference algorithm to a circuit for which the actual physiological synapses between neurons are known. The point process methodology presented here generalizes well to larger networks and can describe the statistics of neural populations. In general we show that advanced statistical models allow for the characterization of effective network structure, deciphering underlying network dynamics and estimating information-processing capabilities.},
  keywords = {Action potentials,Crabs,Network analysis,Neural networks,Neural pathways,Neurons,Neurotransmission,Synapses}
}

@incollection{gerstner2002stochastic,
  title = {Stochastic Resonance},
  booktitle = {Spiking {{Neuron Models}}. {{Single Neurons}}, {{Populations}}, {{Plasticity}}},
  author = {Gerstner, Wulfram and Kistler, Werner},
  year = {2002}
}

@incollection{gerstner2014noisy,
  title = {From Noisy Inputs to Escape Noise},
  booktitle = {Neuronal {{Dynamics}}. {{From}} Single Neurons to Networks and Models of Cognition},
  author = {Gerstner, Wulfram and Kistler, Werner and Naud, Richard and Paninski, Liam},
  year = {2014}
}

@inproceedings{ghassami2018budgeted,
  title = {Budgeted Experiment Design for Causal Structure Learning},
  booktitle = {Proc. 35th {{Int}}. {{Conf}}. on {{Machine Learning}}},
  author = {Ghassami, AmirEmad and Salehkaleybar, Saber and Kiyavash, Negar and Bareinboim, Elias},
  year = {2018},
  month = jul,
  address = {{Stockholm, Sweden}}
}

@article{gingl1995nondynamical,
  title = {Non-{{Dynamical Stochastic Resonance}}: {{Theory}} and {{Experiments}} with {{White}} and {{Arbitrarily Coloured Noise}}},
  shorttitle = {Non-{{Dynamical Stochastic Resonance}}},
  author = {Gingl, Z and Kiss, L. B and Moss, F},
  year = {1995},
  month = jan,
  journal = {Europhysics Letters (EPL)},
  volume = {29},
  number = {3},
  pages = {191--196},
  issn = {0295-5075, 1286-4854},
  doi = {10.1209/0295-5075/29/3/001},
  abstract = {We describe the simplest system which shows stochastic resonance. Theoretical results for white and (almost) arbitrarily coloured noise are presented. The new system has new, unique properties which originate from its non-dynamical character; for example, the strength and phase shift of periodic response of the system is independent of the frequency. Experiments have been carried out with the following noise processes: (physical) white noise, (physical) Lorentzian noise and (physical) l/f noise. With a small extension of the system, its linearresponse regime can be significantly increased. As the system is similar to some simple models of neurons, the new results might have not only physical but also biological importance.},
  keywords = {foundational}
}

@article{granger1969investigating,
  title = {Investigating {{Causal Relations}} by {{Econometric Models}} and {{Cross-spectral Methods}}},
  author = {Granger, C. W. J.},
  year = {1969},
  month = aug,
  journal = {Econometrica},
  volume = {37},
  number = {3},
  pages = {424},
  issn = {00129682},
  doi = {10.2307/1912791}
}

@article{grosenick2016closedloop,
  title = {Closed-{{Loop}} and {{Activity-Guided Optogenetic Control}}},
  author = {Grosenick, Logan and Marshel, James H. and Deisseroth, Karl},
  year = {2016},
  journal = {Neuron},
  volume = {86},
  pages = {106--139},
  doi = {10.1016/j.neuron.2015.03.034}
}

@article{harrisonstatistical,
  title = {Statistical {{Identification}} of {{Synchronous Spiking}}},
  author = {Harrison, Matthew T and Amarasingham, Asohan and Kass, E},
  pages = {42},
  keywords = {+}
}

@article{hauser2012characterization,
  title = {Characterization and Greedy Learning of Interventional {{Markov}} Equivalence Classes of Directed Acyclic Graphs},
  author = {Hauser, Alain and B{\"u}hlmann, Peter},
  year = {2012},
  month = aug,
  journal = {Journal of Machine Learning Research},
  volume = {13},
  pages = {2409--2464},
  abstract = {The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes. In this paper, we extend the notion of Markov equivalence of DAGs to the case of interven- tional distributions arising from multiple intervention experiments. We show that under reasonable assumptions on the intervention experiments, interventional Markov equivalence defines a finer par- titioning of DAGs than observational Markov equivalence and hence improves the identifiability of causal models. We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph (also known as CPDAG in the observational case). These are key insights for deriving a general- ization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data. This new algorithm is evaluated in a simulation study.}
}

@article{hirabayashi2005dynamically,
  title = {Dynamically {{Modulated Spike Correlation}} in {{Monkey Inferior Temporal Cortex Depending}} on the {{Feature Configuration}} within a {{Whole Object}}},
  author = {Hirabayashi, Toshiyuki and Miyashita, Yasushi},
  year = {2005},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {25},
  number = {44},
  pages = {10299--10307},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3036-05.2005},
  abstract = {The mechanism underlying the processing of spatially separated multiple local features to form a unique whole object is an important issue in visual object recognition. We tested whether, in behaving monkeys, the spike correlation between pairs of inferior temporal (IT) neurons dynamically changes depending on the spatial configuration of the local features within a whole object. We prepared more than 60,000 face-like objects (FOs) and their corresponding non-face-like objects (NFOs) that consisted of random arrangements of the same set of local features as those in FOs. The spike correlation between a pair of neurons was quantified by the peak height of the shift predictor-subtracted cross-correlogram. For both neurons of the pair, the local features in a whole object were determined so that they elicited as high a response as possible to enable a reliable cross-correlation analysis. We found that the FOs thus constructed elicited neuronal activities that were more strongly correlated than the corresponding NFOs. Firing rates of the same neurons did not show such a consistent bias depending on the feature configuration. Furthermore, receiver operating characteristic analysis revealed that this FO dominance of spike correlation was robust enough to discriminate between different feature configurations at the population level. Spike correlation of the cell pairs exhibited significant FO dominance within 300 ms after stimulus onset. The present results suggest that feature configuration within a unique whole object can be reflected in the rapid modulation of spike correlation among a population of neurons in the IT cortex.},
  chapter = {Behavioral/Systems/Cognitive},
  copyright = {Copyright \textcopyright{} 2005 Society for Neuroscience 0270-6474/05/2510299-09.00/0},
  pmid = {16267238},
  keywords = {cell assembly,cross-correlation analysis,inferior temporal cortex,macaque monkey,neurophysiology,visual object recognition}
}

@article{hodgkin1949effect,
  title = {The Effect of Sodium Ions on the Electrical Activity of the Giant Axon of the Squid},
  author = {Hodgkin, A. L. and Katz, B.},
  year = {1949},
  month = mar,
  journal = {The Journal of Physiology},
  volume = {108},
  number = {1},
  pages = {37--77},
  issn = {0022-3751},
  pmcid = {PMC1392331},
  pmid = {18128147}
}

@article{hodgkin1952measurement,
  title = {Measurement of Current-Voltage Relationships in the Mebrane of the Giant Axon of Loligo},
  author = {Hodgkin, A L and Huxley, A F and Katz, B},
  year = {1952},
  month = apr,
  journal = {J. Physiol},
  volume = {116},
  number = {4},
  pages = {424--48},
  doi = {10.1113/jphysiol.1952.sp004716},
  pmid = {14946712}
}

@techreport{huang2017circuit,
  type = {Preprint},
  title = {Circuit Models of Low Dimensional Shared Variability in Cortical Networks},
  author = {Huang, Chengcheng and Ruff, Douglas A. and Pyle, Ryan and Rosenbaum, Robert and Cohen, Marlene R. and Doiron, Brent},
  year = {2017},
  month = nov,
  institution = {{Neuroscience}},
  doi = {10.1101/217976},
  abstract = {Abstract                        Trial-to-trial variability is a reflection of the circuitry and cellular physiology that makeup a neuronal network. A pervasive yet puzzling feature of cortical circuits is that despite their complex wiring, population-wide shared spiking variability is low dimensional with all neurons fluctuating en masse. Previous model cortical networks are at loss to explain this global variability, and rather assume it is from external sources. We show that if the spatial and temporal scales of inhibitory coupling match known physiology, model spiking neurons internally generate low dimensional shared variability that captures the properties of             in vivo             population recordings along the visual pathway. Shifting spatial attention into the receptive field of visual neurons has been shown to reduce low dimensional shared variability within a brain area, yet increase the variability shared between areas. A top-down modulation of inhibitory neurons in our network provides a parsimonious mechanism for this attentional modulation, providing support for our theory of cortical variability. Our work provides a critical and previously missing mechanistic link between observed cortical circuit structure and realistic population-wide shared neuronal variability and its modulation.}
}

@article{hubel1959receptive,
  title = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  year = {1959},
  month = oct,
  journal = {The Journal of Physiology},
  volume = {148},
  number = {3},
  pages = {574--591},
  issn = {0022-3751},
  pmcid = {PMC1363130},
  pmid = {14403679}
}

@article{hubel1962receptive,
  title = {Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  year = {1962},
  month = jan,
  journal = {The Journal of Physiology},
  volume = {160},
  number = {1},
  pages = {106-154.2},
  issn = {0022-3751},
  abstract = {Images null},
  pmcid = {PMC1359523},
  pmid = {14449617}
}

@inproceedings{ikegwu2020pyif,
  title = {{{PyIF}}: {{A Fast}} and {{Light Weight Implementation}} to {{Estimate Bivariate Transfer Entropy}} for {{Big Data}}},
  shorttitle = {{{PyIF}}},
  booktitle = {2020 {{SoutheastCon}}},
  author = {Ikegwu, Kelechi M. and Trauger, Jacob and McMullin, Jeff and Brunner, Robert J.},
  year = {2020},
  month = mar,
  pages = {1--6},
  issn = {1558-058X},
  doi = {10.1109/SoutheastCon44009.2020.9249650},
  abstract = {Transfer entropy is an information measure that quantifies information flow between processes evolving in time. Transfer entropy has a plethora of potential applications in financial markets, canonical systems, neuroscience, and social media. We offer a fast open source Python implementation called PyIF that estimates Transfer Entropy with Kraskov's method. PyIF utilizes KD-Trees, multiple processes by parallelizing queries on said KD-Trees, and can be used with CUDA compatible GPUs to significantly reduce the wall time for estimating transfer entropy. We find from our analyses that PyIF's GPU implementation is up to 1072 times faster (and it's CPU implementation is up 181 times faster) than existing implementations to estimate transfer entropy on large data and scales better than existing implementatin.},
  keywords = {Big Data,Entropy,Graphics processing units,Parallel Processing,Social networking (online),Software development management,Systems neuroscience,Time measurement,Transfer Entropy}
}

@article{ioanae.marinescu2018quasiexperimental,
  title = {Quasi-Experimental Causality in Neuroscience and Behavioural Research},
  shorttitle = {Marinescu\_quasi-Experimental},
  author = {{Ioana E. Marinescu} and {Patrick N. Lawlor} and {Konrad P. Kording}},
  year = {2018},
  journal = {Nature Human Behavior},
  volume = {2},
  pages = {891--898},
  abstract = {In many scientific domains, causality is the key question. For example, in neuroscience, we might ask whether a medication affects perception, cognition or action. Randomized controlled trials are the gold standard to establish causality, but they are not always practical. The field of empirical economics has developed rigorous methods to establish causality even when randomized controlled trials are not available. Here we review these quasi-experimental methods and highlight how neuroscience and behavioural researchers can use them to do research that can credibly demonstrate causal effects.}
}

@article{ito2011extending,
  title = {Extending {{Transfer Entropy Improves Identification}} of {{Effective Connectivity}} in a {{Spiking Cortical Network Model}}},
  author = {Ito, Shinya and Hansen, Michael E. and Heiland, Randy and Lumsdaine, Andrew and Litke, Alan M. and Beggs, John M.},
  year = {2011},
  month = nov,
  journal = {PLOS ONE},
  volume = {6},
  number = {11},
  pages = {e27431},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0027431},
  abstract = {Transfer entropy (TE) is an information-theoretic measure which has received recent attention in neuroscience for its potential to identify effective connectivity between neurons. Calculating TE for large ensembles of spiking neurons is computationally intensive, and has caused most investigators to probe neural interactions at only a single time delay and at a message length of only a single time bin. This is problematic, as synaptic delays between cortical neurons, for example, range from one to tens of milliseconds. In addition, neurons produce bursts of spikes spanning multiple time bins. To address these issues, here we introduce a free software package that allows TE to be measured at multiple delays and message lengths. To assess performance, we applied these extensions of TE to a spiking cortical network model (Izhikevich, 2006) with known connectivity and a range of synaptic delays. For comparison, we also investigated single-delay TE, at a message length of one bin (D1TE), and cross-correlation (CC) methods. We found that D1TE could identify 36\% of true connections when evaluated at a false positive rate of 1\%. For extended versions of TE, this dramatically improved to 73\% of true connections. In addition, the connections correctly identified by extended versions of TE accounted for 85\% of the total synaptic weight in the network. Cross correlation methods generally performed more poorly than extended TE, but were useful when data length was short. A computational performance analysis demonstrated that the algorithm for extended TE, when used on currently available desktop computers, could extract effective connectivity from 1 hr recordings containing 200 neurons in {$\sim$}5 min. We conclude that extending TE to multiple delays and message lengths improves its ability to assess effective connectivity between spiking neurons. These extensions to TE soon could become practical tools for experimentalists who record hundreds of spiking neurons.},
  keywords = {+++,Action potentials,Algorithms,bin size,Entropy,Network analysis,Neural networks,Neurons,Peak values,Synapses}
}

@article{itzcovich2017stochastic,
  title = {Stochastic Resonance Improves Vision in the Severely Impaired},
  author = {Itzcovich, Elena and Riani, Massimo and Sannita, Walter G.},
  year = {2017},
  month = oct,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {12840},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-12906-2},
  abstract = {We verified whether a stochastic resonance paradigm (SR), with random interference (``noise'') added in optimal amounts, improves the detection of sub-threshold visual information by subjects with retinal disorder and impaired vision as it does in the normally sighted. Six levels of dynamic, zero-mean Gaussian noise were added to each pixel of images (13 contrast levels) in which alphabet characters were displayed against a uniform gray background. Images were presented with contrast below the subjective threshold to 14 visually impaired subjects (age: 22\textendash 53 yrs.). The fraction of recognized letters varied between 0 and 0.3 at baseline and increased in all subjects when noise was added in optimal amounts; peak recognition ranged between 0.2 and 0.8 at noise sigmas between 6 and 30 grey scale values (GSV) and decreased in all subjects at noise levels with sigma above 30 GSV. The results replicate in the visually impaired the facilitation of visual information processing with images presented in SR paradigms that has been documented in sighted subjects. The effect was obtained with low-level image manipulation and application appears readily possible: it would enhance the efficiency of today vision-improving aids and help in the development of the visual prostheses hopefully available in the future.},
  copyright = {2017 The Author(s)},
  keywords = {Medical research,Object vision},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Medical research;Object vision Subject\_term\_id: medical-research;object-vision}
}

@inproceedings{jaber2020causal,
  title = {Causal Discovery from Soft Interventions with Unknown Targets: {{Characterization}} and Learning},
  booktitle = {Proc. 33rd {{Conf}}. on {{Adv}}. in {{Neural Information Processing Systems}}},
  author = {Jaber, Amin and Kocaoglu, Murat and Shanmugam, Karthikeyan and Bareinboim, Elias},
  year = {2020},
  month = dec,
  address = {{Vancouver, BC, Canada}},
  abstract = {One fundamental problem in the empirical sciences is of reconstructing the causal structure that underlies a phenomenon of interest through observation and experimentation. While there exists a plethora of methods capable of learning the equivalence class of causal structures that are compatible with observations, it is less well-understood how to systematically combine observations and experiments to reconstruct the underlying structure. In this paper, we investigate the task of structural learning in non-Markovian systems (i.e., when latent variables affect more than one observable) from a combination of observational and soft experimental data when the interventional targets are unknown. Using causal invariances found across the collection of observational and interventional distributions (not only conditional independences), we define a property called psi-Markov that connects these distributions to a pair consisting of (1) a causal graph D and (2) a set of interventional targets I. Building on this property, our main contributions are two-fold: First, we provide a graphical characterization that allows one to test whether two causal graphs with possibly different sets of interventional targets belong to the same psi-Markov equivalence class. Second, we develop an algorithm capable of harnessing the collection of data to learn the corresponding equivalence class. We then prove that this algorithm is sound and complete, in the sense that it is the most informative in the sample limit, i.e., it discovers as many tails and arrowheads as can be oriented within a psi-Markov equivalence class.}
}

@article{janzing2013quantifying,
  title = {Quantifying Causal Influences},
  author = {Janzing, Dominik and Balduzzi, David and {Grosse-Wentrup}, Moritz and Sch{\"o}lkopf, Bernhard},
  year = {2013},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {41},
  number = {5},
  issn = {0090-5364},
  doi = {10.1214/13-AOS1145}
}

@misc{johnsen2021taming,
  title = {Taming Variability with Feedback Control of Neural Activity, Aided by Cleosim: An in Silico Testbed},
  shorttitle = {Taming Variability with Feedback Control of Neural Activity, Aided by Cleosim},
  author = {Johnsen, Kyle},
  year = {2021},
  month = nov,
  abstract = {NMC 4.0 flash talk: https://conference.neuromatch.io/abst... Abstract: Many important neuroscience discoveries have been fueled by advances in experimental techniques\textemdash from Golgi staining to neural recording to optogenetics. Relatively recent developments in recording and intervention methods, combined with increases in computing performance, have paved the way for a new technique: closed-loop control on the timescale of firing rates and population dynamics. Just as the dynamic clamp, by controlling membrane potentials and currents, helped cast light on the mechanisms behind the action potential, control at larger scales could prove crucial in elucidating neural circuits. Previous work has begun to realize this in the form of closed-loop optogenetic stimulation combined with electrophysiology recordings. As a new technique, however, many questions about how best to implement and apply it remain. To help answer these questions, we are developing a simulation framework and testbed for prototyping closed-loop control experiments and methods. Specifically, we are developing optogenetic and electrophysiology modules for simple integration with the Brian 2 spiking neural network simulator as well as an abstract signal processing interface capable of modeling delays to explore the effects of control latency in real-time experiments. We will also include a selection of network models to serve as examples and as a testbed for algorithmic development. We hope to thus facilitate the application and development of closed-loop neural control methods, as well as study their potential impact on neuroscience.}
}

@article{jonas2017could,
  title = {Could a {{Neuroscientist Understand}} a {{Microprocessor}}?},
  author = {Jonas, Eric and Kording, Konrad Paul},
  year = {2017},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {1},
  pages = {e1005268},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005268},
  abstract = {There is a popular belief in neuroscience that we are primarily data limited, and that producing large, multimodal, and complex datasets will, with the help of advanced data analysis algorithms, lead to fundamental insights into the way the brain processes information. These datasets do not yet exist, and if they did we would have no way of evaluating whether or not the algorithmically-generated insights were sufficient or even correct. To address this, here we take a classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the microprocessor. This suggests current analytic approaches in neuroscience may fall short of producing meaningful understanding of neural systems, regardless of the amount of data. Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.},
  keywords = {+++,Behavior,Behavioral neuroscience,Computational neuroscience,Connectomics,Microprocessors,Neuronal tuning,Neurons,Neuroscience}
}

@article{kara2003efficacy,
  title = {Efficacy of {{Retinal Spikes}} in {{Driving Cortical Responses}}},
  author = {Kara, Prakash and Reid, R. Clay},
  year = {2003},
  month = sep,
  journal = {The Journal of Neuroscience},
  volume = {23},
  number = {24},
  pages = {8547--8557},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.23-24-08547.2003},
  abstract = {How does a single retinal ganglion cell (RGC) affect the firing of simple cells in the visual cortex? Although much is known of the functional connections between the retina and the lateral geniculate nucleus (LGN) and between LGN and visual cortex, it is hard to infer the effect of disynaptic connections from retina to visual cortex. Most importantly, there is considerable divergence from retina to LGN, so cortical neurons might be influenced by ganglion cells through multiple feedforward pathways. We recorded simultaneously from ganglion cells in the retina and cortical simple cells in the striate cortex with overlapping receptive fields and evaluated disynaptic connections with cross-correlation analysis. In all disynaptically connected pairs, the retinal receptive field center and overlapping cortical subregion always shared the same sign (either both ON or both OFF). Connected pairs were similar in other respects, such as relative position and timing of their receptive fields, and thus obeyed the same rules of connectivity found previously for retinothalamic and thalamocortical connections. We found that a single RGC directly contributed on average to {$\sim$}3\% of the activity of its cortical target. The relative timing of pairs of spikes from the retinal cell affected their efficacy in driving the cortical cell. When two retinal spikes were closely spaced ({$<$}10 msec), the second spike was several times more likely to drive the cortical target. The relative magnitude of this disynaptic paired spike enhancement was considerably larger than has been found previously for retinogeniculate and geniculocortical connections. The amplified paired spike enhancement from retina to cortex ensures that signal transmission from retina to cortex is particularly effective when the retina fires a series of closely spaced action potentials.},
  pmcid = {PMC6740380},
  pmid = {13679424}
}

@incollection{karnath2019lesionbehavior,
  title = {Lesion-{{Behavior Mapping}} in {{Cognitive Neuroscience}}: {{A Practical Guide}} to {{Univariate}} and {{Multivariate Approaches}}},
  shorttitle = {Lesion-{{Behavior Mapping}} in {{Cognitive Neuroscience}}},
  booktitle = {Spatial {{Learning}} and {{Attention Guidance}}},
  author = {Karnath, Hans-Otto and Sperber, Christoph and Wiesen, Daniel and {de Haan}, Bianca},
  editor = {Pollmann, Stefan},
  year = {2019},
  volume = {151},
  pages = {209--238},
  publisher = {{Springer US}},
  address = {{New York, NY}},
  doi = {10.1007/7657_2019_18},
  isbn = {978-1-4939-9947-7 978-1-4939-9948-4}
}

@article{keinan2004causal,
  title = {Causal Localization of Neural Function: The {{Shapley}} Value Method},
  shorttitle = {Causal Localization of Neural Function},
  author = {Keinan, Alon and Hilgetag, Claus C. and Meilijson, Isaac and Ruppin, Eytan},
  year = {2004},
  month = jun,
  journal = {Neurocomputing},
  volume = {58--60},
  pages = {215--222},
  issn = {09252312},
  doi = {10.1016/j.neucom.2004.01.046},
  abstract = {Identifying the functional roles of elements of a neural network is one of the fundamental challenges in understanding neural information processing. Aiming at this goal, lesion studies have been used extensively in neuroscience. Most of these employ single lesions and hence, limited ability in revealing the signi\"ycance of interacting elements. This paper presents the multi-perturbation Shapley value analysis (MSA), an axiomatic, scalable and rigorous method, addressing the challenge of determining the contributions of network elements from a data set of multi-lesions or other perturbations. The successful workings of the MSA are demonstrated on arti\"ycial and biological data. MSA is a novel method for causal function localization, with a wide range of potential applications for the analysis of reversible deactivation experiments and TMS-induced ``virtual lesions''.}
}

@article{keinan2006axiomatic,
  title = {Axiomatic Scalable Neurocontroller Analysis via the {{Shapley}} Value},
  author = {Keinan, Alon and Sandbank, Ben and Hilgetag, Claus C. and Meilijson, Isaac and Ruppin, Eytan},
  year = {2006},
  journal = {Artificial Life},
  volume = {12},
  number = {3},
  pages = {333--352},
  issn = {1064-5462},
  doi = {10.1162/artl.2006.12.3.333},
  abstract = {One of the major challenges in the field of neurally driven evolved autonomous agents is deciphering the neural mechanisms underlying their behavior. Aiming at this goal, we have developed the multi-perturbation Shapley value analysis (MSA)--the first axiomatic and rigorous method for deducing causal function localization from multiple-perturbation data, substantially improving on earlier approaches. Based on fundamental concepts from game theory, the MSA provides a formal way of defining and quantifying the contributions of network elements, as well as the functional interactions between them. The previously presented versions of the MSA require full knowledge (or at least an approximation) of the network's performance under all possible multiple perturbations, limiting their applicability to systems with a small number of elements. This article focuses on presenting new scalable MSA variants, allowing for the analysis of large complex networks in an efficient manner, including large-scale neurocontrollers. The successful operation of the MSA along with the new variants is demonstrated in the analysis of several neurocontrollers solving a food foraging task, consisting of up to 100 neural elements.},
  pmid = {16859444},
  keywords = {Models; Statistical,Neural Networks; Computer,Software,User-Computer Interface}
}

@misc{kirklandcrosscorrelation,
  title = {Crosscorrelation},
  author = {Kirkland, Kyle},
  howpublished = {https://www.med.upenn.edu/mulab/crosscorrelation.html}
}

@article{klink2021combining,
  title = {Combining Brain Perturbation and Neuroimaging in Non-Human Primates},
  author = {Klink, P. Christiaan and Aubry, Jean-Fran{\c c}ois and Ferrera, Vincent P. and Fox, Andrew S. and {Froudist-Walsh}, Sean and Jarraya, B{\'e}chir and Konofagou, Elisa E. and Krauzlis, Richard J. and Messinger, Adam and Mitchell, Anna S. and {Ortiz-Rios}, Michael and Oya, Hiroyuki and Roberts, Angela C. and Roe, Anna Wang and Rushworth, Matthew F. S. and Sallet, J{\'e}r{\^o}me and Schmid, Michael Christoph and Schroeder, Charles E. and Tasserie, Jordy and Tsao, Doris Y. and Uhrig, Lynn and Vanduffel, Wim and Wilke, Melanie and Kagan, Igor and Petkov, Christopher I.},
  year = {2021},
  month = jul,
  journal = {NeuroImage},
  volume = {235},
  pages = {118017},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2021.118017},
  abstract = {Brain perturbation studies allow detailed causal inferences of behavioral and neural processes. Because the combination of brain perturbation methods and neural measurement techniques is inherently challenging, research in humans has predominantly focused on non-invasive, indirect brain perturbations, or neurological lesion studies. Non-human primates have been indispensable as a neurobiological system that is highly similar to humans while simultaneously being more experimentally tractable, allowing visualization of the functional and structural impact of systematic brain perturbation. This review considers the state of the art in non-human primate brain perturbation with a focus on approaches that can be combined with neuroimaging. We consider both non-reversible (lesions) and reversible or temporary perturbations such as electrical, pharmacological, optical, optogenetic, chemogenetic, pathway-selective, and ultrasound based interference methods. Method-specific considerations from the research and development community are offered to facilitate research in this field and support further innovations. We conclude by identifying novel avenues for further research and innovation and by highlighting the clinical translational potential of the methods.},
  keywords = {Causality,Chemogenetics,fMRI,Infrared,Lesion,Microstimulation,Optogenetics,Primates,Ultrasound}
}

@article{knox1981detection,
  title = {Detection of Neuronal Interactions Using Correlation Analysis},
  author = {Knox, Charles K.},
  year = {1981},
  month = jan,
  journal = {Trends in Neurosciences},
  volume = {4},
  pages = {222--225},
  issn = {01662236},
  doi = {10.1016/0166-2236(81)90070-9}
}

@article{kobayashi2019reconstructing,
  title = {Reconstructing Neuronal Circuitry from Parallel Spike Trains},
  author = {Kobayashi, Ryota and Kurita, Shuhei and Kurth, Anno and Kitano, Katsunori and Mizuseki, Kenji and Diesmann, Markus and Richmond, Barry J. and Shinomoto, Shigeru},
  year = {2019},
  month = oct,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {4468},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12225-2},
  abstract = {State-of-the-art techniques allow researchers to record large numbers of spike trains in parallel for many hours. With enough such data, we should be able to infer the connectivity among neurons. Here we develop a method for reconstructing neuronal circuitry by applying a generalized linear model (GLM) to spike cross-correlations. Our method estimates connections between neurons in units of postsynaptic potentials and the amount of spike recordings needed to verify connections. The performance of inference is optimized by counting the estimation errors using synthetic data. This method is superior to other established methods in correctly estimating connectivity. By applying our method to rat hippocampal data, we show that the types of estimated connections match the results inferred from other physiological cues. Thus our method provides the means to build a circuit diagram from recorded spike trains, thereby providing a basis for elucidating the differences in information processing in different brain regions.},
  copyright = {2019 The Author(s)},
  keywords = {+,Computational neuroscience,Neural decoding,Neuroscience},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Neural decoding;Neuroscience Subject\_term\_id: computational-neuroscience;neural-decoding;neuroscience}
}

@article{kooij2005comparison,
  title = {Comparison of Different Methods to Identify and Quantify Balance Control},
  author = {Kooij, Herman and {van Asseldonk}, Edwin and {van der Helm}, Frans},
  year = {2005},
  month = jul,
  journal = {Journal of neuroscience methods},
  volume = {145},
  pages = {175--203},
  doi = {10.1016/j.jneumeth.2005.01.003},
  abstract = {The goal of this paper is to clarify the methodological aspects of studies of human balance during quiet standing and perturbed standing. Centre of mass (CoM), centre of pressure (CoP) and electromyogram (EMG) or similar measures are commonly recorded to quantify human balance control. In this paper we show that to identify the rigid body dynamics and the physiological mechanism that controls the body separately, one has to externally perturb the body with known perturbations and to use the indirect (IA) or joint input-output approach (JA) for identification. However, in many balance control studies the direct approach (DA) have been used, which is well suited to study open-loop systems but will give erroneous results when applied to a closed-loop system, as in human balance control. The cross-correlation function and linear regression are examples of the erroneous application of the DA approach in human balance control studies. The consequences of this erroneous DA are given. In addition a new application of the JA is presented that identifies physiological mechanisms that control balance, including passive and active feedback pathways. This new method is compared with existing identification schemes that use the IA and an existing JA that estimates the active pathway. Also it is shown how descriptive measures such as the power spectral densities (PSD) or the stabilogram diffusion plot (SDP) of the CoP and/or CoM depends on the PSD of internal perturbations and sensor noise, which are not measured. Although descriptive measures can be used to describe the state of the balance control system for a particular situation, it does not separate the dynamics of unknown processes that perturb balance from the dynamics of the active and passive feedback mechanisms that controls balance. Only the IA and the preferred JA can give estimates of the passive and active passive feedback mechanisms that control balance.}
}

@article{koutrouli2020guide,
  title = {A {{Guide}} to {{Conquer}} the {{Biological Network Era Using Graph Theory}}},
  author = {Koutrouli, Mikaela and Karatzas, Evangelos and {Paez-Espino}, David and Pavlopoulos, Georgios A.},
  year = {2020},
  journal = {Frontiers in Bioengineering and Biotechnology},
  volume = {8},
  issn = {2296-4185},
  abstract = {Networks are one of the most common ways to represent biological systems as complex sets of binary interactions or relations between different bioentities. In this article, we discuss the basic graph theory concepts and the various graph types, as well as the available data structures for storing and reading graphs. In addition, we describe several network properties and we highlight some of the widely used network topological features. We briefly mention the network patterns, motifs and models, and we further comment on the types of biological and biomedical networks along with their corresponding computer- and human-readable file formats. Finally, we discuss a variety of algorithms and metrics for network analyses regarding graph drawing, clustering, visualization, link prediction, perturbation, and network alignment as well as the current state-of-the-art tools. We expect this review to reach a very broad spectrum of readers varying from experts to beginners while encouraging them to enhance the field further.}
}

@book{kramer2016case,
  title = {Case Studies in Neural Data Analysis: A Guide for the Practicing Neuroscientist},
  author = {Kramer, Mark A and Eden, Uri T},
  year = {2016},
  publisher = {{MIT Press}}
}

@article{krauss2015adaptive,
  title = {Adaptive Stochastic Resonance Based on Output Autocorrelations},
  author = {Krauss, Patrick and Metzner, Claus and Tziridis, Konstantin and Schulze, Holger},
  year = {2015},
  month = apr,
  journal = {arXiv:1504.05032 [cs, math]},
  eprint = {1504.05032},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Successful detection of weak signals is a universal challenge for numerous technical and biological systems and crucially limits signal transduction and transmission. Stochastic resonance (SR) has been identified to have the potential to tackle this problem, namely to enable non-linear systems to detect small, otherwise sub-threshold signals by means of added non-zero noise. This has been demonstrated within a wide range of systems in physical, technological and biological contexts. Based on its ubiquitous importance, numerous theoretical and technical approaches aim at an optimization of signal transduction based on SR. Several quantities like mutual information, signal-to-noise-ratio, or the cross-correlation between input stimulus and resulting detector response have been used to determine optimal noise intensities for SR. The fundamental shortcoming with all these measures is that knowledge of the signal to be detected is required to compute them. This dilemma prevents the use of adaptive SR procedures in any application where the signal to be detected is unknown. We here show that the autocorrelation function (AC) of the detector response fundamentally overcomes this drawback. For a simplified model system, the equivalence of the output AC with the measures mentioned above is proven analytically. In addition, we test our approach numerically for a variety of systems comprising different input signals and different types of detectors. The results indicate a strong similarity between mutual information and output AC in terms of the optimal noise intensity for SR. Hence, using the output AC to adaptively vary the amount of added noise in order to maximize information transmission via SR might be a fundamental processing principle in nature, in particular within neural systems which could be implemented in future technical applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory}
}

@article{kumar2013challenges,
  title = {Challenges of Understanding Brain Function by Selective Modulation of Neuronal Subpopulations},
  author = {Kumar, Arvind and Vlachos, Ioannis and Aertsen, Ad and Boucsein, Clemens},
  year = {2013},
  month = oct,
  journal = {Trends in Neurosciences},
  volume = {36},
  number = {10},
  pages = {579--586},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2013.06.005},
  abstract = {Neuronal networks confront researchers with an overwhelming complexity of interactions between their elements. A common approach to understanding neuronal processing is to reduce complexity by defining subunits and infer their functional role by selectively modulating them. However, this seemingly straightforward approach may lead to confusing results if the network exhibits parallel pathways leading to recurrent connectivity. We demonstrate limits of the selective modulation approach and argue that, even though highly successful in some instances, the approach fails in networks with complex connectivity. We argue to refine experimental techniques by carefully considering the structural features of the neuronal networks involved. Such methods could dramatically increase the effectiveness of selective modulation and may lead to a mechanistic understanding of principles underlying brain function.},
  keywords = {++,controllability,embeddedness,why closed-loop}
}

@article{lacasa2015network,
  title = {Network Structure of Multivariate Time Series},
  author = {Lacasa, Lucas and Nicosia, Vincenzo and Latora, Vito},
  year = {2015},
  month = oct,
  journal = {Scientific Reports},
  volume = {5},
  number = {1},
  pages = {15508},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep15508},
  abstract = {Our understanding of a variety of phenomena in physics, biology and economics crucially depends on the analysis of multivariate time series. While a wide range tools and techniques for time series analysis already exist, the increasing availability of massive data structures calls for new approaches for multidimensional signal processing. We present here a non-parametric method to analyse multivariate time series, based on the mapping of a multidimensional time series into a multilayer network, which allows to extract information on a high dimensional dynamical system through the analysis of the structure of the associated multiplex network. The method is simple to implement, general, scalable, does not require ad hoc phase space partitioning and is thus suitable for the analysis of large, heterogeneous and non-stationary time series. We show that simple structural descriptors of the associated multiplex networks allow to extract and quantify nontrivial properties of coupled chaotic maps, including the transition between different dynamical phases and the onset of various types of synchronization. As a concrete example we then study financial time series, showing that a multiplex network analysis can efficiently discriminate crises from periods of financial stability, where standard methods based on time-series symbolization often fail.},
  copyright = {2015 The Author(s)},
  keywords = {Complex networks,Nonlinear phenomena},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Complex networks;Nonlinear phenomena Subject\_term\_id: complex-networks;nonlinear-phenomena}
}

@article{lepperod2018inferring,
  title = {Inferring Causal Connectivity from Pairwise Recordings and Optogenetics},
  author = {Lepper{\o}d, Mikkel Elle and St{\"o}ber, Tristan and Hafting, Torkel and Fyhn, Marianne and Kording, Konrad Paul},
  year = {2018},
  month = nov,
  publisher = {{bioRxiv}},
  doi = {10.1101/463760},
  abstract = {To study how the brain works, it is crucial to identify causal interactions between neurons, which is thought to require perturbations. However, when using optogenetics we typically perturb multiple neurons, producing a confound - any of the stimulated neurons can have affected the postsynaptic neuron. Here we show how this produces large biases, and how they can be reduced using the instrumental variable (IV) technique from econometrics. The interaction between stimulation and the absolute refractory period produces a weak, approximately random signal which can be exploited to estimate causal connectivity. When simulating integrate-and-fire neurons, we find that estimates from IV are better than na\"ive techniques (R2 = 0.77 vs R2 = 0.01). The difference is important as the estimates disagree when applied to experimental data from stimulated neurons with recorded spiking activity. Presented is a robust analysis framework for mapping out network connectivity based on causal neuron interactions.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  keywords = {++,cross-correlation}
}

@article{lin2015nature,
  title = {The {{Nature}} of {{Shared Cortical Variability}}},
  author = {Lin, I-Chun and Okun, Michael and Carandini, Matteo and Harris, Kenneth D.},
  year = {2015},
  month = aug,
  journal = {Neuron},
  volume = {87},
  number = {3},
  pages = {644--656},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2015.06.035},
  abstract = {Neuronal responses of sensory cortex are highly variable, and this variability is correlated across neurons. To assess how variability reflects factors shared across a neuronal population, we analyzed the activity of many simultaneously recorded neurons in visual cortex. We developed a simple model that comprises two sources of shared variability: a multiplicative gain, which uniformly scales each neuron's sensory drive, and an additive offset, which affects different neurons to different degrees. This model captured the variability of spike counts and reproduced the dependence of pairwise correlations on neuronal tuning and stimulus orientation. The relative contributions of the additive and multiplicative fluctuations could vary over time and had marked impact on population coding. These observations indicate that shared variability of neuronal populations in sensory cortex can be largely explained by two factors that modulate the whole population.}
}

@misc{linderman2021stats320,
  title = {{{STATS320}}: {{Machine Learning Methods}} for {{Neural Data Analysis}}},
  shorttitle = {{{STATS320}}},
  author = {Linderman, Scott},
  year = {2021},
  month = oct,
  abstract = {STATS320: Statistical Methods for Neural Data Analysis}
}

@article{lindermanbayesian,
  title = {Bayesian Latent Structure Discovery from Multi-Neuron Recordings},
  author = {Linderman, Scott and Adams, Ryan P and Pillow, Jonathan W},
  pages = {9},
  abstract = {Neural circuits contain heterogeneous groups of neurons that differ in type, location, connectivity, and basic response properties. However, traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits. In particular, they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains. Here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (GLM). Our approach combines the GLM with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns. Fully Bayesian inference via P\'olya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains. We demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina, revealing latent patterns of neural types and locations from spike trains alone.}
}

@article{lizier2010differentiating,
  title = {Differentiating Information Transfer and Causal Effect},
  author = {Lizier, J. T. and Prokopenko, M.},
  year = {2010},
  month = feb,
  journal = {The European Physical Journal B},
  volume = {73},
  number = {4},
  pages = {605--615},
  issn = {1434-6028, 1434-6036},
  doi = {10.1140/epjb/e2010-00034-5}
}

@article{lizier2012information,
  title = {Information Storage, Loop Motifs, and Clustered Structure in Complex Networks},
  author = {Lizier, Joseph T. and Atay, Fatihcan M. and Jost, J{\"u}rgen},
  year = {2012},
  month = aug,
  journal = {Physical Review E},
  volume = {86},
  number = {2},
  pages = {026110},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.86.026110}
}

@misc{lizier2020effective,
  title = {Effective Network Inference - {{Part}} 4 - {{Multivariate Transfer Entropy}} (Subtleties, and {{IDTxl}}) - {{YouTube}}},
  author = {Lizier, Joseph T.},
  year = {2020},
  howpublished = {https://www.youtube.com/watch?v=1yxA2Ln-0RU\&ab\_channel=JosephLizier},
  keywords = {IDTxl}
}

@article{lutcke2013inference,
  title = {Inference of Neuronal Network Spike Dynamics and Topology from Calcium Imaging Data},
  author = {L{\"u}tcke, Henry and Gerhard, Felipe and Zenke, Friedemann and Gerstner, Wulfram and Helmchen, Fritjof},
  year = {2013},
  journal = {Frontiers in Neural Circuits},
  volume = {7},
  issn = {1662-5110},
  abstract = {Two-photon calcium imaging enables functional analysis of neuronal circuits by inferring action potential (AP) occurrence (``spike trains'') from cellular fluorescence signals. It remains unclear how experimental parameters such as signal-to-noise ratio (SNR) and acquisition rate affect spike inference and whether additional information about network structure can be extracted. Here we present a simulation framework for quantitatively assessing how well spike dynamics and network topology can be inferred from noisy calcium imaging data. For simulated AP-evoked calcium transients in neocortical pyramidal cells, we analyzed the quality of spike inference as a function of SNR and data acquisition rate using a recently introduced peeling algorithm. Given experimentally attainable values of SNR and acquisition rate, neural spike trains could be reconstructed accurately and with up to millisecond precision. We then applied statistical neuronal network models to explore how remaining uncertainties in spike inference affect estimates of network connectivity and topological features of network organization. We define the experimental conditions suitable for inferring whether the network has a scale-free structure and determine how well hub neurons can be identified. Our findings provide a benchmark for future calcium imaging studies that aim to reliably infer neuronal network properties.}
}

@incollection{maathuis2016review,
  title = {A Review of Some Recent Advances in Causal Inference},
  booktitle = {Handbook of {{Big Data}}},
  author = {Maathuis, Marloes H. and Nandy, Preetam},
  year = {2016},
  pages = {387--408},
  isbn = {978-0-429-16298-5}
}

@article{magransdeabril2018connectivity,
  title = {Connectivity Inference from Neural Recording Data: {{Challenges}}, Mathematical Bases and Research Directions},
  shorttitle = {Connectivity Inference from Neural Recording Data},
  author = {{Magrans de Abril}, Ildefons and Yoshimoto, Junichiro and Doya, Kenji},
  year = {2018},
  month = jun,
  journal = {Neural Networks},
  volume = {102},
  pages = {120--137},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.02.016},
  abstract = {This article presents a review of computational methods for connectivity inference from neural activity data derived from multi-electrode recordings or fluorescence imaging. We first identify biophysical and technical challenges in connectivity inference along the data processing pipeline. We then review connectivity inference methods based on two major mathematical foundations, namely, descriptive model-free approaches and generative model-based approaches. We investigate representative studies in both categories and clarify which challenges have been addressed by which method. We further identify critical open issues and possible research directions.},
  keywords = {+++,Calcium fluorescence imaging,Connectivity inference,Effective connectivity,Functional connectivity,Multi-electrode recording,review}
}

@article{mah2014human,
  title = {Human Brain Lesion-Deficit Inference Remapped},
  author = {Mah, Yee-Haur and Husain, Masud and Rees, Geraint and Nachev, Parashkev},
  year = {2014},
  month = sep,
  journal = {Brain: A Journal of Neurology},
  volume = {137},
  number = {Pt 9},
  pages = {2522--2531},
  issn = {1460-2156},
  doi = {10.1093/brain/awu164},
  abstract = {Our knowledge of the anatomical organization of the human brain in health and disease draws heavily on the study of patients with focal brain lesions. Historically the first method of mapping brain function, it is still potentially the most powerful, establishing the necessity of any putative neural substrate for a given function or deficit. Great inferential power, however, carries a crucial vulnerability: without stronger alternatives any consistent error cannot be easily detected. A hitherto unexamined source of such error is the structure of the high-dimensional distribution of patterns of focal damage, especially in ischaemic injury-the commonest aetiology in lesion-deficit studies-where the anatomy is naturally shaped by the architecture of the vascular tree. This distribution is so complex that analysis of lesion data sets of conventional size cannot illuminate its structure, leaving us in the dark about the presence or absence of such error. To examine this crucial question we assembled the largest known set of focal brain lesions (n = 581), derived from unselected patients with acute ischaemic injury (mean age = 62.3 years, standard deviation = 17.8, male:female ratio = 0.547), visualized with diffusion-weighted magnetic resonance imaging, and processed with validated automated lesion segmentation routines. High-dimensional analysis of this data revealed a hidden bias within the multivariate patterns of damage that will consistently distort lesion-deficit maps, displacing inferred critical regions from their true locations, in a manner opaque to replication. Quantifying the size of this mislocalization demonstrates that past lesion-deficit relationships estimated with conventional inferential methodology are likely to be significantly displaced, by a magnitude dependent on the unknown underlying lesion-deficit relationship itself. Past studies therefore cannot be retrospectively corrected, except by new knowledge that would render them redundant. Positively, we show that novel machine learning techniques employing high-dimensional inference can nonetheless accurately converge on the true locus. We conclude that current inferences about human brain function and deficits based on lesion mapping must be re-evaluated with methodology that adequately captures the high-dimensional structure of lesion data.},
  pmcid = {PMC4132645},
  pmid = {24974384},
  keywords = {Adult,Aged,Aged; 80 and over,Brain,Brain Mapping,Female,focal brain injury,Humans,Imaging; Three-Dimensional,ischaemic brain injury,lesion-deficit inference,Male,Middle Aged}
}

@misc{matheusfacurecausal,
  title = {Causal {{Inference}} for {{The Brave}} and {{True}} \textemdash{} {{Causal Inference}} for the {{Brave}} and {{True}}},
  author = {{Matheus Facure}},
  howpublished = {https://matheusfacure.github.io/python-causality-handbook/landing-page.html},
  keywords = {++,high-level}
}

@article{melssen1987detection,
  title = {Detection and Estimation of Neural Connectivity Based on Crosscorrelation Analysis},
  author = {Melssen, W. J. and Epping, W. J. M.},
  year = {1987},
  journal = {Biological Cybernetics},
  volume = {57},
  number = {6},
  pages = {403--414},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00354985},
  abstract = {Crosscorrelation analysis of simultaneously recorded activity of pairs of neurons is a common tool to infer functional neural connectivity. The adequacy of crosscorrelation procedures to detect and estimate neural connectivity has been investigated by means of computer simulations of small networks composed of fairly realistic modelneurons. If the mean interval of neural firings is much larger than the duration of postsynaptic potentials, which will be the case in many central brain areas excitatory connections are easier to detect than inhibitory ones. On the other hand, inhibitory connections are revealed better if the mean firing interval is much smaller than post-synaptic potential duration. In general the effects of external stimuli and the effects of neural connectivity do not add linearly. Furthermore, neurons may exhibit a certain degree of timelock to the stimulus. For these reasons the commonly applied "shift predictor" procedure to separate stimulus and neural effects appears to be of limited value. In case of parallel direct and indirect neural pathways between two neurons crosscorrelation analysis does not estimate the direct connection but instead an effective connectivity, which reflects the combined influences of the parallel pathways.}
}

@article{miniussi2013modelling,
  title = {Modelling {{Non-Invasive Brain Stimulation}} in {{Cognitive Neuroscience}}.},
  author = {Miniussi, Carlo and Harris, Justin and Ruzzoli, Manuela},
  year = {2013},
  month = jul,
  journal = {Neuroscience and biobehavioral reviews},
  volume = {37},
  pages = {1702--1712.},
  doi = {10.1016/j.neubiorev.2013.06.014},
  abstract = {Non-invasive brain stimulation (NIBS) is a method for the study of cognitive function that is quickly gaining popularity. It bypasses the correlative approaches of other imaging techniques, making it possible to establish a causal relationship between cognitive processes and the functioning of specific brain areas. Like lesion studies, NIBS can provide information about where a particular process occurs. However, NIBS offers the opportunity to study brain mechanisms beyond process localisation, providing information about when activity in a given brain region is involved in a cognitive process, and even how it is involved. When using NIBS to explore cognitive processes, it is important to understand not only how NIBS functions but also the functioning of the neural structures themselves. We know that NIBS techniques have the potential to transiently influence behaviour by altering neuronal activity, which may have facilitatory or inhibitory behavioural effects, and these alterations can be used to understand how the brain works. Given that NIBS necessarily involves the relatively indiscriminate activation of large numbers of neurons, its impact on a neural system can be easily understood as modulation of neural activity that changes the relation between noise and signal. In this review, we describe the mutual interactions between NIBS and brain activity and provide an updated and precise perspective on the theoretical frameworks of NIBS and their impact on cognitive neuroscience. By transitioning our discussion from one aspect (NIBS) to the other (cognition), we aim to provide insights to guide future research.}
}

@article{moore1970statistical,
  title = {Statistical {{Signs}} of {{Synaptic Interaction}} in {{Neurons}}},
  author = {Moore, George P. and Segundo, Jose P. and Perkel, Donald H. and Levitan, Herbert},
  year = {1970},
  month = sep,
  journal = {Biophysical Journal},
  volume = {10},
  number = {9},
  pages = {876--900},
  issn = {0006-3495},
  doi = {10.1016/S0006-3495(70)86341-X},
  abstract = {The influence of basic open-loop synaptic connections on the firing of simultaneously recorded neurons has been investigated with auto- and cross-correlation histograms, using experimental records and computer simulations. The basic connections examined were direct synaptic excitation, direct synaptic inhibition, and shared synaptic input. Each type of synaptic connection produces certain characteristic features in the cross-correlogram depending on the properties of the synapse and statistical features in the firing pattern of each neuron. Thus, empirically derived cross-correlation measures can be interpreted in terms of the underlying physiological mechanisms. Their potential uses and limitations in the detection and identification of synaptic connections between neurons whose extracellularly recorded spike trains are available are discussed.}
}

@article{moreno-bote2015causal,
  title = {Causal {{Inference}} and {{Explaining Away}} in a {{Spiking Network}}},
  author = {{Moreno-Bote}, Rub{\'e}n and Drugowitsch, Jan},
  year = {2015},
  month = dec,
  journal = {Scientific Reports},
  volume = {5},
  number = {1},
  pages = {17531},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep17531},
  abstract = {While the brain uses spiking neurons for communication, theoretical research on brain computations has mostly focused on non-spiking networks. The nature of spike-based algorithms that achieve complex computations, such as object probabilistic inference, is largely unknown. Here we demonstrate that a family of high-dimensional quadratic optimization problems with non-negativity constraints can be solved exactly and efficiently by a network of spiking neurons. The network naturally imposes the non-negativity of causal contributions that is fundamental to causal inference and uses simple operations, such as linear synapses with realistic time constants and neural spike generation and reset non-linearities. The network infers the set of most likely causes from an observation using explaining away, which is dynamically implemented by spike-based, tuned inhibition. The algorithm performs remarkably well even when the network intrinsically generates variable spike trains, the timing of spikes is scrambled by external sources of noise, or the network is mistuned. This type of network might underlie tasks such as odor identification and classification.},
  copyright = {2015 The Author(s)},
  keywords = {Biophysical models,Network models,Neural encoding},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Biophysical models;Network models;Neural encoding Subject\_term\_id: biophysical-models;network-models;neural-encoding}
}

@article{mulansky2016pyspike,
  title = {{{PySpike}}\textemdash{{A Python}} Library for Analyzing Spike Train Synchrony},
  author = {Mulansky, Mario and Kreuz, Thomas},
  year = {2016},
  journal = {SoftwareX},
  volume = {5},
  pages = {183--189},
  issn = {23527110},
  doi = {10.1016/j.softx.2016.07.006}
}

@article{murphy1985cross,
  title = {Cross {{Correlation Studies}} in {{Primate Motor Cortex}}: {{Synaptic Interaction}} and {{Shared Input}}},
  shorttitle = {Cross {{Correlation Studies}} in {{Primate Motor Cortex}}},
  author = {Murphy, John T. and Kwan, Hon C. and Wong, Yiu C.},
  year = {1985},
  month = feb,
  journal = {Canadian Journal of Neurological Sciences / Journal Canadien des Sciences Neurologiques},
  volume = {12},
  number = {1},
  pages = {11--23},
  issn = {0317-1671, 2057-0155},
  doi = {10.1017/S0317167100046527},
  abstract = {Awake, unrestrained monkeys were trained to reach out with the forelimb and touch a button. Extracellular spike trains were recorded from pairs of neurons in contralateral precentral cortex with the same or separate microelectrodes. The neurons were located in the same or different functional columns as defined by intracortical microstimulation and passive sensory stimulation. Cross correlation analysis showed patterns consistent with synaptic excitation and/or inhibition between members of the cell pairs during the voluntary movement. The strength of correlation was inversely related to distance between columns, with the strongest correlations found between cells within the same column. Inhibitory correlations were virtually restricted to cell pairs within a single column. Temporal analysis showed that direct synaptic interaction and shared input patterns could be clearly distinguished in this physiologic setting. Spatial analysis indicated that shared input was concentrated among columns in the same and adjacent joint controlling zones as well as within a single column. No directional preference of shared input was present, a finding which was consistent with the observed nested organization of the forelimb area.}
}

@article{narayanan2009methods,
  title = {Methods for Studying Functional Interactions among Neuronal Populations},
  author = {Narayanan, Nandakumar S. and Laubach, Mark},
  year = {2009},
  journal = {Methods in molecular biology (Clifton, N.J.)},
  volume = {489},
  pages = {10.1007/978-1-59745-543-5\_7},
  issn = {1064-3745},
  doi = {10.1007/978-1-59745-543-5_7},
  abstract = {How do populations of neurons work together to control behavior? To study this issue, our group simultaneously records from populations of neurons across multiple electrodes in multiple brain regions during operant behavior. In this chapter, we describe methods for quantifying the relationship between neuronal population activity and performance of operant behavioral tasks. We describe statistical techniques, based on time- and trial-shuffling, that can establish the significance of correlations between multiple and simultaneously recorded spike trains. Then, we describe several approaches to studying functional interactions between neurons, including principal component analysis, cross-correlation analysis, analyses of rate correlations, and analyses of shared predictive information. Finally, we compare these techniques using a sample dataset and discuss how the combined used of these techniques can lead to novel insights regarding neuronal interactions during behavior.},
  pmcid = {PMC3856913},
  pmid = {18839091},
  keywords = {++,MATLAB,shift predictor}
}

@article{newman2015optogenetic,
  title = {Optogenetic Feedback Control of Neural Activity},
  author = {Newman, Jonathan P and Fong, Ming-fai and Millard, Daniel C and Whitmire, Clarissa J and Stanley, Garrett B and Potter, Steve M},
  editor = {Bartos, Marlene},
  year = {2015},
  month = jul,
  journal = {eLife},
  volume = {4},
  pages = {e07192},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.07192},
  abstract = {Optogenetic techniques enable precise excitation and inhibition of firing in specified neuronal populations and artifact-free recording of firing activity. Several studies have suggested that optical stimulation provides the precision and dynamic range requisite for closed-loop neuronal control, but no approach yet permits feedback control of neuronal firing. Here we present the `optoclamp', a feedback control technology that provides continuous, real-time adjustments of bidirectional optical stimulation in order to lock spiking activity at specified targets over timescales ranging from seconds to days. We demonstrate how this system can be used to decouple neuronal firing levels from ongoing changes in network excitability due to multi-hour periods of glutamatergic or GABAergic neurotransmission blockade in vitro as well as impinging vibrissal sensory drive in vivo. This technology enables continuous, precise optical control of firing in neuronal populations in order to disentangle causally related variables of circuit activation in a physiologically and ethologically relevant manner.},
  keywords = {closed loop,cultured cortical network,optoclamp,optogenetics,real-time,sensory thalamus}
}

@article{nigam2016richclub,
  title = {Rich-{{Club Organization}} in {{Effective Connectivity}} among {{Cortical Neurons}}},
  author = {Nigam, Sunny and Shimono, Masanori and Ito, Shinya and Yeh, Fang-Chin and Timme, Nicholas and Myroshnychenko, Maxym and Lapish, Christopher C. and Tosi, Zachary and Hottowy, Pawel and Smith, Wesley C. and Masmanidis, Sotiris C. and Litke, Alan M. and Sporns, Olaf and Beggs, John M.},
  year = {2016},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {36},
  number = {3},
  pages = {670--684},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2177-15.2016},
  abstract = {{$<$}p{$>$}The performance of complex networks, like the brain, depends on how effectively their elements communicate. Despite the importance of communication, it is virtually unknown how information is transferred in local cortical networks, consisting of hundreds of closely spaced neurons. To address this, it is important to record simultaneously from hundreds of neurons at a spacing that matches typical axonal connection distances, and at a temporal resolution that matches synaptic delays. We used a 512-electrode array (60 {$\mu$}m spacing) to record spontaneous activity at 20 kHz from up to 500 neurons simultaneously in slice cultures of mouse somatosensory cortex for 1 h at a time. We applied a previously validated version of transfer entropy to quantify information transfer. Similar to \emph{in vivo} reports, we found an approximately lognormal distribution of firing rates. Pairwise information transfer strengths also were nearly lognormally distributed, similar to reports of synaptic strengths. Some neurons transferred and received much more information than others, which is consistent with previous predictions. Neurons with the highest outgoing and incoming information transfer were more strongly connected to each other than chance, thus forming a ``rich club.'' We found similar results in networks recorded \emph{in vivo} from rodent cortex, suggesting the generality of these findings. A rich-club structure has been found previously in large-scale human brain networks and is thought to facilitate communication between cortical regions. The discovery of a small, but information-rich, subset of neurons within cortical regions suggests that this population will play a vital role in communication, learning, and memory.{$<$}/p{$><$}p{$>$}\textbf{SIGNIFICANCE STATEMENT} Many studies have focused on communication networks between cortical brain regions. In contrast, very few studies have examined communication networks within a cortical region. This is the first study to combine such a large number of neurons (several hundred at a time) with such high temporal resolution (so we can know the direction of communication between neurons) for mapping networks within cortex. We found that information was not transferred equally through all neurons. Instead, {$\sim$}70\% of the information passed through only 20\% of the neurons. Network models suggest that this highly concentrated pattern of information transfer would be both efficient and robust to damage. Therefore, this work may help in understanding how the cortex processes information and responds to neurodegenerative diseases.{$<$}/p{$>$}},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2016 Nigam et al.. This article is freely available online through the J Neurosci Author Open Choice option.},
  pmid = {26791200}
}

@article{novelli2019largescale,
  title = {Large-Scale Directed Network Inference with Multivariate Transfer Entropy and Hierarchical Statistical Testing},
  author = {Novelli, Leonardo and Wollstadt, Patricia and Mediano, Pedro and Wibral, Michael and Lizier, Joseph T.},
  year = {2019},
  month = jul,
  journal = {Network Neuroscience},
  volume = {3},
  number = {3},
  pages = {827--847},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00092},
  abstract = {Network inference algorithms are valuable tools for the study of large-scale neuroimaging datasets. Multivariate transfer entropy is well suited for this task, being a model-free measure that captures nonlinear and lagged dependencies between time series to infer a minimal directed network model. Greedy algorithms have been proposed to efficiently deal with high-dimensional datasets while avoiding redundant inferences and capturing synergistic effects. However, multiple statistical comparisons may inflate the false positive rate and are computationally demanding, which limited the size of previous validation studies. The algorithm we present\textemdash as implemented in the IDTxl open-source software\textemdash addresses these challenges by employing hierarchical statistical tests to control the family-wise error rate and to allow for efficient parallelization. The method was validated on synthetic datasets involving random networks of increasing size (up to 100 nodes), for both linear and nonlinear dynamics. The performance increased with the length of the time series, reaching consistently high precision, recall, and specificity (\&gt;98\% on average) for 10,000 time samples. Varying the statistical significance threshold showed a more favorable precision-recall trade-off for longer time series. Both the network size and the sample size are one order of magnitude larger than previously demonstrated, showing feasibility for typical EEG and magnetoencephalography experiments.},
  keywords = {IDTxl}
}

@article{novelli2020deriving,
  title = {Deriving Pairwise Transfer Entropy from Network Structure and Motifs},
  author = {Novelli, Leonardo and Atay, Fatihcan M. and Jost, J{\"u}rgen and Lizier, Joseph T.},
  year = {2020},
  month = apr,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {476},
  number = {2236},
  pages = {20190779},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.2019.0779},
  abstract = {Transfer entropy (TE) is an established method for quantifying directed statistical dependencies in neuroimaging and complex systems datasets. The pairwise (or bivariate) TE from a source to a target node in a network does not depend solely on the local source-target link weight, but on the wider network structure that the link is embedded in. This relationship is studied using a discrete-time linearly coupled Gaussian model, which allows us to derive the TE for each link from the network topology. It is shown analytically that the dependence on the directed link weight is only a first approximation, valid for weak coupling. More generally, the TE increases with the in-degree of the source and decreases with the in-degree of the target, indicating an asymmetry of information transfer between hubs and low-degree nodes. In addition, the TE is directly proportional to weighted motif counts involving common parents or multiple walks from the source to the target, which are more abundant in networks with a high clustering coefficient than in random networks. Our findings also apply to Granger causality, which is equivalent to TE for Gaussian variables. Moreover, similar empirical results on random Boolean networks suggest that the dependence of the TE on the in-degree extends to nonlinear dynamics.},
  keywords = {connectome,information theory,motifs,network inference,transfer entropy}
}

@article{novelli2021inferring,
  title = {Inferring Network Properties from Time Series Using Transfer Entropy and Mutual Information: {{Validation}} of Multivariate versus Bivariate Approaches},
  shorttitle = {Inferring Network Properties from Time Series Using Transfer Entropy and Mutual Information},
  author = {Novelli, Leonardo and Lizier, Joseph T.},
  year = {2021},
  month = may,
  journal = {Network Neuroscience},
  volume = {5},
  number = {2},
  pages = {373--404},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00178},
  abstract = {Functional and effective networks inferred from time series are at the core of network neuroscience. Interpreting properties of these networks requires inferred network models to reflect key underlying structural features. However, even a few spurious links can severely distort network measures, posing a challenge for functional connectomes. We study the extent to which micro- and macroscopic properties of underlying networks can be inferred by algorithms based on mutual information and bivariate/multivariate transfer entropy. The validation is performed on two macaque connectomes and on synthetic networks with various topologies (regular lattice, small-world, random, scale-free, modular). Simulations are based on a neural mass model and on autoregressive dynamics (employing Gaussian estimators for direct comparison to functional connectivity and Granger causality). We find that multivariate transfer entropy captures key properties of all network structures for longer time series. Bivariate methods can achieve higher recall (sensitivity) for shorter time series but are unable to control false positives (lower specificity) as available data increases. This leads to overestimated clustering, small-world, and rich-club coefficients, underestimated shortest path lengths and hub centrality, and fattened degree distribution tails. Caution should therefore be used when interpreting network properties of functional connectomes obtained via correlation or pairwise statistical dependence measures, rather than more holistic (yet data-hungry) multivariate models.We compare bivariate and multivariate methods for inferring networks from time series, which are generated using a neural mass model and autoregressive dynamics. We assess their ability to reproduce key properties of the underlying structural network. Validation is performed on two macaque connectomes and on synthetic networks with various topologies (regular lattice, small-world, random, scale-free, modular). Even a few spurious links can severely bias key network properties. Multivariate transfer entropy performs best on all topologies for longer time series.}
}

@incollection{nowak2000cross,
  title = {Cross Correlograms for Neuronal Spike Trains. {{Different}} Types of Temporal Correlation in Neocortex, Their Origin and Significance},
  author = {Nowak, Lionel and J, Bullier},
  year = {2000},
  month = jul,
  pages = {53--96}
}

@article{orlandi2014transfer,
  title = {Transfer {{Entropy Reconstruction}} and {{Labeling}} of {{Neuronal Connections}} from {{Simulated Calcium Imaging}}},
  author = {Orlandi, Javier G. and Stetter, Olav and Soriano, Jordi and Geisel, Theo and Battaglia, Demian},
  year = {2014},
  month = jun,
  journal = {PLOS ONE},
  volume = {9},
  number = {6},
  pages = {e98842},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0098842},
  abstract = {Neuronal dynamics are fundamentally constrained by the underlying structural network architecture, yet much of the details of this synaptic connectivity are still unknown even in neuronal cultures in vitro. Here we extend a previous approach based on information theory, the Generalized Transfer Entropy, to the reconstruction of connectivity of simulated neuronal networks of both excitatory and inhibitory neurons. We show that, due to the model-free nature of the developed measure, both kinds of connections can be reliably inferred if the average firing rate between synchronous burst events exceeds a small minimum frequency. Furthermore, we suggest, based on systematic simulations, that even lower spontaneous inter-burst rates could be raised to meet the requirements of our reconstruction algorithm by applying a weak spatially homogeneous stimulation to the entire network. By combining multiple recordings of the same in silico network before and after pharmacologically blocking inhibitory synaptic transmission, we show then how it becomes possible to infer with high confidence the excitatory or inhibitory nature of each individual neuron.},
  keywords = {Action potentials,Calcium signaling,Entropy,Fluorescence imaging,Neural networks,Neuronal plasticity,Neurons,Neurotransmitters,stimulation}
}

@article{ostojic2009how,
  title = {How {{Connectivity}}, {{Background Activity}}, and {{Synaptic Properties Shape}} the {{Cross-Correlation}} between {{Spike Trains}}},
  author = {Ostojic, Srdjan and Brunel, Nicolas and Hakim, Vincent},
  year = {2009},
  month = aug,
  journal = {Journal of Neuroscience},
  volume = {29},
  number = {33},
  pages = {10234--10253},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1275-09.2009},
  abstract = {Functional interactions between neurons in vivo are often quantified by cross-correlation functions (CCFs) between their spike trains. It is therefore essential to understand quantitatively how CCFs are shaped by different factors, such as connectivity, synaptic parameters, and background activity. Here, we study the CCF between two neurons using analytical calculations and numerical simulations. We quantify the role of synaptic parameters, such as peak conductance, decay time, and reversal potential, and analyze how various patterns of connectivity influence CCF shapes. In particular, we find that the symmetry of the CCF distinguishes in general, but not always, the case of shared inputs between two neurons from the case in which they are directly synaptically connected. We systematically examine the influence of background synaptic inputs from the surrounding network that set the baseline firing statistics of the neurons and modulate their response properties. We find that variations in the background noise modify the amplitude of the cross-correlation function as strongly as variations of synaptic strength. In particular, we show that the postsynaptic neuron spiking regularity has a pronounced influence on CCF amplitude. This suggests an efficient and flexible mechanism for modulating functional interactions.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2009 Society for Neuroscience 0270-6474/09/2910234-20\$15.00/0},
  pmid = {19692598},
  keywords = {+++,LIF,parameter sweep}
}

@article{paiva2010comparison,
  title = {A Comparison of Binless Spike Train Measures},
  author = {Paiva, Ant{\'o}nio R. C. and Park, Il and Pr{\'i}ncipe, Jos{\'e} C.},
  year = {2010},
  month = apr,
  journal = {Neural Computing and Applications},
  volume = {19},
  number = {3},
  pages = {405--419},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-009-0307-6},
  abstract = {Several binless spike train measures which avoid the limitations of binning have been recently been proposed in the literature. This paper presents a systematic comparison of these measures in three simulated paradigms designed to address specific situations of interest in spike train analysis where the relevant feature may be in the form of firing rate, firing rate modulations and/or synchrony. The measures are first disseminated and extended for ease of comparison. It is also discussed how the measures can be used to measure dissimilarity in spike trains' firing rate despite their explicit formulation for synchrony.}
}

@article{pastore2018identification,
  title = {Identification of Excitatory-Inhibitory Links and Network Topology in Large-Scale Neuronal Assemblies from Multi-Electrode Recordings},
  author = {Pastore, Vito Paolo and Massobrio, Paolo and Godjoski, Aleksandar and Martinoia, Sergio},
  year = {2018},
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {14},
  number = {8},
  pages = {e1006381},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006381},
  abstract = {Functional-effective connectivity and network topology are nowadays key issues for studying brain physiological functions and pathologies. Inferring neuronal connectivity from electrophysiological recordings presents open challenges and unsolved problems. In this work, we present a cross-correlation based method for reliably estimating not only excitatory but also inhibitory links, by analyzing multi-unit spike activity from large-scale neuronal networks. The method is validated by means of realistic simulations of large-scale neuronal populations. New results related to functional connectivity estimation and network topology identification obtained by experimental electrophysiological recordings from high-density and large-scale (i.e., 4096 electrodes) microtransducer arrays coupled to in vitro neural populations are presented. Specifically, we show that: (i) functional inhibitory connections are accurately identified in in vitro cortical networks, providing that a reasonable firing rate and recording length are achieved; (ii) small-world topology, with scale-free and rich-club features are reliably obtained, on condition that a minimum number of active recording sites are available. The method and procedure can be directly extended and applied to in vivo multi-units brain activity recordings.},
  keywords = {++,Action potentials,Algorithms,Distribution curves,Electrode recording,Electrophysiology,Network analysis,Neural networks,Neurons}
}

@article{peach2021hcga,
  title = {{{HCGA}}: {{Highly}} Comparative Graph Analysis for Network Phenotyping},
  shorttitle = {{{HCGA}}},
  author = {Peach, Robert L. and Arnaudon, Alexis and Schmidt, Julia A. and Palasciano, Henry A. and Bernier, Nathan R. and Jelfs, Kim E. and Yaliraki, Sophia N. and Barahona, Mauricio},
  year = {2021},
  month = apr,
  journal = {Patterns},
  volume = {2},
  number = {4},
  pages = {100227},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2021.100227},
  abstract = {Networks are widely used as mathematical models of complex systems across many scientific disciplines. Decades of work have produced a vast corpus of research characterizing the topological, combinatorial, statistical, and spectral properties of graphs. Each graph property can be thought of as a feature that captures important (and sometimes overlapping) characteristics of a network. In this paper, we introduce HCGA, a framework for highly comparative analysis of graph datasets that computes several thousands of graph features from any given network. HCGA also offers a suite of statistical learning and data analysis tools for automated identification and selection of important and interpretable features underpinning the characterization of graph datasets. We show that HCGA outperforms other methodologies on supervised classification tasks on benchmark datasets while retaining the interpretability of network features. We exemplify HCGA by predicting the charge transfer in organic semiconductors and clustering a dataset of neuronal morphology images.},
  keywords = {feature extraction,graph classification,graph regression,graph theory,high-throughput phenotyping,machine learning,networks}
}

@book{pearl2009causality,
  title = {Causality: Models, Reasoning, and Inference},
  author = {Pearl, Judea},
  year = {2009},
  edition = {Second},
  publisher = {{Cambridge University Press}},
  abstract = {Introduction to probabilities, graphs, and causal models -- Theory of inferred causation -- Causal diagrams and the identification of causal effects -- Actions, plans, and direct effects -- Causality and structural models in social science and economics -- Simpson's paradox, confounding, and collapsibility -- Logic of structure-based counterfactuals -- Imperfect experiments: bounding effects and counterfactuals -- Probability of causation: interpretation and identification -- The actual cause -- Reflections, elaborations, and discussions with readers -- The art and science of cause and effect.},
  collaborator = {{ebrary}, Inc},
  keywords = {Causation.,Probabilities.}
}

@article{penfield1937somatic,
  title = {Somatic Motor and Sensory Representation in the Cerebral Cortex of Man as Studied by Electrical Stimulation},
  author = {Penfield, W. and Boldrey, E.},
  year = {1937},
  journal = {Brain: A Journal of Neurology},
  volume = {60},
  pages = {389--443},
  publisher = {{Oxford University Press}},
  address = {{United Kingdom}},
  issn = {1460-2156},
  doi = {10.1093/brain/60.4.389},
  abstract = {Summary and complete analysis of the results of electrical stimulation of various regions of the cortex in 126 patients operated under local anesthesia. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@book{penfield1950cerebral,
  title = {The Cerebral Cortex of Man; a Clinical Study of Localization of Function.},
  author = {Penfield, Wilder and Rasmussen, Theodore},
  year = {1950},
  series = {The Cerebral Cortex of Man; a Clinical Study of Localization of Function.},
  pages = {xv, 248},
  publisher = {{Macmillan}},
  address = {{Oxford,  England}},
  abstract = {This book draws on the extensive material of both authors derived from over 400 craniotomies performed under local anesthesia. Evidence was obtained from the effects of cortical stimulation (often in the patients' own words), the effects of cortical ablation, and the effects of epileptic seizure discharge. Separate chapters deal in detail with technique, and various sensorimotor representations of the body in the cortex. The chapter on memory, sensory perception and dreams reveals the importance of temporal lobe cortex. The chapter on excision of cortical regions discloses (with notable exceptions) a surprising amount of dispensable cortex. The concluding chapter integrates and occasionally extends by hypothesis the preceding data, with special emphasis on the elaborative and integrative processes of the cerebral cortex and its intimate relationship to the diencephalon. 84-item bibliography. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{peng2021spatially,
  title = {Spatially Structured Inhibition Defined by Polarized Parvalbumin Interneuron Axons Promotes Head Direction Tuning},
  author = {Peng, Yangfan and Barreda Tomas, Federico J. and Pfeiffer, Paul and Drangmeister, Moritz and Schreiber, Susanne and Vida, Imre and Geiger, J{\"o}rg R.P.},
  year = {2021},
  month = jun,
  journal = {Science Advances},
  volume = {7},
  number = {25},
  pages = {eabg4693},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abg4693},
  abstract = {Anatomical organization of an inhibitory cortical microcircuit suggests an intrinsic compass supporting spatial navigation., In cortical microcircuits, it is generally assumed that fast-spiking parvalbumin interneurons mediate dense and nonselective inhibition. Some reports indicate sparse and structured inhibitory connectivity, but the computational relevance and the underlying spatial organization remain unresolved. In the rat superficial presubiculum, we find that inhibition by fast-spiking interneurons is organized in the form of a dominant super-reciprocal microcircuit motif where multiple pyramidal cells recurrently inhibit each other via a single interneuron. Multineuron recordings and subsequent 3D reconstructions and analysis further show that this nonrandom connectivity arises from an asymmetric, polarized morphology of fast-spiking interneuron axons, which individually cover different directions in the same volume. Network simulations assuming topographically organized input demonstrate that such polarized inhibition can improve head direction tuning of pyramidal cells in comparison to a ``blanket of inhibition.'' We propose that structured inhibition based on asymmetrical axons is an overarching spatial connectivity principle for tailored computation across brain regions.},
  pmcid = {PMC8208710},
  pmid = {34134979}
}

@article{peron2020recurrent,
  title = {Recurrent Interactions in Local Cortical Circuits},
  author = {Peron, Simon and Pancholi, Ravi and Voelcker, Bettina and Wittenbach, Jason D. and {\'O}lafsd{\'o}ttir, H. Freyja and Freeman, Jeremy and Svoboda, Karel},
  year = {2020},
  month = mar,
  journal = {Nature},
  volume = {579},
  number = {7798},
  pages = {256--259},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2062-x},
  abstract = {Most cortical synapses are local and excitatory. Local recurrent circuits could implement amplification, allowing pattern completion and other computations1\textendash 4. Cortical circuits contain subnetworks that consist of neurons with similar receptive fields and increased connectivity relative to the network average5,6. Cortical neurons that encode different types of information are spatially intermingled and distributed over large brain volumes5\textendash 7, and this complexity has hindered attempts to probe the function of these subnetworks by perturbing them individually8. Here we use computational modelling, optical recordings and manipulations to probe the function of recurrent coupling in layer 2/3 of the mouse vibrissal somatosensory cortex during active tactile discrimination. A neural circuit model of layer 2/3 revealed that recurrent excitation enhances sensory signals by amplification, but only for subnetworks with increased connectivity. Model networks with high amplification were sensitive to damage: loss of a few members of the subnetwork degraded stimulus encoding. We tested this prediction by mapping neuronal selectivity7 and photoablating9,10 neurons with specific selectivity. Ablation of a small proportion of layer 2/3 neurons (10\textendash 20, less than 5\% of the total) representing touch markedly reduced responses in the spared touch representation, but not in other representations. Ablations most strongly affected neurons with stimulus responses that were similar to those of the ablated population, which is also consistent with network models. Recurrence among cortical neurons with similar selectivity therefore drives input-specific amplification during behaviour.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  keywords = {Barrel cortex,Brian2,Neural circuits,Sensory processing},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Barrel cortex;Neural circuits;Sensory processing Subject\_term\_id: barrel-cortex;neural-circuit;sensory-processing}
}

@article{peters2016causal,
  title = {Causal Inference by Using Invariant Prediction: Identification and Confidence Intervals},
  shorttitle = {Causal Inference by Using Invariant Prediction},
  author = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
  year = {2016},
  month = nov,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {78},
  number = {5},
  pages = {947--1012},
  issn = {13697412},
  doi = {10.1111/rssb.12167}
}

@article{peters2020causal,
  title = {Causal Models for Dynamical Systems},
  author = {Peters, Jonas and Bauer, Stefan and Pfister, Niklas},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.06208 [math, stat]},
  eprint = {2001.06208},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {A probabilistic model describes a system in its observational state. In many situations, however, we are interested in the system's response under interventions. The class of structural causal models provides a language that allows us to model the behaviour under interventions. It can been taken as a starting point to answer a plethora of causal questions, including the identification of causal effects or causal structure learning. In this chapter, we provide a natural and straight-forward extension of this concept to dynamical systems, focusing on continuous time models. In particular, we introduce two types of causal kinetic models that differ in how the randomness enters into the model: it may either be considered as observational noise or as systematic driving noise. In both cases, we define interventions and therefore provide a possible starting point for causal inference. In this sense, the book chapter provides more questions than answers. The focus of the proposed causal kinetic models lies on the dynamics themselves rather than corresponding stationary distributions, for example. We believe that this is beneficial when the aim is to model the full time evolution of the system and data are measured at different time points. Under this focus, it is natural to consider interventions in the differential equations themselves.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Dynamical Systems,Statistics - Methodology}
}

@article{plesser2000noise,
  title = {Noise in {{Integrate-and-Fire Neurons}}: {{From Stochastic Input}} to {{Escape Rates}}},
  shorttitle = {Noise in {{Integrate-and-Fire Neurons}}},
  author = {Plesser, Hans E. and Gerstner, Wulfram},
  year = {2000},
  month = feb,
  journal = {Neural Computation},
  volume = {12},
  number = {2},
  pages = {367--384},
  issn = {0899-7667},
  doi = {10.1162/089976600300015835},
  abstract = {We analyze the effect of noise in integrate-and-fire neurons driven by time-dependent input and compare the diffusion approximation for the membrane potential to escape noise. It is shown that for time-dependent subthreshold input, diffusive noise can be replaced by escape noise with a hazard function that has a gaussian dependence on the distance between the (noise-free) membrane voltage and threshold. The approximation is improved if we add to the hazard function a probability current proportional to the derivative of the voltage. Stochastic resonance in response to periodic input occurs in both noise models and exhibits similar characteristics.}
}

@article{prinz2004dynamic,
  title = {The Dynamic Clamp Comes of Age},
  author = {Prinz, Astrid A. and Abbott, L. F. and Marder, Eve},
  year = {2004},
  month = apr,
  journal = {Trends in Neurosciences},
  volume = {27},
  number = {4},
  pages = {218--224},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2004.02.004},
  abstract = {The dynamic clamp uses computer simulation to introduce artificial membrane or synaptic conductances into biological neurons and to create hybrid circuits of real and model neurons. In the ten years since it was first developed, the dynamic clamp has become a widely used tool for the study of neural systems at the cellular and circuit levels. This review describes recent state-of-the-art implementations of the dynamic clamp and summarizes insights gained through its use, ranging from the role of voltage-dependent conductances in shaping neuronal activity to the effects of synaptic dynamics on network behavior and the impact of in vivo-like input on neuronal information processing.},
  pmid = {15046881},
  keywords = {Animals,Computer Simulation,Electrophysiology,Humans,Membrane Potentials,Models; Neurological,Neural Conduction,Neural Networks; Computer,Neurons,Patch-Clamp Techniques}
}

@misc{ramaswamy2019algorithmic,
  title = {An {{Algorithmic Barrier}} to {{Neural Circuit Understanding}}},
  author = {Ramaswamy, Venkatakrishnan},
  year = {2019},
  month = may,
  pages = {639724},
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/639724},
  abstract = {Neuroscience is witnessing extraordinary progress in experimental techniques, especially at the neural circuit level. These advances are largely aimed at enabling us to understand how neural circuit computations mechanistically cause behavior. Here, using techniques from Theoretical Computer Science, we examine how many experiments are needed to obtain such an empirical understanding. It is proved, mathematically, that establishing the most extensive notions of understanding need exponentially-many experiments in the number of neurons, in general, unless a widely-posited hypothesis about computation is false. Worse still, the feasible experimental regime is one where the number of experiments scales sub-linearly in the number of neurons, suggesting a fundamental impediment to such an understanding. Determining which notions of understanding are algorithmically tractable, thus, becomes an important new endeavor in Neuroscience.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.}
}

@article{ramot2022closedloop,
  title = {Closed-Loop Neuromodulation for Studying Spontaneous Activity and Causality},
  author = {Ramot, Michal and Martin, Alex},
  year = {2022},
  month = apr,
  journal = {Trends in Cognitive Sciences},
  volume = {26},
  number = {4},
  pages = {290--299},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2022.01.008},
  abstract = {Having established that spontaneous brain activity follows meaningful coactivation patterns and correlates with behavior, researchers have turned their attention to understanding its function and behavioral significance. We suggest closed-loop neuromodulation as a neural perturbation tool uniquely well suited for this task. Closed-loop neuromodulation has primarily been viewed as an interventionist tool to teach subjects to directly control their own brain activity. We examine an alternative operant conditioning model of closed-loop neuromodulation which, through implicit feedback, can manipulate spontaneous activity at the network level, without violating the spontaneous or endogenous nature of the signal, thereby providing a direct test of network causality.},
  keywords = {causality,implicit learning,neurofeedback,perturbation,spontaneous activity}
}

@article{real2017neural,
  title = {Neural Circuit Inference from Function to Structure},
  author = {Real, Esteban and Asari, Hiroki and Gollisch, Tim and Meister, Markus},
  year = {2017},
  month = jan,
  journal = {Current biology : CB},
  volume = {27},
  number = {2},
  pages = {189--198},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2016.11.040},
  abstract = {Advances in technology are opening new windows on the structural connectivity and functional dynamics of brain circuits. Quantitative frameworks are needed that integrate these data from anatomy and physiology. Here we present a modeling approach that creates such a link. The goal is to infer the structure of a neural circuit from sparse neural recordings, using partial knowledge of its anatomy as a regularizing constraint. We recorded visual responses from the output neurons of the retina, the ganglion cells. We then generated a systematic sequence of circuit models that represent retinal neurons and connections, and fitted them to the experimental data. The optimal models faithfully recapitulated the ganglion cell outputs. More importantly, they made predictions about dynamics and connectivity among unobserved neurons internal to the circuit, and these were subsequently confirmed by experiment. This circuit inference framework promises to facilitate the integration and understanding of big data in neuroscience., Neuroscience research faces a need to link big data on brain anatomy and physiology as high-throughput measurements become increasingly feasible. Real et al. present a modeling approach to provide such a link, and test it by inferring the structure of neural circuitry in the retina from sparse physiological recordings.},
  pmcid = {PMC5821114},
  pmid = {28065610}
}

@article{reid2019advancing,
  title = {Advancing Functional Connectivity Research from Association to Causation},
  author = {Reid, Andrew T. and Headley, Drew B. and Mill, Ravi D. and {Sanchez-Romero}, Ruben and Uddin, Lucina Q. and Marinazzo, Daniele and Lurie, Daniel J. and {Vald{\'e}s-Sosa}, Pedro A. and Hanson, Stephen Jos{\'e} and Biswal, Bharat B. and Calhoun, Vince and Poldrack, Russell A. and Cole, Michael W.},
  year = {2019},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {22},
  number = {11},
  pages = {1751--1760},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0510-4},
  abstract = {Cognition and behavior emerge from brain network interactions, such that investigating causal interactions should be central to the study of brain function. Approaches that characterize statistical associations among neural time series\textemdash functional connectivity (FC) methods\textemdash are likely a good starting point for estimating brain network interactions. Yet only a subset of FC methods (`effective connectivity') is explicitly designed to infer causal interactions from statistical associations. Here we incorporate best practices from diverse areas of FC research to illustrate how FC methods can be refined to improve inferences about neural mechanisms, with properties of causal neural interactions as a common ontology to facilitate cumulative progress across FC approaches. We further demonstrate how the most common FC measures (correlation and coherence) reduce the set of likely causal models, facilitating causal inferences despite major limitations. Alternative FC measures are suggested to immediately start improving causal inferences beyond these common FC measures.},
  copyright = {2019 Springer Nature America, Inc.},
  keywords = {Cognitive neuroscience,Network models,Neural circuits}
}

@article{runge2012escaping,
  title = {Escaping the {{Curse}} of {{Dimensionality}} in {{Estimating Multivariate Transfer Entropy}}},
  author = {Runge, Jakob and Heitzig, Jobst and Petoukhov, Vladimir and Kurths, J{\"u}rgen},
  year = {2012},
  month = jun,
  journal = {Physical Review Letters},
  volume = {108},
  number = {25},
  pages = {258701},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.108.258701}
}

@article{runge2012quantifying,
  title = {Quantifying {{Causal Coupling Strength}}: {{A Lag-specific Measure For Multivariate Time Series Related To Transfer Entropy}}},
  shorttitle = {Quantifying {{Causal Coupling Strength}}},
  author = {Runge, Jakob and Heitzig, Jobst and Marwan, Norbert and Kurths, J{\"u}rgen},
  year = {2012},
  month = dec,
  journal = {Physical Review E},
  volume = {86},
  number = {6},
  eprint = {1210.2748},
  eprinttype = {arxiv},
  pages = {061121},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.86.061121},
  abstract = {While it is an important problem to identify the existence of causal associations between two components of a multivariate time series, a topic addressed in Runge et al. (2012), it is even more important to assess the strength of their association in a meaningful way. In the present article we focus on the problem of defining a meaningful coupling strength using information theoretic measures and demonstrate the short-comings of the well-known mutual information and transfer entropy. Instead, we propose a certain time-delayed conditional mutual information, the momentary information transfer (MIT), as a measure of association that is general, causal and lag-specific, reflects a well interpretable notion of coupling strength and is practically computable. MIT is based on the fundamental concept of source entropy, which we utilize to yield a notion of coupling strength that is, compared to mutual information and transfer entropy, well interpretable, in that for many cases it solely depends on the interaction of the two components at a certain lag. In particular, MIT is thus in many cases able to exclude the misleading influence of autodependency within a process in an information-theoretic way. We formalize and prove this idea analytically and numerically for a general class of nonlinear stochastic processes and illustrate the potential of MIT on climatological data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning}
}

@article{runge2018causal,
  title = {Causal Network Reconstruction from Time Series: {{From}} Theoretical Assumptions to Practical Estimation},
  shorttitle = {Causal Network Reconstruction from Time Series},
  author = {Runge, J.},
  year = {2018},
  month = jul,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {28},
  number = {7},
  pages = {075310},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5025050}
}

@article{runge2019inferring,
  title = {Inferring Causation from Time Series in {{Earth}} System Sciences},
  author = {Runge, Jakob and Bathiany, Sebastian and Bollt, Erik and {Camps-Valls}, Gustau and Coumou, Dim and Deyle, Ethan and Glymour, Clark and Kretschmer, Marlene and Mahecha, Miguel D. and {Mu{\~n}oz-Mar{\'i}}, Jordi and {van Nes}, Egbert H. and Peters, Jonas and Quax, Rick and Reichstein, Markus and Scheffer, Marten and Sch{\"o}lkopf, Bernhard and Spirtes, Peter and Sugihara, George and Sun, Jie and Zhang, Kun and Zscheischler, Jakob},
  year = {2019},
  month = dec,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {2553},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10105-3}
}

@article{salinas2001correlated,
  title = {Correlated Neuronal Activity and the Flow of Neural Information},
  author = {Salinas, Emilio and Sejnowski, Terrence J.},
  year = {2001},
  month = aug,
  journal = {Nature Reviews Neuroscience},
  volume = {2},
  number = {8},
  pages = {539--550},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/35086012}
}

@article{sanchez-romero2021combining,
  title = {Combining Multiple Functional Connectivity Methods to Improve Causal Inferences},
  author = {{Sanchez-Romero}, Ruben and Cole, Michael W.},
  year = {2021},
  month = feb,
  journal = {Journal of cognitive neuroscience},
  volume = {33},
  number = {2},
  pages = {180--194},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_01580},
  abstract = {Cognition and behavior emerge from brain network interactions, suggesting that causal interactions should be central to the study of brain function. Yet approaches that characterize relationships among neural time series\textemdash functional connectivity (FC) methods\textemdash are dominated by methods that assess bivariate statistical associations rather than causal interactions. Such bivariate approaches result in substantial false positives since they do not account for confounders (common causes) among neural populations. A major reason for the dominance of methods such as bivariate Pearson correlation (with functional MRI) and coherence (with electrophysiological methods) may be their simplicity. Thus, we sought to identify an FC method that was both simple and improved causal inferences relative to the most popular methods. We started with partial correlation, showing with neural network simulations that this substantially improves causal inferences relative to bivariate correlation. However, the presence of colliders (common effects) in a network resulted in false positives with partial correlation, though this was not a problem for bivariate correlations. This led us to propose a new combined functional connectivity method (combinedFC) that incorporates simple bivariate and partial correlation FC measures to make more valid causal inferences than either alone. We release a toolbox for implementing this new combinedFC method to facilitate improvement of FC-based causal inferences. CombinedFC is a general method for functional connectivity and can be applied equally to resting-state and task-based paradigms.},
  pmcid = {PMC8132338},
  pmid = {32427070}
}

@misc{schlafy2020casestudiespython,
  title = {Case-{{Studies-Python}}},
  author = {Schlafy, Emily and Kramer, Mark and Eden, Uri},
  year = {2020},
  abstract = {Case-Studies-Python  This repository is a companion to the textbook},
  howpublished = {https://mark-kramer.github.io/Case-Studies-Python/intro.html}
}

@misc{schlafybasic,
  title = {Basic {{Analysis}} of {{Spike Train Data}}},
  author = {Schlafy, Emily and Kramer, Mark and Eden, Uri},
  abstract = {Basic Analysis of Spike Train Data},
  howpublished = {https://mark-kramer.github.io/Case-Studies-Python/08.html}
}

@article{schreiber2000measuring,
  title = {Measuring {{Information Transfer}}},
  author = {Schreiber, Thomas},
  year = {2000},
  month = jul,
  journal = {Physical Review Letters},
  volume = {85},
  number = {2},
  pages = {461--464},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.85.461}
}

@article{seth2007granger,
  title = {Granger Causality},
  author = {Seth, Anil},
  year = {2007},
  journal = {Scholarpedia},
  volume = {2},
  number = {7},
  pages = {1667},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.1667}
}

@article{seth2015granger,
  title = {Granger {{Causality Analysis}} in {{Neuroscience}} and {{Neuroimaging}}},
  author = {Seth, A. K. and Barrett, A. B. and Barnett, L.},
  year = {2015},
  month = feb,
  journal = {Journal of Neuroscience},
  volume = {35},
  number = {8},
  pages = {3293--3297},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4399-14.2015}
}

@inproceedings{shanmugam2015learning,
  title = {Learning Causal Graphs with Small Interventions},
  booktitle = {Proc. 29th {{Conf}}. on {{Neural Information Processing Systems}}},
  author = {Shanmugam, Karthikeyan and Kocaoglu, Murat and Dimakis, Alexandros G. and Vishwanath, Sriram},
  year = {2015},
  month = dec,
  address = {{Montr\'eal, QC, Canada}}
}

@article{sharp1993dynamic,
  title = {The Dynamic Clamp: Artificial Conductances in Biological Neurons},
  author = {Sharp, Andrew A and O'Neil, Michael B and Abbott, LF and Marder, Eve},
  year = {1993},
  journal = {Trends in neurosciences},
  volume = {16},
  number = {10},
  pages = {389--394},
  publisher = {{Elsevier}}
}

@article{shimono2015functional,
  title = {Functional {{Clusters}}, {{Hubs}}, and {{Communities}} in the {{Cortical Microconnectome}}},
  author = {Shimono, Masanori and Beggs, John M.},
  year = {2015},
  month = oct,
  journal = {Cerebral Cortex (New York, N.Y.: 1991)},
  volume = {25},
  number = {10},
  pages = {3743--3757},
  issn = {1460-2199},
  doi = {10.1093/cercor/bhu252},
  abstract = {Although relationships between networks of different scales have been observed in macroscopic brain studies, relationships between structures of different scales in networks of neurons are unknown. To address this, we recorded from up to 500 neurons simultaneously from slice cultures of rodent somatosensory cortex. We then measured directed effective networks with transfer entropy, previously validated in simulated cortical networks. These effective networks enabled us to evaluate distinctive nonrandom structures of connectivity at 2 different scales. We have 4 main findings. First, at the scale of 3-6 neurons (clusters), we found that high numbers of connections occurred significantly more often than expected by chance. Second, the distribution of the number of connections per neuron (degree distribution) had a long tail, indicating that the network contained distinctively high-degree neurons, or hubs. Third, at the scale of tens to hundreds of neurons, we typically found 2-3 significantly large communities. Finally, we demonstrated that communities were relatively more robust than clusters against shuffling of connections. We conclude the microconnectome of the cortex has specific organization at different scales, as revealed by differences in robustness. We suggest that this information will help us to understand how the microconnectome is robust against damage.},
  pmcid = {PMC4585513},
  pmid = {25336598},
  keywords = {+++,Animals,cluster,community,Connectome,hub,Mice,microconnectome,Models; Neurological,Nerve Net,networks,Neurons,Organ Culture Techniques,Somatosensory Cortex}
}

@article{shorten2021estimating,
  title = {Estimating {{Transfer Entropy}} in {{Continuous Time Between Neural Spike Trains}} or {{Other Event-Based Data}}},
  author = {Shorten, David P. and Spinney, Richard E. and Lizier, Joseph T.},
  year = {2021},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {4},
  pages = {e1008054},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008054},
  abstract = {Transfer entropy (TE) is a widely used measure of directed information flows in a number of domains including neuroscience. Many real-world time series for which we are interested in information flows come in the form of (near) instantaneous events occurring over time. Examples include the spiking of biological neurons, trades on stock markets and posts to social media, amongst myriad other systems involving events in continuous time throughout the natural and social sciences. However, there exist severe limitations to the current approach to TE estimation on such event-based data via discretising the time series into time bins: it is not consistent, has high bias, converges slowly and cannot simultaneously capture relationships that occur with very fine time precision as well as those that occur over long time intervals. Building on recent work which derived a theoretical framework for TE in continuous time, we present an estimation framework for TE on event-based data and develop a k-nearest-neighbours estimator within this framework. This estimator is provably consistent, has favourable bias properties and converges orders of magnitude more quickly than the current state-of-the-art in discrete-time estimation on synthetic examples. We demonstrate failures of the traditionally-used source-time-shift method for null surrogate generation. In order to overcome these failures, we develop a local permutation scheme for generating surrogate time series conforming to the appropriate null hypothesis in order to test for the statistical significance of the TE and, as such, test for the conditional independence between the history of one point process and the updates of another. Our approach is shown to be capable of correctly rejecting or accepting the null hypothesis of conditional independence even in the presence of strong pairwise time-directed correlations. This capacity to accurately test for conditional independence is further demonstrated on models of a spiking neural circuit inspired by the pyloric circuit of the crustacean stomatogastric ganglion, succeeding where previous related estimators have failed.},
  keywords = {Algorithms,Entropy,Ganglia,Network analysis,Neural pathways,Neurons,Permutation,Probability density}
}

@article{siddiqi2022causal,
  title = {Causal Mapping of Human Brain Function},
  author = {Siddiqi, Shan H. and Kording, Konrad P. and Parvizi, Josef and Fox, Michael D.},
  year = {2022},
  month = apr,
  journal = {Nature Reviews Neuroscience},
  pages = {1--15},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-022-00583-8},
  abstract = {Mapping human brain function is a long-standing goal of neuroscience that promises to inform the development of new treatments for brain disorders. Early maps of human brain function were based on locations of brain damage or brain stimulation that caused a functional change. Over time, this approach was largely replaced by technologies such as functional neuroimaging, which identify brain regions in which activity is correlated with behaviours or symptoms. Despite their advantages, these technologies reveal correlations, not causation. This creates challenges for interpreting the data generated from these tools and using them to develop treatments for brain disorders. A return to causal mapping of human brain function based on brain lesions and brain stimulation is underway. New approaches can combine these causal sources of information with modern neuroimaging and electrophysiology techniques to gain new insights into the functions of specific brain areas. In this Review, we provide a definition of causality for translational research, propose a continuum along which to assess the relative strength of causal information from human brain mapping studies and discuss recent advances in causal brain mapping and their relevance for developing treatments.},
  copyright = {2022 Springer Nature Limited},
  keywords = {Network models,Neural circuits}
}

@article{silvanto2012why,
  title = {Why the Assessment of Causality in Brain-Behavior Relations Requires Brain Stimulation},
  author = {Silvanto, Juha and {Pascual-Leone}, Alvaro},
  year = {2012},
  month = apr,
  journal = {Journal of Cognitive Neuroscience},
  volume = {24},
  number = {4},
  pages = {775--777},
  issn = {1530-8898},
  doi = {10.1162/jocn_a_00193},
  abstract = {A central aim in cognitive neuroscience is to explain how neural activity gives rise to perception and behavior; the causal link of paramount interest is thus from brain to behavior. Functional neuroimaging studies, however, tend to provide information in the opposite direction by informing us how manipulation of behavior may affect neural activity. Although this may provide valuable insights into neuronal properties, one cannot use such evidence to make inferences about the behavioral significance of the observed activations; if A causes B, it does not necessarily follow that B causes A. In contrast, brain stimulation techniques enable us to directly modulate brain activity as the source of behavior and thus establish causal links.},
  pmid = {22264196},
  keywords = {Behavior,Brain,Brain Mapping,Cognition,Humans,Perception}
}

@article{simonotto1997visual,
  title = {Visual {{Perception}} of {{Stochastic Resonance}}},
  author = {Simonotto, Enrico and Riani, Massimo and Seife, Charles and Roberts, Mark and Twitty, Jennifer and Moss, Frank},
  year = {1997},
  month = feb,
  journal = {Physical Review Letters},
  volume = {78},
  number = {6},
  pages = {1186--1189},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.78.1186}
}

@incollection{skiena2011transitive,
  title = {Transitive Closure and Reduction},
  booktitle = {The Algorithm Design Manual},
  author = {Skiena, Steven S.},
  year = {2011},
  edition = {Second},
  pages = {495--97},
  publisher = {{Springer}}
}

@article{smith2005eeg,
  title = {{{EEG}} in the Diagnosis, Classification, and Management of Patients with Epilepsy},
  author = {Smith, S},
  year = {2005},
  month = jun,
  journal = {Journal of Neurology, Neurosurgery, and Psychiatry},
  volume = {76},
  number = {Suppl 2},
  pages = {ii2-ii7},
  issn = {0022-3050},
  doi = {10.1136/jnnp.2005.069245},
  pmcid = {PMC1765691},
  pmid = {15961864}
}

@article{so2012assessing,
  title = {Assessing Functional Connectivity of Neural Ensembles Using Directed Information},
  author = {So, Kelvin and Koralek, Aaron C and Ganguly, Karunesh and Gastpar, Michael C and Carmena, Jose M},
  year = {2012},
  month = apr,
  journal = {Journal of Neural Engineering},
  volume = {9},
  number = {2},
  pages = {026004},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/9/2/026004},
  abstract = {Neurons in the brain form highly complex networks through synaptic connections. Traditionally, functional connectivity between neurons has been explored using methods such as correlations, which do not contain any notion of directionality. Recently, an information-theoretic approach based on directed information theory has been proposed as a way to infer the direction of influence. However, it is still unclear whether this new approach provides any additional insight beyond conventional correlation analyses. In this paper, we present a modified procedure for estimating directed information and provide a comparison of results obtained using correlation analyses on both simulated and experimental data. Using physiologically realistic simulations, we demonstrate that directed information can outperform correlation in determining connections between neural spike trains while also providing directionality of the relationship, which cannot be assessed using correlation. Secondly, applying our method to rodent and primate data sets, we demonstrate that directed information can accurately estimate the conduction delay in connections between different brain structures. Moreover, directed information reveals connectivity structures that are not captured by correlations. Hence, directed information provides accurate and novel insights into the functional connectivity of neural ensembles that are applicable to data from neurophysiological studies in awake behaving animals.}
}

@article{song2005highly,
  title = {Highly {{Nonrandom Features}} of {{Synaptic Connectivity}} in {{Local Cortical Circuits}}},
  author = {Song, Sen and Sj{\"o}str{\"o}m, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B.},
  year = {2005},
  month = mar,
  journal = {PLOS Biology},
  volume = {3},
  number = {3},
  pages = {e68},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0030068},
  abstract = {How different is local cortical circuitry from a random network? To answer this question, we probed synaptic connections with several hundred simultaneous quadruple whole-cell recordings from layer 5 pyramidal neurons in the rat visual cortex. Analysis of this dataset revealed several nonrandom features in synaptic connectivity. We confirmed previous reports that bidirectional connections are more common than expected in a random network. We found that several highly clustered three-neuron connectivity patterns are overrepresented, suggesting that connections tend to cluster together. We also analyzed synaptic connection strength as defined by the peak excitatory postsynaptic potential amplitude. We found that the distribution of synaptic connection strength differs significantly from the Poisson distribution and can be fitted by a lognormal distribution. Such a distribution has a heavier tail and implies that synaptic weight is concentrated among few synaptic connections. In addition, the strengths of synaptic connections sharing pre- or postsynaptic neurons are correlated, implying that strong connections are even more clustered than the weak ones. Therefore, the local cortical network structure can be viewed as a skeleton of stronger connections in a sea of weaker ones. Such a skeleton is likely to play an important role in network dynamics and should be investigated further.},
  keywords = {+++,Action potentials,Axons,Excitatory postsynaptic potentials,Network motifs,Network reciprocity,Neural networks,Neuronal dendrites,Neurons}
}

@misc{spike,
  title = {Spike Train Correlation \textemdash{} {{Elephant}} 0.3.0 Documentation},
  howpublished = {https://elephant.readthedocs.io/en/0.3.0/reference/spike\_train\_correlation.html}
}

@misc{spiketrain,
  title = {Spiketrain \textemdash{} Neuronpy 0.1.5 Documentation},
  howpublished = {https://pythonhosted.org/neuronpy/spiketrain.html}
}

@article{stephenlkeeley2020modeling,
  title = {Modeling Statistical Dependencies in Multi-Region Spike Train Data},
  author = {{Stephen L Keeley} and {David M Zoltowski} and {Mikio C Aoi} and {Jonathan W Pillow}},
  year = {2020},
  journal = {Current Opinion in Neurobiology},
  volume = {65},
  pages = {194--202},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2020.11.005.},
  abstract = {Neural computations underlying cognition and behavior rely on the coordination of neural activity across multiple brain areas. Understanding how brain areas interact to process information or generate behavior is thus a central question in neuroscience. Here we provide an overview of statistical approaches for characterizing statistical dependencies in multi-region spike train recordings. We focus on two classes of models in particular: regression-based models and shared latent variable models. Regression-based models describe interactions in terms of a directed transformation of information from one region to another. Shared latent variable models, on the other hand, seek to describe interactions in terms of sources that capture common fluctuations in spiking activity across regions. We discuss the advantages and limitations of each of these approaches and future directions for the field. We intend this review to be an introduction to the statistical methods in multi- region models for computational neuroscientists and experimentalists alike.},
  keywords = {++}
}

@article{stetter2012modelfree,
  title = {Model-{{Free Reconstruction}} of {{Excitatory Neuronal Connectivity}} from {{Calcium Imaging Signals}}},
  author = {Stetter, Olav and Battaglia, Demian and Soriano, Jordi and Geisel, Theo},
  year = {2012},
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {8},
  number = {8},
  pages = {e1002653},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002653},
  abstract = {A systematic assessment of global neural network connectivity through direct electrophysiological assays has remained technically infeasible, even in simpler systems like dissociated neuronal cultures. We introduce an improved algorithmic approach based on Transfer Entropy to reconstruct structural connectivity from network activity monitored through calcium imaging. We focus in this study on the inference of excitatory synaptic links. Based on information theory, our method requires no prior assumptions on the statistics of neuronal firing and neuronal connections. The performance of our algorithm is benchmarked on surrogate time series of calcium fluorescence generated by the simulated dynamics of a network with known ground-truth topology. We find that the functional network topology revealed by Transfer Entropy depends qualitatively on the time-dependent dynamic state of the network (bursting or non-bursting). Thus by conditioning with respect to the global mean activity, we improve the performance of our method. This allows us to focus the analysis to specific dynamical regimes of the network in which the inferred functional connectivity is shaped by monosynaptic excitatory connections, rather than by collective synchrony. Our method can discriminate between actual causal influences between neurons and spurious non-causal correlations due to light scattering artifacts, which inherently affect the quality of fluorescence imaging. Compared to other reconstruction strategies such as cross-correlation or Granger Causality methods, our method based on improved Transfer Entropy is remarkably more accurate. In particular, it provides a good estimation of the excitatory network clustering coefficient, allowing for discrimination between weakly and strongly clustered topologies. Finally, we demonstrate the applicability of our method to analyses of real recordings of in vitro disinhibited cortical cultures where we suggest that excitatory connections are characterized by an elevated level of clustering compared to a random graph (although not extreme) and can be markedly non-local.},
  keywords = {Action potentials,Calcium imaging,Calcium signaling,Clustering coefficients,Fluorescence imaging,Light scattering,Neural networks,Neurons}
}

@article{stevenson2008inferring,
  title = {Inferring Functional Connections between Neurons},
  author = {Stevenson, Ian H. and Rebesco, James M. and Miller, Lee E. and K{\"o}rding, Konrad P.},
  year = {2008},
  month = dec,
  journal = {Current opinion in neurobiology},
  volume = {18},
  number = {6},
  pages = {582--588},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2008.11.005},
  abstract = {A central question in neuroscience is how interactions between neurons give rise to behavior. In many electrophysiological experiments, the activity of a set of neurons is recorded while sensory stimuli or movement tasks are varied. Tools that aim to reveal underlying interactions between neurons from such data can be extremely useful. Traditionally, neuroscientists have studied these interactions using purely descriptive statistics (cross-correlograms or joint peri-stimulus time histograms). However, the interpretation of such data is often difficult, particularly as the number of recorded neurons grows. Recent research suggests that model-based, maximum likelihood methods can improve these analyses. In addition to estimating neural interactions, application of these techniques has improved decoding of external variables, created novel interpretations of existing electrophysiological data, and may provide new insight into how the brain represents information.},
  pmcid = {PMC2706692},
  pmid = {19081241},
  keywords = {++}
}

@article{stevenson2009bayesian,
  title = {Bayesian Inference of Functional Connectivity and Network Structure from Spikes},
  author = {Stevenson, Ian H. and Rebesco, James M. and Hatsopoulos, Nicholas G. and Haga, Zach and Miller, Lee E. and K{\"o}rding, Konrad P.},
  year = {2009},
  month = jun,
  journal = {Ieee Transactions on Neural Systems and Rehabilitation Engineering},
  volume = {17},
  number = {3},
  pages = {203--213},
  issn = {1534-4320},
  doi = {10.1109/TNSRE.2008.2010471},
  abstract = {Current multi-electrode techniques enable the simultaneous recording of spikes from hundreds of neurons. To study neural plasticity and network structure it is desirable to infer the underlying functional connectivity between the recorded neurons. Functional connectivity is defined by a large number of parameters, which characterize how each neuron influences the other neurons. A Bayesian approach that combines information from the recorded spikes (likelihood) with prior beliefs about functional connectivity (prior) can improve inference of these parameters and reduce overfitting. Recent studies have used likelihood functions based on the statistics of point-processes and a prior that captures the sparseness of neural connections. Here we include a prior that captures the empirical finding that interactions tend to vary smoothly in time. We show that this method can successfully infer connectivity patterns in simulated data and apply the algorithm to spike data recorded from primary motor (M1) and premotor (PMd) cortices of a monkey. Finally, we present a new approach to studying structure in inferred connections based on a Bayesian clustering algorithm. Groups of neurons in M1 and PMd show common patterns of input and output that may correspond to functional assemblies.},
  pmcid = {PMC3406607},
  pmid = {19273038},
  keywords = {++}
}

@article{stokes2017study,
  title = {A Study of Problems Encountered in {{Granger}} Causality Analysis from a Neuroscience Perspective},
  author = {Stokes, Patrick A. and Purdon, Patrick L.},
  year = {2017},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {34},
  pages = {E7063-E7072},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1704663114},
  abstract = {Granger causality methods were developed to analyze the flow of information between time series. These methods have become more widely applied in neuroscience. Frequency-domain causality measures, such as those of Geweke, as well as multivariate methods, have particular appeal in neuroscience due to the prevalence of oscillatory phenomena and highly multivariate experimental recordings. Despite its widespread application in many fields, there are ongoing concerns regarding the applicability of Granger causality methods in neuroscience. When are these methods appropriate? How reliably do they recover the system structure underlying the observed data? What do frequency-domain causality measures tell us about the functional properties of oscillatory neural systems? In this paper, we analyze fundamental properties of Granger\textendash Geweke (GG) causality, both computational and conceptual. Specifically, we show that (i) GG causality estimates can be either severely biased or of high variance, both leading to spurious results; (ii) even if estimated correctly, GG causality estimates alone are not interpretable without examining the component behaviors of the system model; and (iii) GG causality ignores critical components of a system's dynamics. Based on this analysis, we find that the notion of causality quantified is incompatible with the objectives of many neuroscience investigations, leading to highly counterintuitive and potentially misleading results. Through the analysis of these problems, we provide important conceptual clarification of GG causality, with implications for other related causality approaches and for the role of causality analyses in neuroscience as a whole.},
  chapter = {PNAS Plus},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  pmid = {28778996},
  keywords = {connectivity,Granger causality,neural oscillations,system identification,time series analysis}
}

@misc{student.utwente.nl2011fieldtrip,
  title = {[{{FieldTrip}}] {{Cross}} Correlation Using Xcorr In	ft\_connectivityanalysis},
  author = {j f verwer at {student.utwente.nl}, s},
  year = {Fri Mar 25 15:00:50 CET 2011}
}

@incollection{sutera2017simple,
  title = {Simple {{Connectome Inference}} from {{Partial Correlation Statistics}} in {{Calcium Imaging}}},
  booktitle = {Neural {{Connectomics Challenge}}},
  author = {Sutera, Antonio and Joly, Arnaud and {Franois-Lavet}, Vincent and Aaron Qiu, Zixiao and Louppe, Gilles and Ernst, Damien and Geurts, Pierre},
  editor = {Battaglia, Demian and Guyon, Isabelle and Lemaire, Vincent and Orlandi, Javier and Ray, Bisakha and Soriano, Jordi},
  year = {2017},
  pages = {23--36},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-53070-3_2},
  abstract = {In this work, we propose a simple yet effective solution to the problem of connectome inference in calcium imaging data. The proposed algorithm consists of two steps. First, processing the raw signals to detect neural peak activities. Second, inferring the degree of association between neurons from partial correlation statistics. This paper summarises the methodology that led us to win the Connectomics Challenge, proposes a simplified version of our method, and finally compares our results with respect to other inference methods.},
  isbn = {978-3-319-53069-7 978-3-319-53070-3}
}

@article{tchumatchenko2011spike,
  title = {Spike {{Correlations}} \textendash{} {{What Can They Tell About Synchrony}}?},
  author = {Tchumatchenko, Tatjana and Geisel, Theo and Volgushev, Maxim and Wolf, Fred},
  year = {2011},
  journal = {Frontiers in Neuroscience},
  volume = {5},
  pages = {68},
  issn = {1662-453X},
  doi = {10.3389/fnins.2011.00068},
  abstract = {Sensory and cognitive processing relies on the concerted activity of large populations of neurons. The advent of modern experimental techniques like two-photon population calcium imaging makes it possible to monitor the spiking activity of multiple neurons as they are participating in specific cognitive tasks. The development of appropriate theoretical tools to quantify and interpret the spiking activity of multiple neurons, however, is still in its infancy. One of the simplest and widely used measures of correlated activity is the pairwise correlation coefficient. While spike correlation coefficients are easy to compute using the available numerical toolboxes, it has remained largely an open question whether they are indeed a reliable measure of synchrony. Surprisingly, despite the intense use of correlation coefficients in the design of synthetic spike trains, the construction of population models and the assessment of the synchrony level in live neuronal networks very little was known about their computational properties. We showed that many features of pairwise spike correlations can be studied analytically in a tractable threshold model. Importantly, we demonstrated that under some circumstances the correlation coefficients can vanish, even though input and also pairwise spike cross correlations are present. This finding suggests that the most popular and frequently used measures can, by design, fail to capture the neuronal synchrony.},
  keywords = {review,spike train analysis,synchrony}
}

@article{timme2014multiplex,
  title = {Multiplex {{Networks}} of {{Cortical}} and {{Hippocampal Neurons Revealed}} at {{Different Timescales}}},
  author = {Timme, Nicholas and Ito, Shinya and Myroshnychenko, Maxym and Yeh, Fang-Chin and Hiolski, Emma and Hottowy, Pawel and Beggs, John M.},
  year = {2014},
  month = dec,
  journal = {PLOS ONE},
  volume = {9},
  number = {12},
  pages = {e115764},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0115764},
  abstract = {Recent studies have emphasized the importance of multiplex networks \textendash{} interdependent networks with shared nodes and different types of connections \textendash{} in systems primarily outside of neuroscience. Though the multiplex properties of networks are frequently not considered, most networks are actually multiplex networks and the multiplex specific features of networks can greatly affect network behavior (e.g. fault tolerance). Thus, the study of networks of neurons could potentially be greatly enhanced using a multiplex perspective. Given the wide range of temporally dependent rhythms and phenomena present in neural systems, we chose to examine multiplex networks of individual neurons with time scale dependent connections. To study these networks, we used transfer entropy \textendash{} an information theoretic quantity that can be used to measure linear and nonlinear interactions \textendash{} to systematically measure the connectivity between individual neurons at different time scales in cortical and hippocampal slice cultures. We recorded the spiking activity of almost 12,000 neurons across 60 tissue samples using a 512-electrode array with 60 micrometer inter-electrode spacing and 50 microsecond temporal resolution. To the best of our knowledge, this preparation and recording method represents a superior combination of number of recorded neurons and temporal and spatial recording resolutions to any currently available in vivo system. We found that highly connected neurons (``hubs'') were localized to certain time scales, which, we hypothesize, increases the fault tolerance of the network. Conversely, a large proportion of non-hub neurons were not localized to certain time scales. In addition, we found that long and short time scale connectivity was uncorrelated. Finally, we found that long time scale networks were significantly less modular and more disassortative than short time scale networks in both tissue types. As far as we are aware, this analysis represents the first systematic study of temporally dependent multiplex networks among individual neurons.},
  keywords = {Action potentials,Behavior,Fault tolerance,Hippocampus,Mathematical models,Multiplex networks,Neural networks,Neurons}
}

@article{timme2016highdegree,
  title = {High-{{Degree Neurons Feed Cortical Computations}}},
  author = {Timme, Nicholas M. and Ito, Shinya and Myroshnychenko, Maxym and Nigam, Sunny and Shimono, Masanori and Yeh, Fang-Chin and Hottowy, Pawel and Litke, Alan M. and Beggs, John M.},
  year = {2016},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {12},
  number = {5},
  pages = {e1004858},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004858},
  abstract = {Recent work has shown that functional connectivity among cortical neurons is highly varied, with a small percentage of neurons having many more connections than others. Also, recent theoretical developments now make it possible to quantify how neurons modify information from the connections they receive. Therefore, it is now possible to investigate how information modification, or computation, depends on the number of connections a neuron receives (in-degree) or sends out (out-degree). To do this, we recorded the simultaneous spiking activity of hundreds of neurons in cortico-hippocampal slice cultures using a high-density 512-electrode array. This preparation and recording method combination produced large numbers of neurons recorded at temporal and spatial resolutions that are not currently available in any in vivo recording system. We utilized transfer entropy (a well-established method for detecting linear and nonlinear interactions in time series) and the partial information decomposition (a powerful, recently developed tool for dissecting multivariate information processing into distinct parts) to quantify computation between neurons where information flows converged. We found that computations did not occur equally in all neurons throughout the networks. Surprisingly, neurons that computed large amounts of information tended to receive connections from high out-degree neurons. However, the in-degree of a neuron was not related to the amount of information it computed. To gain insight into these findings, we developed a simple feedforward network model. We found that a degree-modified Hebbian wiring rule best reproduced the pattern of computation and degree correlation results seen in the real data. Interestingly, this rule also maximized signal propagation in the presence of network-wide correlations, suggesting a mechanism by which cortex could deal with common random background input. These are the first results to show that the extent to which a neuron modifies incoming information streams depends on its topological location in the surrounding functional network.},
  keywords = {Action potentials,Communications,Information entropy,Information theory,Network analysis,Neural networks,Neurons,Signaling networks}
}

@article{toba2017game,
  title = {Game Theoretical Mapping of Causal Interactions Underlying Visuo-spatial Attention in the Human Brain Based on Stroke Lesions},
  author = {Toba, Monica N. and Zavaglia, Melissa and Rastelli, Federica and Valabr{\'e}gue, Romain and Pradat-Diehl, Pascale and Valero-Cabr{\'e}, Antoni and Hilgetag, Claus C.},
  year = {2017},
  month = apr,
  journal = {Human Brain Mapping},
  volume = {38},
  number = {7},
  pages = {3454--3471},
  issn = {1065-9471},
  doi = {10.1002/hbm.23601},
  abstract = {Anatomical studies conducted in neurological conditions have developed our understanding of the causal relationships between brain lesions and their clinical consequences. The analysis of lesion patterns extended across brain networks has been particularly useful in offering new insights on brain\textendash behavior relationships. Here we applied multiperturbation Shapley value Analysis (MSA), a multivariate method based on coalitional game theory inferring causal regional contributions to specific behavioral outcomes from the characteristic functional deficits after stroke lesions. We established the causal patterns of contributions and interactions of nodes of the attentional orienting network on the basis of lesion and behavioral data from 25 right hemisphere stroke patients tested in visuo-spatial attention tasks. We calculated the percentage of damaged voxels for five right hemisphere cortical regions contributing to attentional orienting, involving seven specific Brodmann Areas (BA): Frontal Eye Fields, (FEF-BA6), Intraparietal Sulcus (IPS-BA7), Inferior Frontal Gyrus (IFG-BA44/BA45), Temporo-Parietal Junction (TPJ-BA39/BA40), and Inferior Occipital Gyrus (IOG-BA19). We computed the MSA contributions of these seven BAs to three behavioral clinical tests (line bisection, bells cancellation, and letter cancelation). Our analyses indicated IPS as the main contributor to the attentional orienting and also revealed synergistic influences among IPS, TPJ, and IOG (for bells cancellation and line bisection) and between TPJ and IFG (for bells and letter cancellation tasks). The findings demonstrate the ability of the MSA approach to infer plausible causal contributions of relevant right hemisphere sites in poststroke visuo-spatial attention and awareness disorders. Hum Brain Mapp 38:3454\textendash 3471, 2017. \textcopyright{} 2017 Wiley Periodicals, Inc.},
  pmcid = {PMC5645205},
  pmid = {28419682}
}

@article{towlson2020synthetic,
  title = {Synthetic Ablations in the {{C}}. Elegans Nervous System},
  author = {Towlson, Emma K. and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2020},
  month = mar,
  journal = {Network Neuroscience},
  volume = {4},
  number = {1},
  pages = {200--216},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00115},
  abstract = {Synthetic lethality, the finding that the simultaneous knockout of two or more individually nonessential genes leads to cell or organism death, has offered a systematic framework to explore cellular function, and also offered therapeutic applications. Yet the concept lacks its parallel in neuroscience\textemdash a systematic knowledge base on the role of double or higher order ablations in the functioning of a neural system. Here, we use the framework of network control to systematically predict the effects of ablating neuron pairs and triplets on the gentle touch response. We find that surprisingly small sets of 58 pairs and 46 triplets can reduce muscle controllability in this context, and that these sets are localized in the nervous system in distinct groups. Further, they lead to highly specific experimentally testable predictions about mechanisms of loss of control, and which muscle cells are expected to experience this loss., ``Synthetic lethality'' in cell biology is an extreme example of the effects of higher order genetic interactions: The simultaneous knockout of two or more individually nonessential genes leads to cell death. We define a neural analog to this concept in relation to the locomotor response to gentle touch in C. elegans. Two or more neurons are synthetic essential if individually they are not required for this behavior, yet their combination is. We employ a network control approach to systematically assess all pairs and triplets of neurons by their effect on body wall muscle controllability, and find that only surprisingly small sets of neurons are synthetic essential. They are highly localized in the nervous system and predicted to affect control over specific sets of muscles.},
  pmcid = {PMC7055645},
  pmid = {32166208}
}

@article{ursino2020transfer,
  title = {Transfer {{Entropy}} as a {{Measure}} of {{Brain Connectivity}}: {{A Critical Analysis With}} the {{Help}} of {{Neural Mass Models}}},
  shorttitle = {Transfer {{Entropy}} as a {{Measure}} of {{Brain Connectivity}}},
  author = {Ursino, Mauro and Ricci, Giulia and Magosso, Elisa},
  year = {2020},
  journal = {Frontiers in Computational Neuroscience},
  volume = {14},
  pages = {45},
  issn = {1662-5188},
  doi = {10.3389/fncom.2020.00045},
  abstract = {Objective: Assessing brain connectivity from electrophysiological signals is of great relevance in neuroscience, but results are still debated and depend crucially on how connectivity is defined and on mathematical instruments utilized. Aim of this work is to assess the capacity of bivariate Transfer Entropy (TE) to evaluate connectivity, using data generated from simple neural mass models of connected Regions of Interest (ROIs).Approach: Signals simulating mean field potentials were generated assuming two, three or four ROIs, connected via excitatory or by-synaptic inhibitory links. We investigated whether the presence of a statistically significant connection can be detected and if connection strength can be quantified.Main Results: Results suggest that TE can reliably estimate the strength of connectivity if neural populations work in their linear regions, and if the epoch lengths are longer than 10 s. In case of multivariate networks, some spurious connections can emerge (i.e., a statistically significant TE even in the absence of a true connection); however, quite a good correlation between TE and synaptic strength is still preserved. Moreover, TE appears more robust for distal regions (longer delays) compared with proximal regions (smaller delays): an approximate a priori knowledge on this delay can improve the procedure. Finally, non-linear phenomena affect the assessment of connectivity, since they may significantly reduce TE estimation: information transmission between two ROIs may be weak, due to non-linear phenomena, even if a strong causal connection is present.Significance: Changes in functional connectivity during different tasks or brain conditions, might not always reflect a true change in the connecting network, but rather a change in information transmission. A limitation of the work is the use of bivariate TE. In perspective, the use of multivariate TE can improve estimation and reduce some of the problems encountered in the present study.},
  keywords = {++,parameter sweep}
}

@article{vaidya2019lesion,
  title = {Lesion {{Studies}} in {{Contemporary Neuroscience}}},
  author = {Vaidya, Avinash R. and Pujara, Maia S. and Petrides, Michael and Murray, Elisabeth A. and Fellows, Lesley K.},
  year = {2019},
  month = aug,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {8},
  pages = {653--671},
  issn = {1879-307X},
  doi = {10.1016/j.tics.2019.05.009},
  abstract = {Studies of humans with focal brain damage and non-human animals with experimentally induced brain lesions have provided pivotal insights into the neural basis of behavior. As the repertoire of neural manipulation and recording techniques expands, the utility of studying permanent brain lesions bears re-examination. Studies on the effects of permanent lesions provide vital data about brain function that are distinct from those of reversible manipulations. Focusing on work carried out in humans and nonhuman primates, we address the inferential strengths and limitations of lesion studies, recent methodological developments, the integration of this approach with other methods, and the clinical and ecological relevance of this research. We argue that lesion studies are essential to the rigorous assessment of neuroscience theories.},
  pmcid = {PMC6712987},
  pmid = {31279672},
  keywords = {++,Animals,Brain,brain damage,Brain Injuries,Brain Mapping,humans,Humans,lesionâbehavior mapping,lesionâsymptom mapping,methods,neuropsychology,Neurosciences,nonhuman primates}
}

@article{valero-cabre2020perturbationdriven,
  title = {Perturbation-Driven Paradoxical Facilitation of Visuo-Spatial Function: {{Revisiting}} the `{{Sprague}} Effect'},
  shorttitle = {Perturbation-Driven Paradoxical Facilitation of Visuo-Spatial Function},
  author = {{Valero-Cabr{\'e}}, Antoni and Toba, Monica N. and Hilgetag, Claus C. and Rushmore, R. Jarrett},
  year = {2020},
  month = jan,
  journal = {Cortex},
  series = {In {{Honour}} of {{Dr Robert Rafal}}},
  volume = {122},
  pages = {10--39},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2019.01.031},
  abstract = {The `Sprague Effect' described in the seminal paper of James Sprague (Science 153:1544\textendash 1547, 1966a) is an unexpected paradoxical effect in which a second brain lesion reversed functional deficits induced by an earlier lesion. It was observed initially in the cat where severe and permanent contralateral visually guided attentional deficits generated by the ablation of large areas of the visual cortex were reversed by the subsequent removal of the superior colliculus (SC) opposite to the cortical lesion or by the splitting of the collicular commissure. Physiologically, this effect has been explained in several ways-most notably by the reduction of the functional inhibition of the ipsilateral SC by the contralateral SC, and the restoration of normal interactions between cortical and midbrain structures after ablation. In the present review, we aim at reappraising the `Sprague Effect' by critically analyzing studies that have been conducted in the feline and human brain. Moreover, we assess applications of the `Sprague Effect' in the rehabilitation of visually guided attentional impairments by using non-invasive therapeutic approaches such as transcranial magnetic stimulation (TMS) and transcranial direct-current stimulation (tDCS). We also review theoretical models of the effect that emphasize the inhibition and balancing between the two hemispheres and show implications for lesion inference approaches. Last, we critically review whether the resulting inter-hemispheric rivalry theories lead toward an efficient rehabilitation of stroke in humans. We conclude by emphasizing key challenges in the field of `Sprague Effect' applications in order to design better therapies for brain-damaged patients.},
  keywords = {Cat,Conscious visual perception,Focal brain damage,Human,Intercollicular commissural interactions,Interhemispheric rivalry,Lesion inference,Mutual inhibitory projections,Non-invasive brain stimulation (NIBS),Paradoxical functional restoration,Paradoxical lesion effects,Posterior parietal cortex,Reversible cortical deactivations,Right hemisphere stroke,Spatial attention,Sprague effect,Thermal deactivations,Transcallosal interactions,Transcranial direct current stimulation (tDCS),Transcranial magnetic stimulation (TMS),Unilateral neglect,Visuo-spatial attention rehabilitation}
}

@article{vdgroen2018stochastic,
  title = {Stochastic Resonance Enhances the Rate of Evidence Accumulation during Combined Brain Stimulation and Perceptual Decision-Making},
  author = {Vd Groen, Onno and Tang, Matthew and Wenderoth, Nicole and Mattingley, Jason},
  year = {2018},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {14},
  pages = {e1006301},
  doi = {10.1371/journal.pcbi.1006301},
  abstract = {Perceptual decision-making relies on the gradual accumulation of noisy sensory evidence. It is often assumed that such decisions are degraded by adding noise to a stimulus, or to the neural systems involved in the decision making process itself. But it has been suggested that adding an optimal amount of noise can, under appropriate conditions, enhance the quality of subthreshold signals in nonlinear systems, a phenomenon known as stochastic resonance. Here we asked whether perceptual decisions made by human observers obey these stochastic resonance principles, by adding noise directly to the visual cortex using transcranial random noise stimulation (tRNS) while participants judged the direction of coherent motion in random-dot kinematograms presented at the fovea. We found that adding tRNS bilaterally to visual cortex enhanced decision-making when stimuli were just below perceptual threshold, but not when they were well below or above threshold. We modelled the data under a drift diffusion framework, and showed that bilateral tRNS selectively increased the drift rate parameter, which indexes the rate of evidence accumulation. Our study is the first to provide causal evidence that perceptual decision-making is susceptible to a stochastic resonance effect induced by tRNS, and to show that this effect arises from selective enhancement of the rate of evidence accumulation for sub-threshold sensory events.}
}

@article{veit2021temporal,
  title = {Temporal Order of Signal Propagation within and across Intrinsic Brain Networks},
  author = {Veit, Mike J. and Kucyi, Aaron and Hu, Wenhan and Zhang, Chao and Zhao, Baotian and Guo, Zhihao and Yang, Bowen and {Sava-Segal}, Clara and Perry, Claire and Zhang, Jianguo and Zhang, Kai and Parvizi, Josef},
  year = {2021},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {48},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2105031118},
  abstract = {We studied the temporal dynamics of activity within and across functional MRI (fMRI)\textendash derived nodes of intrinsic resting-state networks of the human brain using intracranial electroencephalography (iEEG) and repeated single-pulse electrical stimulation (SPES) in neurosurgical subjects implanted with intracranial electrodes. We stimulated and recorded from 2,133 and 2,372 sites, respectively, in 29 subjects. We found that N1 and N2 segments of the evoked responses are associated with intra- and internetwork communications, respectively. In a separate cognitive experiment, evoked electrophysiological responses to visual target stimuli occurred with less temporal separation across pairs of electrodes that were located within the same fMRI-defined resting-state networks compared with those located across different resting-state networks. Our results suggest intranetwork prior to internetwork information processing at the subsecond timescale.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2021 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  pmid = {34819365},
  keywords = {CCEP,event related potentials,gradual-onset continuous performance task,human,intracranial EEG}
}

@article{ventura2012accurately,
  title = {Accurately Estimating Neuronal Correlation Requires a New Spike-Sorting Paradigm},
  author = {Ventura, Val{\'e}rie and Gerkin, Richard C.},
  year = {2012},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {19},
  pages = {7230--7235},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1115236109},
  abstract = {Neurophysiology is increasingly focused on identifying coincident activity among neurons. Strong inferences about neural computation are made from the results of such studies, so it is important that these results be accurate. However, the preliminary step in the analysis of such data, the assignment of spike waveforms to individual neurons (``spike-sorting''), makes a critical assumption which undermines the analysis: that spikes, and hence neurons, are independent. We show that this assumption guarantees that coincident spiking estimates such as correlation coefficients are biased. We also show how to eliminate this bias. Our solution involves sorting spikes jointly, which contrasts with the current practice of sorting spikes independently of other spikes. This new ``ensemble sorting'' yields unbiased estimates of coincident spiking, and permits more data to be analyzed with confidence, improving the quality and quantity of neurophysiological inferences. These results should be of interest outside the context of neuronal correlations studies. Indeed, simultaneous recording of many neurons has become the rule rather than the exception in experiments, so it is essential to spike sort correctly if we are to make valid inferences about any properties of, and relationships between, neurons.},
  chapter = {Physical Sciences},
  pmid = {22529350},
  keywords = {clustering,correlated spikes,point process history,statistical bias,statistical power}
}

@article{wang2014systematic,
  title = {A Systematic Framework for Functional Connectivity Measures},
  author = {Wang, Huifang E. and B{\~A}{\textcopyright}nar, Christian G. and Quilichini, Pascale P. and Friston, Karl J. and Jirsa, Viktor K. and Bernard, Christophe},
  year = {2014},
  month = dec,
  journal = {Frontiers in Neuroscience},
  volume = {8},
  issn = {1662-453X},
  doi = {10.3389/fnins.2014.00405},
  keywords = {++,Granger causality,Information theory,parameter sweep,transfer entropy}
}

@article{wang2019estimating,
  title = {Estimating {{Multiscale Direct Causality Graphs}} in {{Neural Spike-Field Networks}}},
  author = {Wang, Chuanmeizhi and Shanechi, Maryam M.},
  year = {2019},
  month = may,
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  volume = {27},
  number = {5},
  pages = {857--866},
  issn = {1558-0210},
  doi = {10.1109/TNSRE.2019.2908156},
  abstract = {Neural representations span various spatiotemporal scales of brain activity, from the spiking activity of single neurons to field activity measuring large-scale networks. The simultaneous analyses of spikes and fields to uncover causal interactions in multiscale networks could help understand neural mechanisms. However, assessing causality within spike-field networks is challenging as spikes are binary-valued with a fast time-scale while fields are continuous-valued with slower time-scales. Current causality measures are largely not applicable to mixed discrete-continuous network activity. Here, in this paper, we develop a novel multiscale causality estimation algorithm for spike-field networks. We construct a likelihood function comprised of point process models for spikes and linear Gaussian models for fields. For spikes, firing rates are modeled as a function of the history of both field signals and binary spike events within the network. For fields, to make their linear models consistent with biophysical findings, we use the history of field signals and the history of the latent log-firing rates of neurons as predictors. To resolve the challenge of estimating the network model parameters in the presence of latent firing rates, we develop a sequential maximum-likelihood parameter estimation procedure that extends to large networks. Once models are estimated, we compute directed information as our measure of multiscale causality and devise two statistical tests to assess its significance. Using extensive simulations, we show that the algorithm can accurately reconstruct the true causality graphs of random spike-field networks. Moreover, the algorithm is robust to the number of connections, connection strengths, or exact topology of the network. This multiscale causality estimation algorithm has important implications for studying neural mechanisms and for future neurotechnology design.},
  keywords = {Biological system modeling,Causality,Computational modeling,Estimation,History,local field potentials (LFP),multiscale data,Network topology,neural encoding,Neurons,Spatiotemporal phenomena,spikes}
}

@article{wang2022modeling,
  title = {Modeling Multiscale Causal Interactions between Spiking and Field Potential Signals during Behavior},
  author = {Wang, Chuanmeizhi and Pesaran, Bijan and Shanechi, Maryam M},
  year = {2022},
  journal = {Journal of Neural Engineering},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/ac4e1c},
  abstract = {Objective. Brain recordings exhibit dynamics at multiple spatiotemporal scales, which are measured with spike trains and larger-scale field potential signals. To study neural processes, it is important to identify and model causal interactions not only at a single scale of activity, but also across multiple scales, i.e. between spike trains and field potential signals. Standard causality measures are not directly applicable here because spike trains are binary-valued but field potentials are continuous-valued. It is thus important to develop computational tools to recover multiscale neural causality during behavior, assess their performance on neural datasets, and study whether modeling multiscale causalities can improve the prediction of neural signals beyond what is possible with single-scale causality. Approach. We design a multiscale model-based Granger-like causality method based on directed information and evaluate its success both in realistic biophysical spike-field simulations and in motor cortical datasets from two non-human primates (NHP) performing a motor behavior. To compute multiscale causality, we learn point-process generalized linear models that predict the spike events at a given time based on the history of both spike trains and field potential signals. We also learn linear Gaussian models that predict the field potential signals at a given time based on their own history as well as either the history of binary spike events or that of latent firing rates. Main results. We find that our method reveals the true multiscale causality network structure in biophysical simulations despite the presence of model mismatch. Further, models with the identified multiscale causalities in the NHP neural datasets lead to better prediction of both spike trains and field potential signals compared to just modeling single-scale causalities. Finally, we find that latent firing rates are better predictors of field potential signals compared with the binary spike events in the NHP datasets. Significance. This multiscale causality method can reveal the directed functional interactions across spatiotemporal scales of brain activity to inform basic science investigations and neurotechnologies.}
}

@article{weber2017influence,
  title = {The Influence of Filtering and Downsampling on the Estimation of Transfer Entropy},
  author = {Weber, Immo and Florin, Esther and {von Papen}, Michael and Timmermann, Lars},
  year = {2017},
  month = nov,
  journal = {PLoS ONE},
  volume = {12},
  number = {11},
  pages = {e0188210},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0188210},
  abstract = {Transfer entropy (TE) provides a generalized and model-free framework to study Wiener-Granger causality between brain regions. Because of its nonparametric character, TE can infer directed information flow also from nonlinear systems. Despite its increasing number of applications in neuroscience, not much is known regarding the influence of common electrophysiological preprocessing on its estimation. We test the influence of filtering and downsampling on a recently proposed nearest neighborhood based TE estimator. Different filter settings and downsampling factors were tested in a simulation framework using a model with a linear coupling function and two nonlinear models with sigmoid and logistic coupling functions. For nonlinear coupling and progressively lower low-pass filter cut-off frequencies up to 72\% false negative direct connections and up to 26\% false positive connections were identified. In contrast, for the linear model, a monotonic increase was only observed for missed indirect connections (up to 86\%). High-pass filtering (1 Hz, 2 Hz) had no impact on TE estimation. After low-pass filtering interaction delays were significantly underestimated. Downsampling the data by a factor greater than the assumed interaction delay erased most of the transmitted information and thus led to a very high percentage (67\textendash 100\%) of false negative direct connections. Low-pass filtering increases the number of missed connections depending on the filters cut-off frequency. Downsampling should only be done if the sampling factor is smaller than the smallest assumed interaction delay of the analyzed network.},
  pmcid = {PMC5693301},
  pmid = {29149201}
}

@book{wibral2014directed,
  title = {Directed {{Information Measures}} in {{Neuroscience}}},
  editor = {Wibral, Michael and Vicente, Raul and Lizier, Joseph T.},
  year = {2014},
  series = {Understanding {{Complex Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-54474-3},
  isbn = {978-3-642-54473-6 978-3-642-54474-3}
}

@incollection{wiener1956theory,
  title = {The Theory of Prediction},
  booktitle = {Modern Mathematics for the Engineer},
  author = {Wiener, N},
  year = {1956},
  publisher = {{McGraw-Hill}}
}

@article{wismuller2021largescale,
  title = {Large-Scale Nonlinear {{Granger}} Causality for Inferring Directed Dependence from Short Multivariate Time-Series Data},
  author = {Wism{\"u}ller, Axel and Dsouza, Adora M. and Vosoughi, M. Ali and Abidin, Anas},
  year = {2021},
  month = apr,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {7817},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-87316-6},
  abstract = {A key challenge to gaining insight into complex systems is inferring nonlinear causal directional relations from observational time-series data. Specifically, estimating causal relationships between interacting components in large systems with only short recordings over few temporal observations remains an important, yet unresolved problem. Here, we introduce large-scale nonlinear Granger causality (lsNGC) which facilitates conditional Granger causality between two multivariate time series conditioned on a large number of confounding time series with a small number of observations. By modeling interactions with nonlinear state-space transformations from limited observational data, lsNGC identifies casual relations with no explicit a priori assumptions on functional interdependence between component time series in a computationally efficient manner. Additionally, our method provides a mathematical formulation revealing statistical significance of inferred causal relations. We extensively study the ability of lsNGC in inferring directed relations from two-node to thirty-four node chaotic time-series systems. Our results suggest that lsNGC captures meaningful interactions from limited observational data, where it performs favorably when compared to traditionally used methods. Finally, we demonstrate the applicability of lsNGC to estimating causality in large, real-world systems by inferring directional nonlinear, causal relationships among a large number of relatively short time series acquired from functional Magnetic Resonance Imaging (fMRI) data of the human brain.},
  copyright = {2021 The Author(s)},
  keywords = {Computational neuroscience,Computational science,Computer science,IDTxl,Machine learning,Software},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Computational science;Computer science;Machine learning;Software Subject\_term\_id: computational-neuroscience;computational-science;computer-science;machine-learning;software}
}

@article{wolff2018promise,
  title = {The Promise and Perils of Causal Circuit Manipulations},
  author = {Wolff, Steffen BE and {\"O}lveczky, Bence P},
  year = {2018},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  series = {Neurobiology of {{Behavior}}},
  volume = {49},
  pages = {84--94},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2018.01.004},
  abstract = {The development of increasingly sophisticated methods for recording and manipulating neural activity is revolutionizing neuroscience. By probing how activity patterns in different types of neurons and circuits contribute to behavior, these tools can help inform mechanistic models of brain function and explain the roles of distinct circuit elements. However, in systems where functions are distributed over large networks, interpreting causality experiments can be challenging. Here we review common assumptions underlying circuit manipulations in behaving animals and discuss the strengths and limitations of different approaches.}
}

@article{wollstadt2014efficient,
  title = {Efficient {{Transfer Entropy Analysis}} of {{Non-Stationary Neural Time Series}}},
  author = {Wollstadt, Patricia and {Mart{\'i}nez-Zarzuela}, Mario and Vicente, Raul and {D{\'i}az-Pernas}, Francisco J. and Wibral, Michael},
  year = {2014},
  month = jul,
  journal = {PLOS ONE},
  volume = {9},
  number = {7},
  pages = {e102833},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0102833},
  abstract = {Information theory allows us to investigate information processing in neural systems in terms of information transfer, storage and modification. Especially the measure of information transfer, transfer entropy, has seen a dramatic surge of interest in neuroscience. Estimating transfer entropy from two processes requires the observation of multiple realizations of these processes to estimate associated probability density functions. To obtain these necessary observations, available estimators typically assume stationarity of processes to allow pooling of observations over time. This assumption however, is a major obstacle to the application of these estimators in neuroscience as observed processes are often non-stationary. As a solution, Gomez-Herrero and colleagues theoretically showed that the stationarity assumption may be avoided by estimating transfer entropy from an ensemble of realizations. Such an ensemble of realizations is often readily available in neuroscience experiments in the form of experimental trials. Thus, in this work we combine the ensemble method with a recently proposed transfer entropy estimator to make transfer entropy estimation applicable to non-stationary time series. We present an efficient implementation of the approach that is suitable for the increased computational demand of the ensemble method's practical application. In particular, we use a massively parallel implementation for a graphics processing unit to handle the computationally most heavy aspects of the ensemble method for transfer entropy estimation. We test the performance and robustness of our implementation on data from numerical simulations of stochastic processes. We also demonstrate the applicability of the ensemble method to magnetoencephalographic data. While we mainly evaluate the proposed method for neuroscience data, we expect it to be applicable in a variety of fields that are concerned with the analysis of information transfer in complex biological, social, and artificial systems.},
  keywords = {+++,Algorithms,Ensemble methods,Entropy,IDTxl,Information entropy,Information processing,Random variables,Statistical data,Stochastic processes}
}

@article{wollstadt2019idtxl,
  title = {{{IDTxl}}: {{The Information Dynamics Toolkit}} Xl: A {{Python}} Package for the Efficient Analysis of Multivariate Information Dynamics in Networks},
  shorttitle = {{{IDTxl}}},
  author = {Wollstadt, Patricia and Lizier, Joseph T. and Vicente, Raul and Finn, Conor and {Mart{\'i}nez-Zarzuela}, Mario and Mediano, Pedro and Novelli, Leonardo and Wibral, Michael},
  year = {2019},
  month = feb,
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {34},
  eprint = {1807.10459},
  eprinttype = {arxiv},
  pages = {1081},
  issn = {2475-9066},
  doi = {10.21105/joss.01081},
  abstract = {The Information Dynamics Toolkit xl (IDTxl) is a comprehensive software package for efficient inference of networks and their node dynamics from multivariate time series data using information theory. IDTxl provides functionality to estimate the following measures: 1) For network inference: multivariate transfer entropy (TE)/Granger causality (GC), multivariate mutual information (MI), bivariate TE/GC, bivariate MI 2) For analysis of node dynamics: active information storage (AIS), partial information decomposition (PID) IDTxl implements estimators for discrete and continuous data with parallel computing engines for both GPU and CPU platforms. Written for Python3.4.3+.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,IDTxl}
}

@misc{wollstadttheoretical,
  title = {Theoretical {{Introduction}} {$\cdot$} Pwollstadt/{{IDTxl Wiki}}},
  author = {Wollstadt, Patricia},
  journal = {GitHub},
  abstract = {The Information Dynamics Toolkit xl (IDTxl) is a comprehensive software package for efficient inference of networks and their node dynamics from multivariate time series data using information theo...},
  howpublished = {https://github.com/pwollstadt/IDTxl},
  keywords = {IDTxl}
}

@article{womelsdorf2014dynamic,
  title = {Dynamic Circuit Motifs Underlying Rhythmic Gain Control, Gating and Integration},
  author = {Womelsdorf, Thilo and Valiante, Taufik A. and Sahin, Ned T. and Miller, Kai J. and Tiesinga, Paul},
  year = {2014},
  month = aug,
  journal = {Nature Neuroscience},
  volume = {17},
  number = {8},
  pages = {1031--1039},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3764},
  abstract = {In this paper, Womelsdorf and colleagues review the recent advances in our understanding of how rhythmic activity across multiple frequency bands and brain areas affects neural computations. The authors suggest a dynamic tripartite motif framework that links the activity signatures of given circuits with their structural elements and the proposed computational output.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  keywords = {Attention,Dynamical systems,Neurophysiology,Psychology},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Attention;Dynamical systems;Neurophysiology;Psychology Subject\_term\_id: attention;dynamical-systems;neurophysiology;psychology}
}

@article{xiao2011concerted,
  title = {Concerted Activity and Information Coding in the Retinal Ganglion Cells},
  author = {Xiao, Lei and Jing, Wei and Liang, Pei-Ji},
  year = {2011},
  month = oct,
  journal = {Sheng li xue bao : [Acta physiologica Sinica]},
  volume = {63},
  pages = {423--30},
  abstract = {In vertebrate visual system, information is firstly processed in retina. With the development of the multi-electrode recording technique, concerted activity has been extensively observed in retinal ganglion cells of different species. However, the role of concerted activity in visual information processing is still unclear and under debating. This article reviews the recent studies focused on concerted activity among retinal ganglion cells, discussing the issues about its category, detection and physiological function.}
}

@article{xu2018dimensionalities,
  title = {The Dimensionalities of Lesion-Deficit Mapping},
  author = {Xu, Tianbo and Jha, Ashwani and Nachev, Parashkev},
  year = {2018},
  month = jul,
  journal = {Neuropsychologia},
  volume = {115},
  pages = {134--141},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2017.09.007},
  abstract = {Lesion-deficit mapping remains the most powerful method for localising function in the human brain. As the highest court of appeal where competing theories of cerebral function conflict, it ought to be held to the most stringent inferential standards. Though at first sight elegantly transferable, the mass-univariate statistical framework popularized by functional imaging is demonstrably ill-suited to the task, both theoretically and empirically. The critical difficulty lies with the handling of the data's intrinsically high dimensionality. Conceptual opacity and computational complexity lead lesion-deficit mappers to neglect two distinct sets of anatomical interactions: those between areas unified by function, and those between areas unified by the natural pattern of pathological damage. Though both are soluble through high-dimensional multivariate analysis, the consequences of ignoring them are radically different. The former will bleach and coarsen a picture of the functional anatomy that is nonetheless broadly faithful to reality; the latter may alter it beyond all recognition. That the field continues to cling to mass-univariate methods suggests the latter problem is misidentified with the former, and that their distinction is in need of elaboration. We further argue that the vicious effects of lesion-driven interactions are not limited to anatomical localisation but will inevitably degrade purely predictive models of function such as those conceived for clinical prognostic use. Finally, we suggest there is a great deal to be learnt about lesion-mapping by simulation-based modelling of lesion data, for the fundamental problems lie upstream of the experimental data themselves.,                                        \textbullet{}               Neglecting two dimensionalities\textemdash lesion and functional anatomy\textemdash distorts lesion maps.                                         \textbullet{}               Lesion dimensionality is critical, mandating high-dimensional multivariate methods.                                         \textbullet{}               Simulations based on real lesion data applied to synthetic ground truths are needed.                                         \textbullet{}               Anatomical fidelity matters for clinical outcome prediction as for spatial inference.},
  pmcid = {PMC6018623},
  pmid = {28935195}
}

@article{yan2017network,
  title = {Network Control Principles Predict Neuron Function in the {{Caenorhabditis}} Elegans Connectome},
  author = {Yan, Gang and V{\'e}rtes, Petra E. and Towlson, Emma K. and Chew, Yee Lian and Walker, Denise S. and Schafer, William R. and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7677},
  pages = {519--523},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature24056},
  abstract = {Application of network control theory to the neuronal connectome of Caenorhabditis elegans, allowing prediction of the involvement of individual neurons in locomotion.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  keywords = {++,ablation,Applied physics,controllability,lesion,maximum flow,Neuroscience},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Applied physics;Neuroscience Subject\_term\_id: applied-physics;neuroscience}
}

@inproceedings{yang2018characterizing,
  title = {Characterizing and Learning Equivalence Classes of Causal {{DAGs}} under Interventions},
  booktitle = {Proc. 35th {{Int}}. {{Conf}}. on {{Machine Learning}}},
  author = {Yang, Karren D. and Katoff, Abigail and Uhler, Caroline},
  year = {2018},
  month = jul,
  address = {{Stockholm, Sweden}},
  abstract = {We consider the problem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser and B\textbackslash "uhlmann (2012) previously characterized the identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes. In this paper, we extend these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them. We define and characterize the interventional Markov equivalence class that can be identified from general (not necessarily perfect) intervention experiments. We also propose the first provably consistent algorithm for learning DAGs in this setting and evaluate our algorithm on simulated and biological datasets.}
}

@article{yang2018controltheoretic,
  title = {A Control-Theoretic System Identification Framework and a Real-Time Closed-Loop Clinical Simulation Testbed for Electrical Brain Stimulation},
  author = {Yang, Yuxiao and Connolly, Allison T. and Shanechi, Maryam M.},
  year = {2018},
  month = sep,
  journal = {Journal of Neural Engineering},
  volume = {15},
  number = {6},
  pages = {066007},
  publisher = {{IOP Publishing}},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/aad1a8},
  abstract = {Objective. Closed-loop electrical brain stimulation systems may enable a precisely-tailored treatment for neurological and neuropsychiatric disorders by controlling the stimulation based on neural activity feedback in real time. Developing model-based closed-loop systems requires a principled system identification framework to quantify the effect of input stimulation on output neural activity by learning an input-output (IO) dynamic model from data. Further, developing these systems needs a realistic clinical simulation testbed to design and validate the closed-loop controllers derived from the IO models before testing in human patients. Approach. First, we design a control-theoretic system identification framework to build dynamic IO models for neural activity that are amenable to closed-loop control design. To enable tractable model-based control, we use a data-driven linear state-space IO model that characterizes the effect of input on neural activity in terms of a low-dimensional hidden neural state. To learn the model parameters, we design a novel input waveform\textemdash a pulse train modulated by stochastic binary noise (BN) parameters\textemdash that we show is optimal for collecting informative IO datasets in system identification and conforms to clinical safety requirements. Second, we further extend this waveform to a generalized BN (GBN)-modulated waveform to reduce the required system identification time. Third, to enable extensive testing of system identification and closed-loop control, we develop a real-time closed-loop clinical hardware-in-the-loop (HIL) simulation testbed using the microelectrode recording and stimulation device, which incorporates stochastic noises, unknown disturbances and stimulation artifacts. Using this testbed, we implement both the system identification and the closed-loop controller by taking control of mood in depression as an example. Results. Testbed simulation results show that the closed-loop controller designed from IO models identified with the BN-modulated waveform achieves tight control, and performs similar to a controller that knows the true IO model of neural activity. When system identification time is limited, performance is further improved using the GBN-modulated waveform. Significance. The system identification framework with the new BN-modulated waveform and the clinical HIL simulation testbed can help develop future model-based closed-loop electrical brain stimulation systems for treatment of neurological and neuropsychiatric disorders.}
}

@article{young2000imputing,
  title = {On Imputing Function to Structure from the Behavioural Effects of Brain Lesions.},
  author = {Young, M P and Hilgetag, C C and Scannell, J W},
  year = {2000},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {355},
  number = {1393},
  pages = {147--161},
  issn = {0962-8436},
  abstract = {What is the link, if any, between the patterns of connections in the brain and the behavioural effects of localized brain lesions? We explored this question in four related ways. First, we investigated the distribution of activity decrements that followed simulated damage to elements of the thalamocortical network, using integrative mechanisms that have recently been used to successfully relate connection data to information on the spread of activation, and to account simultaneously for a variety of lesion effects. Second, we examined the consequences of the patterns of decrement seen in the simulation for each type of inference that has been employed to impute function to structure on the basis of the effects of brain lesions. Every variety of conventional inference, including double dissociation, readily misattributed function to structure. Third, we tried to derive a more reliable framework of inference for imputing function to structure, by clarifying concepts of function, and exploring a more formal framework, in which knowledge of connectivity is necessary but insufficient, based on concepts capable of mathematical specification. Fourth, we applied this framework to inferences about function relating to a simple network that reproduces intact, lesioned and paradoxically restored orientating behaviour. Lesion effects could be used to recover detailed and reliable information on which structures contributed to particular functions in this simple network. Finally, we explored how the effects of brain lesions and this formal approach could be used in conjunction with information from multiple neuroscience methodologies to develop a practical and reliable approach to inferring the functional roles of brain structures.},
  pmcid = {PMC1692718},
  pmid = {10703050}
}


