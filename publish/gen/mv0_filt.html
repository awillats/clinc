<!DOCTYPE html><html><head>
      <title>Closed-Loop Identifiability in Neural Circuits</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"],"imageFont":null},"root":"file:///Users/adam/.dotfiles/atom/packages/markdown-preview-enhanced/node_modules/@shd101wyy/mume/dependencies/mathjax"});
        </script>
        <script type="text/javascript" async src="file:////Users/adam/.dotfiles/atom/packages/markdown-preview-enhanced/node_modules/@shd101wyy/mume/dependencies/mathjax/MathJax.js" charset="UTF-8"></script>
        
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
.markdown-preview.markdown-preview {
  /* Heading numbering, inspired by Typora 
     https://support.typora.io/Auto-Numbering/
    */
  counter-reset: h1;
}
.markdown-preview.markdown-preview h1 {
  counter-reset: h2;
}
.markdown-preview.markdown-preview h2 {
  counter-reset: h3;
}
.markdown-preview.markdown-preview h3 {
  counter-reset: h4;
}
.markdown-preview.markdown-preview h4 {
  counter-reset: h5;
}
.markdown-preview.markdown-preview h5 {
  counter-reset: h6;
}
.markdown-preview.markdown-preview h1:before {
  counter-increment: h1;
  content: counter(h1) ". ";
}
.markdown-preview.markdown-preview h2:before {
  counter-increment: h2;
  content: counter(h1) "." counter(h2) ". ";
}
.markdown-preview.markdown-preview h3:before {
  counter-increment: h3;
  content: counter(h1) "." counter(h2) "." counter(h3) ". ";
}
.markdown-preview.markdown-preview h4:before {
  counter-increment: h4;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) ". ";
}
.markdown-preview.markdown-preview h5:before {
  counter-increment: h5;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) ". ";
}
.markdown-preview.markdown-preview h6:before {
  counter-increment: h6;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) "." counter(h6) ". ";
}
.do-number-sections {
  counter-reset: h1;
}
.do-number-sections h1 {
  counter-reset: h2;
}
.do-number-sections h2 {
  counter-reset: h3;
}
.do-number-sections h3 {
  counter-reset: h4;
}
.do-number-sections h4 {
  counter-reset: h5;
}
.do-number-sections h5 {
  counter-reset: h6;
}
.do-number-sections h1:before {
  counter-increment: h1;
  content: counter(h1) ". ";
}
.do-number-sections h2:before {
  counter-increment: h2;
  content: counter(h1) "." counter(h2) ". ";
}
.do-number-sections h3:before {
  counter-increment: h3;
  content: counter(h1) "." counter(h2) "." counter(h3) ". ";
}
.do-number-sections h4:before {
  counter-increment: h4;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) ". ";
}
.do-number-sections h5:before {
  counter-increment: h5;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) ". ";
}
.do-number-sections h6:before {
  counter-increment: h6;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) "." counter(h6) ". ";
}

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 class="mume-header" id="abstract">Abstract</h1>

<p>The necessity of intervention in inferring cause has long been<br>
understood in neuroscience. Recent work has highlighted the limitations<br>
of passive observation and single-site lesion studies in accurately<br>
recovering causal circuit structure. The advent of optogenetics has<br>
facilitated increasingly precise forms of intervention including<br>
closed-loop control which may help eliminate confounding influences.<br>
However, it is not yet clear how best to apply closed-loop control to<br>
leverage this increased inferential power. In this paper, we use tools<br>
from causal inference, control theory, and neuroscience to show when and<br>
how closed-loop interventions can more effectively reveal causal<br>
relationships.<br>
<code>We also examine the performance of standard network inference procedures in simulated spiking networks under passive, open-loop and closed-loop conditions.</code><br>
We demonstrate a unique capacity of feedback control to distinguish<br>
competing circuit hypotheses by disrupting connections which would<br>
otherwise result in equivalent patterns of correlation<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. Our results<br>
build toward a practical framework to improve design of neuroscience<br>
experiments to answer causal questions about neural circuits.</p>
<h1 class="mume-header" id="introduction">Introduction</h1>

<h2 class="mume-header" id="estimating-causal-interactions-in-the-brain">Estimating causal interactions in the brain</h2>

<p>Many hypotheses about neural circuits are phrased in terms of causal<br>
relationships: &quot;will changes in activity to this region of the brain<br>
produce corresponding changes in another region?&quot; Understanding these<br>
causal relationships is critical to both scientific understanding and to<br>
developing effective therapeutic interventions, which require knowledge<br>
of how potential therapies will impact brain activity and patient<br>
outcomes.</p>
<p>A range of mathematical and practical challenges make it difficult to<br>
determine these causal relationships. In studies that rely only<br>
observational data, it is often impossible to determine whether observed<br>
patterns of activity are caused by known and controlled inputs, or<br>
whether they are instead spurious connections generated by recurrent<br>
activity, indirect relationships, or unobserved &quot;confounders.&quot; It is<br>
generally understood that moving from experiments involving passive<br>
observation to more complex levels of intervention allows experimenters<br>
to better tackle challenges to circuit identification. However, while<br>
chemical and surgical lesion experiments have historically been employed<br>
to remove the influence of possible confounds, they are likely to<br>
dramatically disrupt circuits from their typical functions, making<br>
conclusions about underlying causal structure drawn from these<br>
experiments unlikely to hold in naturalistic settings . <em>Closed-loop</em><br>
interventions </p><div class="mathjax-exps">\[...\]</div> short description of closed-loop in neuro, maybe<br>
drawing from text in this<p></p>
<p>Despite the promise of these closed-loop strategies for identifying<br>
causal relations in neural circuits, however, it is not yet fully<br>
understood <em>when</em> more complex intervention strategies can provide<br>
additional inferential power, or <em>how</em> these experiments should be<br>
optimally designed. In this paper we demonstrate when and how<br>
closed-loop interventions can reveal the causal structure governing<br>
neural circuits. Drawing from ideas in causal inference , we describe<br>
the classes of models that can be distinguished by a given set of<br>
input-output experiments, and what experiments are necessary to uniquely<br>
determine specific causal relationships.</p>
<p>We first propose a mathematical framework that describes how open- and<br>
closed-loop interventions impact observable qualities of neural<br>
circuits. Using this framework, experimentalists propose a set of<br>
candidate hypotheses describing the potential causal structure of the<br>
circuit under study, and then select a series of interventions that best<br>
allows them to distinguish between these hypotheses. Using both simple<br>
controlled models and in silico models of spiking networks, we explore<br>
factors that govern the efficacy of these types of interventions. Guided<br>
by the results of this exploration, we present a set of recommendations<br>
that can guide the design of open- and closed-loop experiments to better<br>
uncover the causal structure underlying neural circuits.</p>
<p><strong>Inferring causal interactions from time series.</strong> A number of<br>
strategies have been proposed to detect causal relationships between<br>
observed variables. Wiener-Granger (or predictive) causality states that<br>
a variable <span class="mathjax-exps">$X$</span> &quot;Granger-causes&quot; <span class="mathjax-exps">$Y$</span> if <span class="mathjax-exps">$X$</span> contains information relevant<br>
to <span class="mathjax-exps">$Y$</span> that is not contained in <span class="mathjax-exps">$Y$</span> itself or any other variable . This<br>
concept has traditionally been operationalized with vector<br>
autoregressive models ; the requirement that <em>all</em> potentially causative<br>
variables be considered makes these notions of dependence susceptible to<br>
unobserved confounders .</p>
<p>Our work initially focuses on measures of directional interaction that<br>
are based on lagged correlations . These metrics look at the correlation<br>
of time series collected from pairs of nodes at various lags and detect<br>
peaks at negative time lags. Such peaks could indicate the presence of a<br>
direct causal relationship -- but they could also stem from indirect<br>
causal links or hidden confounders . In these bivariate correlation<br>
methods, it is thus necessary to consider patterns of correlation<br>
between many pairs of nodes in order to differentiate between direct,<br>
indirect, and confounding relationships . This distinguishes these<br>
strategies from some multivariate methods that &quot;control&quot; for the effects<br>
of potential confounders. While cross-correlation-based measures are<br>
generally limited to detecting linear functional relationships between<br>
nodes, their computational feasibility makes them a frequent metric of<br>
choice in experimental neuroscience work .</p>
<p>Other techniques detect directional interaction stemming from more<br>
general or complex relationships. Information-theoretic methods, which<br>
use information-based measures to assess the reduction in entropy<br>
knowledge of one variable provides about another, are closely related to<br>
Granger causality . The <em>transfer entropy</em><br>
<span class="mathjax-exps">$T_{X \to Y}(t) = I(Y_t \colon X_{&lt;t} \mid Y_{&lt;t})$</span> extends this notion<br>
to time series by measuring the amount of information present in <span class="mathjax-exps">$Y_t$</span><br>
that is not contained in the past of either <span class="mathjax-exps">$X$</span> or <span class="mathjax-exps">$Y$</span> (denoted <span class="mathjax-exps">$X_{&lt;t}$</span><br>
and <span class="mathjax-exps">$Y_{&lt;t}$</span>) . Using transfer entropy as a measure of causal<br>
interaction requires accounting for potential confounding variables; the<br>
<em>conditional transfer entropy</em><br>
<span class="mathjax-exps">$T_{X \to Y \mid Z}(t) = I(Y_t \colon X_{&lt;t} \mid Y_{&lt;t}, Z_{&lt;t})$</span><br>
conditions on the past of other variables to account for their potential<br>
confounding influence . Conditional transfer entropy can thus be<br>
interpreted as the amount of information present in <span class="mathjax-exps">$Y$</span> that is not<br>
contained in the past of <span class="mathjax-exps">$X$</span>, the past of <span class="mathjax-exps">$Y$</span>, or the past of other<br>
variables <span class="mathjax-exps">$Z$</span>.</p>
<p>To quantify the strength of causal interactions, information-theoretic<br>
and transfer-entropy-based methods typically require knowledge of the<br>
ground truth causal relationships that exist or an ability to perturb<br>
the system . In practice, these quantities are typically interpreted as<br>
&quot;information transfer,&quot; and a variety of estimation strategies and<br>
methods to automatically select the conditioning set (i.e., the<br>
variables and time lags that should be conditioned on) are used (e.g.,<br>
). Multivariate conditional transfer entropy approaches using various<br>
variable selection schemes can differentiate between direct<br>
interactions, indirect interactions, and common causes, but their<br>
results depend on choices such as the binning strategies used to<br>
discretize continuous signals, the specific statistical tests used, and<br>
the estimator used to compute transfer entropy . However, despite their<br>
mathematical differences, previous work has found that<br>
cross-correlation-based metrics and information-based metrics tend to<br>
produce qualitatively similar results, with similar patterns of true and<br>
false positives .</p>
<h2 class="mume-header" id="interventions-in-neuroscience-causal-inference">Interventions in neuroscience &amp; causal inference</h2>

<p>Data collected from experimental settings can provide more inferential<br>
power than observational data alone. For example, consider an<br>
experimentalist who is considering multiple causal hypotheses for two<br>
nodes under study, <span class="mathjax-exps">$x$</span> and <span class="mathjax-exps">$y$</span>: the hypothesis that <span class="mathjax-exps">$x$</span> is driving <span class="mathjax-exps">$y$</span>,<br>
the hypothesis that <span class="mathjax-exps">$y$</span> is driving <span class="mathjax-exps">$x$</span>, or the hypothesis that the two<br>
variables are being independently driven by a hidden confounder.<br>
Observational data revealing that <span class="mathjax-exps">$x$</span> and <span class="mathjax-exps">$y$</span> produce correlated<br>
time-series data is equally consistent with each of these three causal<br>
hypotheses, providing the experimentalist with no inferential power.<br>
Experimentally manipulating <span class="mathjax-exps">$x$</span> and observing the output of <span class="mathjax-exps">$y$</span>,<br>
however, allows the scientist to begin to establish which causal<br>
interaction pattern is at work. Consistent with intuition from<br>
neuroscience literature, a rich theoretical literature has described the<br>
central role of interventions in inferring causal structure from data .</p>
<p><img src="figures/core_figure_sketches/figure1_sketch.png" alt> &gt; <strong>Figure<br>
INTRO:</strong> Examples of the roles interventions have played in<br>
neuroscience. (A) <em>Passive observation</em> does not involve stimulating the<br>
brain. In this example, passive observational data is used to identify<br>
patients suffering from absence seizures. (B) <em>Open-loop stimulation</em><br>
involves recording activity in the brain after perturbing a region with<br>
a known input signal. Using systematic <em>open-loop stimulation<br>
experiments</em>, Penfield uncovered the spatial organization of how senses<br>
and movement are mapped in the cortex . (C) <em>Closed-loop control</em> uses<br>
feedback control to precisely specify activity in certain brain regions<br>
regardless of activity in other regions. Using closed-loop control, .</p>
<p>The inferential power of interventions is depends on <em>where</em> stimulation<br>
is applied: interventions on some portions of a system may provide more<br>
information about the system&apos;s causal structure than interventions in<br>
other areas. And interventions are also more valuable when they more<br>
effectively set the state of the system: &quot;perfect&quot; closed-loop control,<br>
which completely severs a node&apos;s activity from its inputs, are often<br>
more informative than &quot;soft&quot; interventions that only partially control a<br>
part of the system .</p>
<p>In experimental neuroscience settings, experimenters are faced with<br>
deciding between interventions that differ in both location and<br>
effectiveness. For example, stimulation can often only be applied to<br>
certain regions of the brain. And while experimenters may be able to<br>
exactly manipulate activity in some parts of the brain using closed-loop<br>
control, in other locations it may only be possible to apply weaker<br>
forms of intervention that perturb a region but do not manipulate its<br>
activity exactly to a desired state. In Section <code>X</code>, we compare the<br>
effectiveness of open-loop, closed-loop, and partially-effective<br>
closed-loop control.</p>
<p>Although algorithms designed to choose optimal interventions are often<br>
designed for simple models with strong assumptions,<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> they provide<br>
intuition that can aid practitioners seeking to design real-world<br>
experiments that provide as much scientific insight as possible.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup><br>
Importantly, the informativeness of interventions is often independent<br>
of the algorithm used to infer causal connections, meaning that certain<br>
interventions can reveal portions of a circuit&apos;s causal structure that<br>
would be impossible for <em>any</em> algorithm to infer from only observational<br>
data Matt to Adam: make sure this citation is in the right We similarly<br>
expect the results we demonstrate in this paper to both inform<br>
experimentalists and open avenues for further research.</p>
<h2 class="mume-header" id="representations-reachability">Representations &amp; reachability</h2>

<p>import &quot;/section_content/background_representation_reach.md&quot;</p>
<p><img src="figures/core_figure_sketches/circuit_walkthrough_3circuits_key_sketch.png" alt title="generated by /code/fig_circuit_walkthrough.py"><br>
&gt; <strong>Figure DEMO <em>(box format)</em>: Applying CLINC to distinguish a pair of<br>
circuits</strong> &gt; &gt; Consider the three-node identification problem shown in<br>
the figure above, in which the experimenter has identified three<br>
hypotheses for the causal structure of the circuit. These circuit<br>
hypotheses, shown as directed graphs in column 1, can each also be<br>
represented by an adjacency matrix of the form : for example, circuit A<br>
is represented by an adjacency matrix in which <span class="mathjax-exps">$w_{01}$</span>, <span class="mathjax-exps">$w_{20}$</span>, and<br>
<span class="mathjax-exps">$w_{21} \neq 0$</span>. Note that hypotheses A and C have direct connections<br>
between nodes 0 and 2; while hypothesis B does not have a direct<br>
connection between these nodes, computing the weighted reachability<br>
matrix <span class="mathjax-exps">$\widetilde{W}$</span> in circuit B an <em>indirect</em> connection exists<br>
through the path 2 <span class="mathjax-exps">$\to$</span> 1 <span class="mathjax-exps">$\to$</span> 0 (illustrated in gray in column 2). &gt;<br>
&gt; Because there are direct or indirect connections between each pair of<br>
nodes, passive observation of each hypothesized circuit would reveal<br>
that each pair of nodes is correlated (column 3). These three hypotheses<br>
are therefore difficult to distinguish<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> for an experimentalist who<br>
performs only passive observation, but can be distinguished through<br>
stimulation. &gt; &gt; Column 4 shows the impact on observed correlations of<br>
performing <em>open-loop</em> control on node 1. In hypothesis A, node 1 is not<br>
a driver of other nodes, so open-loop stimulation at this site will not<br>
increase the correlation between the signal observed at node 1 and other<br>
nodes. The path from node 1 to 0 in hypotheses B and C, meanwhile,<br>
causes the open-loop stimulation at node 1 to <em>increase</em> the observed<br>
correlation between nodes 1 and 0. An experimenter can thus distinguish<br>
between hypothesis A and the other two hypotheses by appling open-loop<br>
control and observing the resulting pattern of correlations (column 4).<br>
However, this pattern of open-loop stimulation would not allow the<br>
experimenter to distinguish between hypotheses B and C. &gt; &gt;<br>
<em>Closed-loop</em> control (columns 5 and 6) can provide the experimenter<br>
with even more inferential power. Column 5 shows the resulting adjacency<br>
matrix when this closed-loop control is applied to node 1. In each<br>
hypothesis, the impact of this closed-loop control is to remove the<br>
impact of other nodes on node 1, because when perfect closed-loop is<br>
applied the activity of node 1 is completely independent of other nodes.<br>
(These severed connections are depicted in column 5 by dashed lines.) In<br>
hypothesis B, this also results in the elimation of the indirect<br>
connection from node 2 to node 1. The application of closed-loop control<br>
at node 1 thus results in a different observed correlation structure in<br>
each of the three circuit hypotheses (column 6). This means that the<br>
experimenter can therefore distinguish between these circuit hypotheses<br>
by applying closed-loop control -- a task not possible with passive<br>
observation or open-loop control.</p>
<h1 class="mume-header" id="theory-prediction">Theory / Prediction</h1>

<p><img src="figures/core_figure_sketches/methods_overview_pipeline_sketch.png" alt><br>
&gt; <strong>Figure OVERVIEW:</strong> ...</p>
<h2 class="mume-header" id="predicting-correlation-structure-theory">Predicting correlation structure (theory)</h2>

<p>A linear-Gaussian circuit can be described by 1) the variance of the<br>
gaussian private (independent) noise at each node, and 2) the weight of<br>
the linear relationships between each pair of connected nodes. Let<br>
<span class="mathjax-exps">$s \in \mathbb{R}^p$</span> denote the variance of each of the <span class="mathjax-exps">$p$</span> nodes in the<br>
circuit, and <span class="mathjax-exps">$W \in \mathbb{R}^{p \times p}$</span> denote the matrix of<br>
connection strengths such that<br>
</p><div class="mathjax-exps">$$W_{ij} = \text{strength of $i \to j$ connection}.$$</div><p></p>
<p>Note that <span class="mathjax-exps">$\left[(W^T) s\right]_j$</span> gives the variance at node <span class="mathjax-exps">$j$</span> due to<br>
length-1 (direct) connections, and more generally,<br>
<span class="mathjax-exps">$\left[ (W^T)^k s \right]_j$</span> gives the variance at node <span class="mathjax-exps">$j$</span> due to<br>
length-<span class="mathjax-exps">$k$</span> (indirect) connections. The <em>total</em> variance at node <span class="mathjax-exps">$j$</span> is<br>
thus <span class="mathjax-exps">$\left[ \sum_{k=0}^{\infty} (W^T)^k s \right]_j$</span>.</p>
<p>Our goal is to connect private variances and connection strengths to<br>
observed pairwise correlations in the circuit. Defining<br>
<span class="mathjax-exps">$X \in \mathbb{R}^{p \times n}$</span> as the matrix of <span class="mathjax-exps">$n$</span> observations of<br>
each node, we have<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> </p><div class="mathjax-exps">$$\begin{aligned}     \Sigma &amp;= \mathrm{cov}(X) = \mathbb{E}\left[X X^T\right] \\     &amp;= (I-W^T)^{-1} \mathrm{diag}(s) (I-W^T)^{-T} \\     &amp;= \widetilde{W} \mathrm{diag}(s) \widetilde{W}^T, \end{aligned}$$</div> where <span class="mathjax-exps">$\widetilde{W} = \sum_{k=0}^{\infty} (W)^k$</span> denotes the<br>
<em>weighted reachability matrix</em>, whose <span class="mathjax-exps">$(i,j)^\mathrm{th}$</span> entry<br>
indicates the total influence of node <span class="mathjax-exps">$i$</span> on node <span class="mathjax-exps">$j$</span> through both<br>
direct and indirect connections.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> That is, <span class="mathjax-exps">$\widetilde{W}_{ij}$</span> tells<br>
us how much variance at node <span class="mathjax-exps">$j$</span> would result from injecting a unit of<br>
private variance at node <span class="mathjax-exps">$i$</span>. We can equivalently write<br>
<span class="mathjax-exps">$\Sigma_{ij} = \sum_{k=1}^p \widetilde{W}_{ik} \widetilde{W}_{jk} s_k$</span>.<p></p>
<p>Under passive observation, the squared correlation coefficient can thus<br>
be written as </p><div class="mathjax-exps">$$\begin{aligned}     r^2(i,j) &amp;= \frac{\Sigma_{ij}}{\Sigma_{ii} \Sigma_{jj}} \\     &amp;= \frac{\left( \sum_{k=1}^p \widetilde{W}_{ik} \widetilde{W}_{jk} s_k \right)^2}{\left(\sum_{k=1}^p \widetilde{W}_{ik}^2 s_k\right)\left(\sum_{k=1}^p \widetilde{W}_{jk}^2 s_k\right)}. \end{aligned}$$</div><p></p>
<p>This framework also allows us to predict the impact of open- and<br>
closed-loop control on the pairwise correlations we expect to observe.<br>
To model the application of open-loop control on node <span class="mathjax-exps">$c$</span>, we add an<br>
arbitrary amount of private variance to <span class="mathjax-exps">$s_c$</span>:<br>
<span class="mathjax-exps">$s_c \leftarrow s_c + s_c^{(OL)}$</span>. To model the application of<br>
closed-loop control on node <span class="mathjax-exps">$c$</span>, we first sever inputs to node <span class="mathjax-exps">$c$</span> by<br>
setting <span class="mathjax-exps">$W_{k,c} = 0$</span> for <span class="mathjax-exps">$k = 1, \dots p$</span>, and then set the private<br>
variance of node <span class="mathjax-exps">$c$</span> by setting <span class="mathjax-exps">$s_c$</span> to any arbitrary value. Because<br>
<span class="mathjax-exps">$c$</span>&apos;s inputs have been severed, this private noise will become exactly<br>
node <span class="mathjax-exps">$c$</span>&apos;s output variance.</p>
<h1 class="mume-header" id="simulation-methods">Simulation Methods</h1>

<h2 class="mume-header" id="modeling-network-structure-and-dynamics">Modeling network structure and dynamics</h2>

<p>We sought to understand both general principles (abstracted across<br>
particulars of network implementation) as well as some practical<br>
considerations introduced by dealing with spikes and synapses.</p>
<h3 class="mume-header" id="stochastic-network-dynamics">Stochastic network dynamics</h3>

<p>The first approach is accomplished with a network of nodes with gaussian<br>
noise sources, linear interactions, and linear dynamics. The second<br>
approach is achieved with a network of nodes consisting of populations<br>
of leaky integrate-and-fire (LIF) neurons. These differ from the simpler<br>
case in their nonlinear-outputs, arising from inclusion of a spiking<br>
threshold. Interactions between neurons happen through spiking synapses,<br>
meaning information is passed between neurons sparsely in time<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>.</p>
<p><em>Neuron dynamics:</em> </p><div class="mathjax-exps">$$\frac{dV}{dt} = \frac{V_0 + I - V}{\tau_m} + \sigma_m \sqrt{\tau_m} \xi(t)$$</div><p></p>
<h3 class="mume-header" id="time-resolvable-interactions">Time-resolvable interactions</h3>

<p>Additionally we study two domains of interactions between populations;<br>
contemporaneous and delay-resolvable connections. These domains<br>
represent the relative timescales of measurement versus timescale of<br>
synaptic delay. </p><div class="mathjax-exps">\[\^cases\]</div> <div class="mathjax-exps">\[\^cases\]</div>: cases doesnt work with pandoc<br>
yet, also want to talk about positive and negative lags here In the<br>
delay-resolvable domain, directionality of connections may be inferred<br>
even under passive observations by looking at temporal precedence -<br>
whether the past of one signal is more strongly correlated with future<br>
lags of another signal <em>(i.e.&#xA0;cross-correlation)</em>. In the<br>
contemporaneous domain, network influences act within the time of a<br>
single sample<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> so this temporal precedence clue is lost (although<br>
directionality can still be inferred in the presence of intervention).<p></p>
<p>The following work is presented with the linear-Gaussian and<br>
contemporaneous domains as the default for simplicity and conciseness.</p>
<h3 class="mume-header" id="code-implementation">Code implementation</h3>

<p>Software for data generation, analysis, and plotting is available at<br>
<a href="https://github.com/awillats/clinc">https://github.com/awillats/clinc</a>. Both linear-gaussian and spiking<br>
networks are simulated with code built from the<br>
<a href="https://elifesciences.org/articles/47314">Brian2</a> spiking neural<br>
network simulator. This allows for highly modular code with easily<br>
interchanged neuron models and standardized output preprocessing and<br>
plotting. It was necessary to write an additional custom extension to<br>
Brian2 in order to capture delayed linear-gaussian interactions,<br>
available at<br>
<a href="https://github.com/awillats/brian_delayed_gaussian">brian_delayed_gaussian</a>.<br>
With this added functionality, it is possible to compare the equivalent<br>
network parameters only changing linear-gaussian versus spiking dynamics<br>
and inspect differences solely due to spiking.</p>
<p><em>see <a href="_network_parameters_table.md">_network_parameters_table.md</a> for<br>
list of relevant parameters</em></p>
<h2 class="mume-header" id="implementing-interventions">Implementing interventions</h2>

<p><img src="figures/core_figure_sketches/figure1_sketch.png" alt></p>
<p>To study the effect of various interventions we simulated inputs to<br>
nodes in a network. In the <strong>passive setting</strong>, nodes receive additive<br>
drive from <em>private</em> Gaussian noise sources common to all neurons within<br>
a node, but independent across nodes. The variance of this noise is<br>
specified by <span class="mathjax-exps">$\sigma_m \sqrt{\tau_m}$</span>.<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></p>
<p></p><div class="mathjax-exps">$$\frac{dV}{dt} = \frac{V_0 + I - V}{\tau_m} + \sigma_m \sqrt{\tau_m} \xi(t)$$</div><p></p>
<p>To emulate <strong>open-loop intervention</strong> we simulated current injection<br>
from an external source. This is intended to represent experiments<br>
involving stimulation from microelectrodes or optogenetics <em>(albeit<br>
simplifying away any impact of actuator dynamics)</em>. By default,<br>
open-loop intervention is specified as white noise sampled at each<br>
timestep from Gaussian distribution with mean and variance <span class="mathjax-exps">$\mu_{intv.}$</span><br>
and <span class="mathjax-exps">$\sigma^2_{intv.}$</span><sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></p>
<p></p><div class="mathjax-exps">$$I_{open-loop} \sim \mathcal{N}(\mu_{intv.},\,\sigma^{2}_{intv.})\\$$</div> Ignoring the effect of signal means in the linear-Gaussian setting:<br>
<div class="mathjax-exps">$$X_k = f(\sigma^2_m, \sigma^{2}_{intv.})$$</div> <code>per-node indexing needs resolving here also</code><p></p>
<p>Ideal <strong>closed-loop control</strong> is able to overwrite the output of a node,<br>
setting it precisely to the specified target.<br>
<code>making up notation as I go here, needs tightening up:</code> </p><div class="mathjax-exps">$$\begin{aligned} T &amp;\sim \mathcal{N}(\mu_{intv.},\,\sigma^{2}_{intv.}) \\ I_{closed-loop} &amp;= f(X, T)  \\ X_k | CL_{k} &amp;\approx T \end{aligned}$$</div> Note that in this setting, the <em>output</em> of a node <span class="mathjax-exps">$X_k$</span> under<br>
closed-loop control is identical to the target, therefore <div class="mathjax-exps">$$X_k | CL_{k} = f(\sigma^{2}_{intv.}) \perp \sigma^2_m$$</div> In practice, near-ideal control is only possible with very fast<br>
measurement and computation relative to the network&apos;s intrinsic<br>
dynamics, such as in the case of dynamic clamp<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>. To demonstrate a<br>
broader class of closed-loop interventions (such as those achievable<br>
with extracellular recording and stimulation), imperfect &quot;partial&quot;<br>
control is simulated by linearly interpolating the output of each node<br>
between the target <span class="mathjax-exps">$T$</span> and the uncontrolled output based on a control<br>
effectiveness parameter <span class="mathjax-exps">$\gamma$</span><p></p>
<p></p><div class="mathjax-exps">$$X | CL_{k, \gamma} = \gamma T + (1-\gamma) X$$</div><p></p>
<p>In the full discrete-time simulation, closed-loop interventions are<br>
instead simulated through a proportional-integral-derivative (PID)<br>
control policy with control efficacy determined functionally by the<br>
strength of controller gains <span class="mathjax-exps">$K = \{k_P, k_I, k_D\}$</span> relative to the<br>
dynamics of the network.</p>
<p></p><div class="mathjax-exps">$$I_{PID} = \text{PID}(X,T| K)$$</div><p></p>
<p>Another interesting intervention to study is <strong>open-loop replay of a<br>
closed-loop stimulus</strong>, <em>that is</em> taking a particular injected current<br>
<span class="mathjax-exps">$I_{CL,\,prev}$</span> used to drive nodes to a target <span class="mathjax-exps">$T_{prev}$</span> and adding it<br>
back to the network in a separate trial.</p>
<p>Because the instantiation of noise in the network will be different from<br>
trial to trial, this &quot;replay&quot; stimulus will no longer adapt<br>
sample-by-sample (therefore it should be considered open-loop) and the<br>
node&apos;s output cannot be expected to match the target precisely, however<br>
the statistics of externally applied inputs will be the same. In effect,<br>
the comparison between closed-loop and open-loop replay conditions<br>
reveals the specific effect of feedback intervention while controlling<br>
for any confounds from input statistics.</p>
<h2 class="mume-header" id="extracting-circuit-estimates">Extracting circuit estimates</h2>

<p>While a broad range of techniques<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup> exist for inferring functional<br>
relationships from observational data, <code>(for the majority of this work)</code><br>
we choose to focus on simple bivariate correlation as a measure of<br>
dependence in the linear-Gaussian network. The impact of intervention on<br>
this metric is analytically tractable <em>(see<br>
<a href="methods1_predicting_correlation.md">methods1_predicting_correlation.md</a>)</em>,<br>
and can be thought of as a prototype<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup> for more sophisticated<br>
measures of dependence such as time-lagged cross-correlations, bivariate<br>
and multivariate transfer entropy.</p>
<p>We implement a naive comparison strategy to estimate the circuit<br>
adjacency from emprical correlations; Thresholded empirical correlation<br>
matrices are compared to correlation matrices predicted from each<br>
circuit in a hypothesis set. Any hypothesized cirucits which are<br>
predicted to have a similar correlation structure as is observed<br>
(i.e.&#xA0;corr. mats equal after thresholding) are marked as &quot;plausible<br>
circuits.&quot;<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup> If only one circuit amongst the hypothesis set is a<br>
plausible match, this is considered to be the estimated circuit. The<br>
threshold for &quot;binarizing&quot; the empirical correlation matrix is treated<br>
as a hyperparameter to be swept at the time of analysis.<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup></p>
<h2 class="mume-header" id="information-theoretic-measures-of-hypothesis-ambiguity">Information-theoretic measures of hypothesis ambiguity</h2>

<p><em>see <a href="_steps_of_inference.md">_steps_of_inference.md</a> for entropy<br>
writeup</em></p>
<h1 class="mume-header" id="results">Results</h1>

<h2 class="mume-header" id="impact-of-intervention-on-estimation-performance">Impact of intervention on estimation performance</h2>

<h3 class="mume-header" id="intervening-provides-categorical-improvements-in-inference-power-beyond-passive-observation">Intervening provides (categorical) improvements in inference power beyond passive observation</h3>

<p>Next, we apply (steps 1-3 of) this circuit search procedure to a<br>
collection of closely related hypotheses for 3 interacting nodes<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> to<br>
illustrate the impact of intervention. &#x1F6A7;<br>
<code>most of the story in the figure caption for now</code> &#x1F6A7;</p>
<p><code>&lt;a id=&quot;fig-disambig&quot;&gt;</code>{=html}<code>&lt;/a&gt;</code>{=html}<br>
<img src="figures/core_figure_sketches/circuit_entropy_sketch.png" alt> &gt; <strong>Figure<br>
DISAMBIG: Interventions narrow the set of hypotheses consistent with<br>
observed correlations</strong> <em>source: <a href="https://docs.google.com/drawings/d/1CBp1MhOW7OGNuBvo7OkIuzqnq8kmN8EEX_AkFuKpVtM/edit">google<br>
drawing</a></em><br>
&gt;<strong>(A)</strong> Directed adjacency matrices represent the true and<br>
hypothesized causal circuit structure &gt;<strong>(B)</strong> Directed reachability<br>
matrices represent the direct <em>(black)</em> and indirect <em>(grey)</em> influences<br>
in a network. Notably, different adjacency matrices can have equivalent<br>
reachability matrices making distinguishing between similar causal<br>
structures difficult, even with open-loop control. &gt;<strong>(C)</strong><br>
Correlations between pairs of nodes. Under passive observation, the<br>
direction of influence is difficult to ascertain. In densely connected<br>
networks, many distinct ground-truth causal structures result in similar<br>
&quot;all correlated with all&quot; patterns providing little information about<br>
the true structure. &gt;<strong>(D-F)</strong> The impact of open-loop intervention at<br>
each of the nodes in the network is illustrated by modifications to the<br>
passive correlation pattern. Thick orange<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup> edges denote correlations<br>
which increase above their baseline value with high variance open-loop<br>
input. Thin blue<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup> edges denote correlations which decrease, often as<br>
a result of increased connection-independent &quot;noise&quot; variance in one of<br>
the participating nodes. Grey edges are unaffected by intervention at<br>
that location. &gt; A given hypotheses set (A) will result in an<br>
&quot;intervention-specific fingerprint&quot;, that is a distribution of<br>
frequencies for observing patterns of modified correlations <em>(across a<br>
single row within D-F)</em>. If this fingerprint contains many examples of<br>
the same pattern of correlation (such as <strong>B</strong>), many hypotheses<br>
correspond to the same observation, and that experiment contributes low<br>
information to distinguish between structures. A maximally informative<br>
intervention would produce a unique pattern of correlation for each<br>
member of the hypothesis set. &#x1F6A7;<code>caption too long</code></p>
<p><strong>Why does closed-loop control provide a categorical advantage?</strong><br>
Because it severs indirect links <code>is this redundant with intro?</code><br>
<code>needs to be backed here up by aggregate results?</code> - this is especially<br>
relevant in recurrently connected networks where the reachability matrix<br>
becomes more dense. - more stuff is connected to other stuff, so there<br>
are more indirect connections, and the resulting correlations look more<br>
similar (more circuits in the equivalence class) - patterns of<br>
correlation become more specific with increasing intervention strength -<br>
more severed links &#x2192; more unique adjacency-specific patterns of<br>
correlation</p>
<blockquote>
<p><strong>Where you intervene</strong><sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> strongly determines the inference power<br>
of your experiment. <strong>secondary point:</strong> having (binary) prediction<br>
helps capture this relationship</p>
</blockquote>
<h3 class="mume-header" id="stronger-intervention-shapes-correlation-resulting-in-more-data-efficient-inference-with-less-bias">Stronger intervention shapes correlation, resulting in more data-efficient inference with less bias</h3>

<p>While a primary advantage of closed-loop interventions for circuit<br>
inference is its ability to functionally lesion indirect connections,<br>
another, more nuanced <code>(quantitative)</code> advantage of closed-loop control<br>
lies in its capacity to bidirectionally control output variance. While<br>
the variance of an open-loop stimulus can be titrated to adjust the<br>
output variance at a node, in general, an open-loop stimulus cannot<br>
reduce this variance below its instrinsic<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup> variability. That is, if<br>
the system is linear with gaussian noise,</p>
<p></p><div class="mathjax-exps">$$\mathbb{V}_{i}(C|S=\text{open},\sigma^2_S) \geq \mathbb{V}_{i}(C)$$</div><br>
More specifically, if the open-loop stimulus is statistically<br>
independent from the intrinsic variability<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup><br>
<div class="mathjax-exps">$$\mathbb{V}_{i}(C|S=\text{open},\sigma^2_S) = \mathbb{V}_{i}(C) + \sigma^2_S$$</div><br>
Applying closed-loop to a linear gaussian circuit:<p></p>
<p></p><div class="mathjax-exps">$$\begin{aligned} \mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) &amp;= \sigma^2_S  \\ \mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) &amp;\perp \mathbb{V}_{i}(C) \end{aligned}$$</div><p></p>
<h4 class="mume-header" id="impact-of-intervention-location-and-variance-on-pairwise-correlations">Impact of intervention location and variance on pairwise correlations</h4>

<p><a href="methods1_predicting_correlation.md">related methods</a></p>
<p>We have shown that closed-loop interventions provide more flexible<br>
control over output variance of nodes in a network, and that shared and<br>
independent sources of variance determine pairwise correlations between<br>
node outputs. Together, this suggests closed-loop interventions may<br>
allow us to shape the pattern of correlations with more degrees of<br>
freedom</p><div class="mathjax-exps">\[\^dof\]</div> <code>[why do we want to?...]</code><p></p>
<p>One application of this increased flexibility is to increase<br>
correlations associated with pairs of directly correlated nodes, while<br>
decreasing spurious correlations associated with pairs of nodes without<br>
a direct connection (but perhaps are influenced by a common input, or<br>
are connected only indirectly). While &quot;correlation does not imply<br>
causation,&quot; intervention may decrease the gap between the two.</p>
<p>Our hypothesis is that this shaping of pairwise correlations will result<br>
in reduced false positive edges in inferred circuits, &quot;unblurring&quot; the<br>
indirect associations that would otherwise confound circuit inference.<br>
However care must be taken, as this strategy relies on a hypothesis for<br>
the ground truth adjacency and may also result in a &quot;confirmation bias&quot;<br>
as new spurious correlations can be introduced through closed-loop<br>
intervention.</p>
<p>The impact of intervention on correlations can be summarized through the<br>
co-reachability <span class="mathjax-exps">$\text{CoReach}(i,j|S_k)$</span>. A useful distillation of this<br>
mapping is to understand the sign of <span class="mathjax-exps">$\frac{dR_{ij}}{dS_k}$</span>, that is<br>
whether increasing the variance of an intervention at node <span class="mathjax-exps">$k$</span> increases<br>
or decreases the correlation between nodes <span class="mathjax-exps">$i$</span> and <span class="mathjax-exps">$j$</span></p>
<p>In a simulated network A&#x2192;B <a href="#fig-var">(fig.&#xA0;variance)</a> we demonstrate<br>
predicted and emprirical correlations between a pair of nodes as a<br>
function of intervention type, location, and variance. A few features<br>
are present which provide a general intuition for the impact of<br>
intervention location in larger circuits: First, interventions<br>
&quot;upstream&quot; of a true connection <a href="#fig-var">(lower left, fig.&#xA0;variance)</a><br>
tend to increase the connection-related variance, and therefore<br>
strengthen the observed correlations.<br>
</p><div class="mathjax-exps">$$\text{Reach}(S_k&#x2192;i) \neq 0 \\ \text{Reach}(i&#x2192;j) \neq 0 \\ \frac{dR}{dS_k} &gt; 0$$</div><p></p>
<p>Second, interventions affecting only the downstream node <a href="#fig-var">(lower right,<br>
fig.&#xA0;variance)</a> of a true connection introduce variance which<br>
is independent of the connection A&#x2192;B, decreasing the observed<br>
correlation.<br>
</p><div class="mathjax-exps">$$\text{Reach}(S_k &#x2192; j) = 0 \\ \text{Reach}(S_k &#x2192; j) \neq 0 \\ \frac{dR}{dS_k} &lt; 0$$</div><p></p>
<p>Third, interventions which reach both nodes will tend to increase the<br>
observed correlations <a href="#fig-var">(upper left, fig.&#xA0;variance)</a>, moreover<br>
this can be achieved even if no direct connection <span class="mathjax-exps">$i&#x2192;j$</span> exists.<br>
</p><div class="mathjax-exps">$$\text{Reach}(S_k &#x2192; i) \neq 0 \\ \text{Reach}(S_k &#x2192; j) \neq 0 \\ \text{Reach}(i &#x2192; j) = 0 \\ \frac{dR}{dS_k} &gt; 0$$</div><p></p>
<p>Notably, the impact of an intervention which is a &quot;common cause&quot; for<br>
both nodes depends on the relative weighted reachability between the<br>
source and each of the nodes. Correlations induced by a common cause are<br>
maximized when the input to each node is equal, that is<br>
<span class="mathjax-exps">$\widetilde{W}_{S_k&#x2192;i} \approx \widetilde{W}_{S_k&#x2192;j}$</span> (upper right * in<br>
<a href="#fig-var">fig.&#xA0;variance</a>). If i&#x2192;j are connected<br>
<span class="mathjax-exps">$\widetilde{W}_{S_k&#x2192;i} \gg \widetilde{W}_{S_k&#x2192;j}$</span> results in an<br>
variance-correlation relationship similar to the &quot;upstream source&quot; case<br>
(increasing source variance increases correlation<br>
<span class="mathjax-exps">$\frac{dR}{dS_k} &gt; 0$</span>), while<br>
<span class="mathjax-exps">$\widetilde{W}_{S_k&#x2192;i} \ll \widetilde{W}_{S_k&#x2192;j}$</span> results in a<br>
relationship similar to the &quot;downstream source&quot; case<br>
(<span class="mathjax-exps">$\frac{dR}{dS_k} &lt; 0$</span>)<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup></p>
<p><code>&lt;a id=&quot;fig-predict&quot;&gt;</code>{=html}<code>&lt;/a&gt;</code>{=html}<br>
<img src="figures/misc_figure_sketches/quant_r2_prediction_common.png" alt><br>
<img src="figures/from_code/bidirectional_correlation.png" alt title="generated by sweep_gaussian_SNR.py"></p>
<blockquote>
<p>&#x1F6A7;(Final figure will be a mix of these two panels, caption will need<br>
updating) <strong>Figure VAR: Location, variance, and type of intervention<br>
shape pairwise correlations</strong> <strong>(CENTER)</strong> A two-node linear gaussian<br>
network is simulated with a connection from A&#x2192;B. Open-loop<br>
interventions <em>(blue)</em> consist of independent gaussian inputs with a<br>
range of variances <span class="mathjax-exps">$\sigma^2_S$</span>. Closed-loop interventions <em>(orange)</em><br>
consist of feedback control with an independent gaussian target with a<br>
range of variances. <em>Incomplete closed-loop interventions result in<br>
node outputs which are a mix of the control target and network-driven<br>
activity</em>. Connections from sources to nodes are colored by their<br>
impact on correlations between A and B; green denotes <span class="mathjax-exps">$dR/dS &gt; 0$</span>, red<br>
denotes <span class="mathjax-exps">$dR/dS&lt;0$</span>. <strong>(lower left)</strong> Intervention &quot;upstream&quot; of the<br>
connection A&#x2192;B increases the correlation <span class="mathjax-exps">$r^2(A,B)$</span>. <strong>(lower right)</strong><br>
Intervention at the terminal of the connection A&#x2192;B decreases the<br>
correlation <span class="mathjax-exps">$r^2(A,B)$</span> by adding connection-independent noise.<br>
<strong>(upper left)</strong> Intervention with shared inputs to both nodes<br>
generally increases <span class="mathjax-exps">$r^2(A,B)$</span>, <em>(even without A&#x2192;B, see supplement)</em>.<br>
<strong>(upper right)</strong> The impact of shared interventions depends on<br>
relative weighted reachability<br>
<span class="mathjax-exps">$\text{Reach}(S_k&#x2192;A) / \text{Reach}(S_k&#x2192;B)$</span>, with highest correlations<br>
when these terms are matched (see *) Closed-loop interventions<br>
*(orange)* generally result in larger changes in correlation across<br>
<span class="mathjax-exps">$\sigma^2_S$</span> than the equivalent open-loop intervention. Closed-loop<br>
control at B effectively lesions the connection A&#x2192;B, resulting in<br>
near-zero correlation. <sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup></p>
</blockquote>
<p>&#x1F6A7; The change in correlation as a function of changing intervention<br>
variance (<span class="mathjax-exps">$\frac{dr^2_{ij}}{dS}$</span>) can therefore be used as an additional<br>
indicator of presence/absence and directionality of the connection<br>
between A,B <em>(see <a href="fig-disambig">fig.&#xA0;disambig. D.)</a>)</em> &#x1F6A7;</p>
<p><a href="#fig-var">Fig. variance</a> also demonstrates the relative dynamic range<br>
of correlations achievable under passive, open- and closed-loop<br>
intervention. In the passive case, correlations are determined by<br>
instrinsic properties of the network <span class="mathjax-exps">$\sigma^2_{base}$</span>. These properties<br>
have influence over the observed correlations in a way that can be<br>
difficult to separate from differences due to the ground-truth circuit.<br>
With open-loop intervention we can observe the impact of increasing<br>
variance at a particular node, but the dynamic range of achievable<br>
correlations is bounded by not being able to reduce variance below its<br>
baseline level. With closed-loop control, the bidirectional control of<br>
the output variance for a node means a much wider range of correlations<br>
can be achieved <a href="#fig-var">(blue v.s. orange in fig.&#xA0;variance)</a>,<br>
resulting in a more sensitive signal reflecting the ground-truth<br>
connectivity.</p>
<p><em>see also<br>
<a href="results1B_data_efficiency_and_bias.md">results1B_data_efficiency_and_bias.md</a></em></p>
<h1 class="mume-header" id="discussion">Discussion</h1>

<h3 class="mume-header" id="limitations">limitations</h3>

<p>The examples explored in this work simplify several key features that<br>
may have relevant contributions to circuit identification in practical<br>
experiments. </p><div class="mathjax-exps">\[...\]</div><p></p>
<p><code>full observability</code></p>
<h3 class="mume-header" id="results-summary-summary-of-value-closed-loop-generally">results summary &#x2192; summary of value closed-loop generally</h3>

<p>Closed-loop control has the disadvantages of being more complex to<br>
implement and requires specialized real-time hardware and software,<br>
however it has been shown to have multifaceted usefulness in clinical<br>
and basic science applications. Here we focused on two advantages in<br>
particular; First, the capacity for functional lesioning which<br>
(reversibly) severs inputs to nodes and second, closed-loop control&apos;s<br>
capacity to precisely shape variance across nodes. Both of these<br>
advantages facilitate opportunities for closed-loop intervention to<br>
reveal more circuit structure than passive observation or even open-loop<br>
experiments.</p>
<h3 class="mume-header" id="summary-of-guidelines-for-experimenters">summary of guidelines for experimenters</h3>

<p>In studying the utility of various intervention for circuit inference we<br>
arrived at a few general guidelines which may assist experimental<br>
neuroscientists in designing the right intervention for the quesiton at<br>
hand. First, more ambiguous hypotheses sets require &quot;stronger&quot;<br>
interventions to distinguish. Open-loop intervention may be sufficient<br>
to determine directionality of functional relationships, but as larger<br>
numbers of similar hypotheses </p><div class="mathjax-exps">\[...\]</div> closed-loop intervention reduces<br>
the hypothesis set more efficiently. Second, we find that dense networks<br>
with strong reciprocal connections tend to result in many equivalent<br>
circuit hypotheses, but that well-placed closed-loop control can disrupt<br>
loops and simplify correlation structure to be more identifiable.<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup><br>
Recurrent loops are a common feature of neural circuit, and represent<br>
key opportunities for successful closed-loop intervention. The same is<br>
true for circuits with strong indirect correlations<p></p>
<p><code>hidden confounds</code></p>
<h3 class="mume-header" id="funnel-out-future-work-broad-impact">&quot;funnel out&quot;, future work &#x2192; broad impact</h3>

<p><code>sequential experimental design</code></p>
<p><em>see<br>
<a href="../../sketches_and_notation/discussion/limitations_future_work.md">limitations_future_work.md</a></em></p>
<h1 class="mume-header" id="references">References</h1>

<p><em>see <a href="https://github.com/shd101wyy/markdown-preview-enhanced/blob/master/docs/pandoc-bibliographies-and-citations.md">pandoc<br>
pandoc-citations</a></em></p>
<h1 class="mume-header" id="supplement">Supplement</h1>

<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>may end up discussing quantitative advantages such as<br>
bidirectional variance (and correlation) control. If that&apos;s a strong<br>
focus in the results, should be talked about more in the abstract<br>
also <a href="#fnref1" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn2" class="footnote-item"><p>These assumptions are typically on properties such as the types of<br>
functional relationships that exist in circuits, the visibility and<br>
structure of confounding relationships, and noise statistics. <a href="#fnref2" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn3" class="footnote-item"><p>if citations needed here, could start by looking for a good<br>
high-level reference in either or . (Both of these papers are pretty<br>
technical, so likely wouln&apos;t be great citations on their own.) <a href="#fnref3" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn4" class="footnote-item"><p>saying &quot;difficult to distinguish&quot; instead of &quot;indistinguishable&quot;<br>
here since the magnitudes of the correlations could also be<br>
informative with different assumptions <a href="#fnref4" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn5" class="footnote-item"><p>To see this, denote by <span class="mathjax-exps">$E \in \mathbb{R}^{p \times n}$</span> the matrix<br>
of <span class="mathjax-exps">$n$</span> private noise observations for each node. Note that<br>
<span class="mathjax-exps">$X = W^T X + E$</span>, so <span class="mathjax-exps">$X = E(I-W^T)^{-1}$</span>. The covariance matrix<br>
<span class="mathjax-exps">$\Sigma = \mathrm{cov}(X) = \mathbb{E}\left[X X^T\right]$</span> can then<br>
be written as<br>
<span class="mathjax-exps">$\Sigma = \mathbb{E}\left[ (I-W^T)^{-1} E E^T (I-W^T)^{-1} \right] = (I-W^T)^{-1} \mathrm{cov}(E) (I-W^T)^{-T} = (I-W^T)^{-1} \mathrm{diag}(s) (I-W^T)^{-T}$</span>. <a href="#fnref5" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn6" class="footnote-item"><p>We can use <span class="mathjax-exps">$p-1$</span> as an upper limit on the sum<br>
<span class="mathjax-exps">$\widetilde{W} = \sum_{k=0}^{\infty} W^k$</span> when there are no<br>
recurrent connections. <a href="#fnref6" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn7" class="footnote-item"><p>However, depending on overall firing rates and population sizes,<br>
this sparse spike-based transmission can be coarse-grained to a<br>
firing-rate-based model. <a href="#fnref7" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn8" class="footnote-item"><p>the effective <span class="mathjax-exps">$\Delta_{sample}$</span> would be broadened in the presence<br>
of jitter in connection delay, measurement noise, or temporal<br>
smoothing applied post-hoc, leading <a href="#fnref8" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn9" class="footnote-item"><p>need to triple check indexing w.r.t. nodes, neurons <a href="#fnref9" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn10" class="footnote-item"><p>need to resolve differences in implementation between<br>
contemporaneous and voltage simulation cases <a href="#fnref10" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn11" class="footnote-item"><p>NEED dynamic clamp refs -<br>
<a href="http://www.scholarpedia.org/article/Dynamic_clamp">http://www.scholarpedia.org/article/Dynamic_clamp</a> <a href="#fnref11" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn12" class="footnote-item"><p><em>inference techniques mentioned in the intro...</em> <a href="#fnref12" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn13" class="footnote-item"><p>what does &quot;prototype&quot; mean here? something like MI and corr are<br>
equivalent in the linear-Gaussian case, ... <a href="#fnref13" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn14" class="footnote-item"><p>TODO? formalize notation for this <a href="#fnref14" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn15" class="footnote-item"><p>not sure how important this is. would prefer to set this<br>
threshold at some ad-hoc value since we&apos;re sweeping other<br>
properties. But a more in-depth analysis could look at a<br>
receiver-operator curve with respect to this threshold <a href="#fnref15" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn16" class="footnote-item"><p>nodes in such a graphical model may represent populations of<br>
neurons, distinct cell-types, different regions within the brain, or<br>
components of a latent variable represented in the brain. <a href="#fnref16" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn17" class="footnote-item"><p>will change the color scheme for final figure. Likely using<br>
orange and blue to denote closed and open-loop interventions. Will<br>
also add in indication of severed edges <a href="#fnref17" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn18" class="footnote-item"><p>will change the color scheme for final figure. Likely using<br>
orange and blue to denote closed and open-loop interventions. Will<br>
also add in indication of severed edges <a href="#fnref18" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn19" class="footnote-item"><p>Figure VAR shows this pretty well, perhaps sink this section<br>
until after discussing categorical and quantitative? <a href="#fnref19" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn20" class="footnote-item"><p>below the level set by added, independent/&quot;private&quot; sources <a href="#fnref20" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn21" class="footnote-item"><p>notably, this is part of the definition of open-loop intervention <a href="#fnref21" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn22" class="footnote-item"><p>not 100% sure this is true, the empirical results are really<br>
pointing to dR/dW&lt;0 rather than dR/dS&lt;0. Also this should really<br>
be something like <span class="mathjax-exps">$\frac{d|R|}{dS}$</span> or <span class="mathjax-exps">$\frac{dr^2}{dS}$</span> since these<br>
effects decrease the <em>magnitude</em> of correlations. I.e. if<br>
<span class="mathjax-exps">$\frac{d|R|}{dS} &lt; 0$</span> increasing <span class="mathjax-exps">$S$</span> might move <span class="mathjax-exps">$r$</span> from <span class="mathjax-exps">$-0.8$</span> to<br>
<span class="mathjax-exps">$-0.2$</span>, i.e.&#xA0;decrease its magnitude not its value. <a href="#fnref22" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn23" class="footnote-item"><p>compare especially to <a href="https://www.frontiersin.org/articles/10.3389/fncom.2020.00045/full">&quot;Transfer Entropy as a Measure of Brain<br>
Connectivity&quot;</a>,<br>
<a href="https://www.jneurosci.org/content/29/33/10234">&quot;How Connectivity, Background Activity, and Synaptic Properties<br>
Shape the Cross-Correlation between Spike<br>
Trains&quot;</a> Figure 3. <a href="#fnref23" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn24" class="footnote-item"><p>this corroborates Ila Fiete&apos;s paper on bias as a function of<br>
recurrent network strength <a href="#fnref24" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
</ol>
</section>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>