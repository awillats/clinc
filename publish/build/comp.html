<!DOCTYPE html><html><head>
      <title>Closed-Loop Identifiability in Neural Circuits</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"],"imageFont":null},"root":"file:///Users/adam/.dotfiles/atom/packages/markdown-preview-enhanced/node_modules/@shd101wyy/mume/dependencies/mathjax"});
        </script>
        <script type="text/javascript" async src="file:////Users/adam/.dotfiles/atom/packages/markdown-preview-enhanced/node_modules/@shd101wyy/mume/dependencies/mathjax/MathJax.js" charset="UTF-8"></script>
        
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
.markdown-preview.markdown-preview {
  /* Heading numbering, inspired by Typora 
     https://support.typora.io/Auto-Numbering/
    */
  counter-reset: h1;
}
.markdown-preview.markdown-preview h1 {
  counter-reset: h2;
}
.markdown-preview.markdown-preview h2 {
  counter-reset: h3;
}
.markdown-preview.markdown-preview h3 {
  counter-reset: h4;
}
.markdown-preview.markdown-preview h4 {
  counter-reset: h5;
}
.markdown-preview.markdown-preview h5 {
  counter-reset: h6;
}
.markdown-preview.markdown-preview h1:before {
  counter-increment: h1;
  content: counter(h1) ". ";
}
.markdown-preview.markdown-preview h2:before {
  counter-increment: h2;
  content: counter(h1) "." counter(h2) ". ";
}
.markdown-preview.markdown-preview h3:before {
  counter-increment: h3;
  content: counter(h1) "." counter(h2) "." counter(h3) ". ";
}
.markdown-preview.markdown-preview h4:before {
  counter-increment: h4;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) ". ";
}
.markdown-preview.markdown-preview h5:before {
  counter-increment: h5;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) ". ";
}
.markdown-preview.markdown-preview h6:before {
  counter-increment: h6;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) "." counter(h6) ". ";
}
.do-number-sections {
  counter-reset: h1;
}
.do-number-sections h1 {
  counter-reset: h2;
}
.do-number-sections h2 {
  counter-reset: h3;
}
.do-number-sections h3 {
  counter-reset: h4;
}
.do-number-sections h4 {
  counter-reset: h5;
}
.do-number-sections h5 {
  counter-reset: h6;
}
.do-number-sections h1:before {
  counter-increment: h1;
  content: counter(h1) ". ";
}
.do-number-sections h2:before {
  counter-increment: h2;
  content: counter(h1) "." counter(h2) ". ";
}
.do-number-sections h3:before {
  counter-increment: h3;
  content: counter(h1) "." counter(h2) "." counter(h3) ". ";
}
.do-number-sections h4:before {
  counter-increment: h4;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) ". ";
}
.do-number-sections h5:before {
  counter-increment: h5;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) ". ";
}
.do-number-sections h6:before {
  counter-increment: h6;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) "." counter(h6) ". ";
}

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <pre>[WARNING] Citeproc: Citation with no printed form: 
</pre><p>SECTION: Abstract</p>
<p> <span class="math display">\[x=2^2\]</span> The necessity of
intervention in inferring cause has long been understood in
neuroscience. Recent work has highlighted the limitations of passive
observation and single-site lesion studies in accurately recovering
causal circuit structure. The advent of optogenetics has facilitated
increasingly precise forms of intervention including closed-loop control
which may help eliminate confounding influences. However, it is not yet
clear how best to apply closed-loop control to leverage this increased
inferential power. In this paper, we use tools from causal inference,
control theory, and neuroscience to show when and how closed-loop
interventions can more effectively reveal causal relationships.
<code>We also examine the performance of standard network inference procedures in simulated spiking networks under passive, open-loop and closed-loop conditions.</code>
We demonstrate a unique capacity of feedback control to distinguish
competing circuit hypotheses by disrupting connections which would
otherwise result in equivalent patterns of correlation<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.
Our results build toward a practical framework to improve design of
neuroscience experiments to answer causal questions about neural
circuits.</p>
<p>SECTION: Introduction</p>
<p>SUB SECTION: Estimating causal interactions in the brain</p>
<p>Many hypotheses about neural circuits are phrased in terms of causal
relationships: &#x201C;will changes in activity to this region of the brain
produce corresponding changes in another region?&#x201D; Understanding these
causal relationships is critical to both scientific understanding and to
developing effective therapeutic interventions, which require knowledge
of how potential therapies will impact brain activity and patient
outcomes.</p>
<p>A range of mathematical and practical challenges make it difficult to
determine these causal relationships. In studies that rely only
observational data, it is often impossible to determine whether observed
patterns of activity are caused by known and controlled inputs, or
whether they are instead spurious connections generated by recurrent
activity, indirect relationships, or unobserved &#x201C;confounders.&#x201D; It is
generally understood that moving from experiments involving passive
observation to more complex levels of intervention allows experimenters
to better tackle challenges to circuit identification. However, while
chemical and surgical lesion experiments have historically been employed
to remove the influence of possible confounds, they are likely to
dramatically disrupt circuits from their typical functions, making
conclusions about underlying causal structure drawn from these
experiments unlikely to hold in naturalistic settings <span class="citation" data-cites></span>. <em>Closed-loop</em>
interventions <span class="citation" data-cites></span>: short
description of closed-loop in neuro, maybe drawing from text in this
</p>
<p>Despite the promise of these closed-loop strategies for identifying
causal relations in neural circuits, however, it is not yet fully
understood <em>when</em> more complex intervention strategies can
provide additional inferential power, or <em>how</em> these experiments
should be optimally designed. In this paper we demonstrate when and how
closed-loop interventions can reveal the causal structure governing
neural circuits. Drawing from ideas in causal inference <span class="citation" data-cites></span> <span class="citation" data-cites></span> we describe the classes of models that can be
distinguished by a given set of input-output experiments, and what
experiments are necessary to uniquely determine specific causal
relationships.</p>
<p>We first propose a mathematical framework that describes how open-
and closed-loop interventions impact observable qualities of neural
circuits. Using this framework, experimentalists propose a set of
candidate hypotheses describing the potential causal structure of the
circuit under study, and then select a series of interventions that best
allows them to distinguish between these hypotheses. Using both simple
controlled models and in silico models of spiking networks, we explore
factors that govern the efficacy of these types of interventions. Guided
by the results of this exploration, we present a set of recommendations
that can guide the design of open- and closed-loop experiments to better
uncover the causal structure underlying neural circuits.</p>
<p><strong>Inferring causal interactions from time series.</strong> A
number of strategies have been proposed to detect causal relationships
between observed variables. Wiener-Granger (or predictive) causality
states that a variable <span class="math inline">\(X\)</span>
&#x201C;Granger-causes&#x201D; <span class="math inline">\(Y\)</span> if <span class="math inline">\(X\)</span> contains information relevant to <span class="math inline">\(Y\)</span> that is not contained in <span class="math inline">\(Y\)</span> itself or any other variable This
concept has traditionally been operationalized with vector
autoregressive models the requirement that <em>all</em> potentially
causative variables be considered makes these notions of dependence
susceptible to unobserved confounders </p>
<p>Our work initially focuses on measures of directional interaction
that are based on lagged correlations These metrics look at the
correlation of time series collected from pairs of nodes at various lags
and detect peaks at negative time lags. Such peaks could indicate the
presence of a direct causal relationship &#x2013; but they could also stem from
indirect causal links or hidden confounders In these bivariate
correlation methods, it is thus necessary to consider patterns of
correlation between many pairs of nodes in order to differentiate
between direct, indirect, and confounding relationships This
distinguishes these strategies from some multivariate methods that
&#x201C;control&#x201D; for the effects of potential confounders. While
cross-correlation-based measures are generally limited to detecting
linear functional relationships between nodes, their computational
feasibility makes them a frequent metric of choice in experimental
neuroscience work </p>
<p>Other techniques detect directional interaction stemming from more
general or complex relationships. Information-theoretic methods, which
use information-based measures to assess the reduction in entropy
knowledge of one variable provides about another, are closely related to
Granger causality The <em>transfer entropy</em> <span class="math inline">\(T_{X \to Y}(t) = I(Y_t \colon X_{&lt;t} \mid
Y_{&lt;t})\)</span> extends this notion to time series by measuring the
amount of information present in <span class="math inline">\(Y_t\)</span> that is not contained in the past of
either <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> (denoted <span class="math inline">\(X_{&lt;t}\)</span> and <span class="math inline">\(Y_{&lt;t}\)</span>) Using transfer entropy as a
measure of causal interaction requires accounting for potential
confounding variables; the <em>conditional transfer entropy</em> <span class="math inline">\(T_{X \to Y \mid Z}(t) = I(Y_t \colon X_{&lt;t}
\mid Y_{&lt;t}, Z_{&lt;t})\)</span> conditions on the past of other
variables to account for their potential confounding influence
Conditional transfer entropy can thus be interpreted as the amount of
information present in <span class="math inline">\(Y\)</span> that is
not contained in the past of <span class="math inline">\(X\)</span>, the
past of <span class="math inline">\(Y\)</span>, or the past of other
variables <span class="math inline">\(Z\)</span>.</p>
<p>To quantify the strength of causal interactions,
information-theoretic and transfer-entropy-based methods typically
require knowledge of the ground truth causal relationships that exist or
an ability to perturb the system In practice, these quantities are
typically interpreted as &#x201C;information transfer,&#x201D; and a variety of
estimation strategies and methods to automatically select the
conditioning set (i.e., the variables and time lags that should be
conditioned on) are used (e.g., Multivariate conditional transfer
entropy approaches using various variable selection schemes can
differentiate between direct interactions, indirect interactions, and
common causes, but their results depend on choices such as the binning
strategies used to discretize continuous signals, the specific
statistical tests used, and the estimator used to compute transfer
entropy However, despite their mathematical differences, previous work
has found that cross-correlation-based metrics and information-based
metrics tend to produce qualitatively similar results, with similar
patterns of true and false positives </p>
<p>SUB SECTION: Interventions in neuroscience &amp; causal inference</p>
<p>Data collected from experimental settings can provide more
inferential power than observational data alone. For example, consider
an experimentalist who is considering multiple causal hypotheses for two
nodes under study, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>: the hypothesis that <span class="math inline">\(x\)</span> is driving <span class="math inline">\(y\)</span>, the hypothesis that <span class="math inline">\(y\)</span> is driving <span class="math inline">\(x\)</span>, or the hypothesis that the two
variables are being independently driven by a hidden confounder.
Observational data revealing that <span class="math inline">\(x\)</span>
and <span class="math inline">\(y\)</span> produce correlated
time-series data is equally consistent with each of these three causal
hypotheses, providing the experimentalist with no inferential power.
Experimentally manipulating <span class="math inline">\(x\)</span> and
observing the output of <span class="math inline">\(y\)</span>, however,
allows the scientist to begin to establish which causal interaction
pattern is at work. Consistent with intuition from neuroscience
literature, a rich theoretical literature has described the central role
of interventions in inferring causal structure from data </p>
<p><img src="figures/core_figure_sketches/figure1_sketch.png"> &gt;
<strong>Figure INTRO:</strong> Examples of the roles interventions have
played in neuroscience. (A) <em>Passive observation</em> does not
involve stimulating the brain. In this example, passive observational
data is used to identify patients suffering from absence seizures. (B)
<em>Open-loop stimulation</em> involves recording activity in the brain
after perturbing a region with a known input signal. Using systematic
<em>open-loop stimulation experiments</em>, Penfield uncovered the
spatial organization of how senses and movement are mapped in the cortex
(C) <em>Closed-loop control</em> uses feedback control to precisely
specify activity in certain brain regions regardless of activity in
other regions. Using closed-loop control, </p>
<p>The inferential power of interventions is depends on <em>where</em>
stimulation is applied: interventions on some portions of a system may
provide more information about the system&#x2019;s causal structure than
interventions in other areas. And interventions are also more valuable
when they more effectively set the state of the system: &#x201C;perfect&#x201D;
closed-loop control, which completely severs a node&#x2019;s activity from its
inputs, are often more informative than &#x201C;soft&#x201D; interventions that only
partially control a part of the system </p>
<p>In experimental neuroscience settings, experimenters are faced with
deciding between interventions that differ in both location and
effectiveness. For example, stimulation can often only be applied to
certain regions of the brain. And while experimenters may be able to
exactly manipulate activity in some parts of the brain using closed-loop
control, in other locations it may only be possible to apply weaker
forms of intervention that perturb a region but do not manipulate its
activity exactly to a desired state. In Section we compare the
effectiveness of open-loop, closed-loop, and partially-effective
closed-loop control.</p>
<p>Although algorithms designed to choose optimal interventions are
often designed for simple models with strong assumptions,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
they provide intuition that can aid practitioners seeking to design
real-world experiments that provide as much scientific insight as
possible.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Importantly, the informativeness of
interventions is often independent of the algorithm used to infer causal
connections, meaning that certain interventions can reveal portions of a
circuit&#x2019;s causal structure that would be impossible for <em>any</em>
algorithm to infer from only observational data Matt to Adam: make sure
this citation is in the right We similarly expect the results we
demonstrate in this paper to both inform experimentalists and open
avenues for further research.</p>
<p>SUB SECTION: Representations &amp; reachability</p>
<p><img src="figures/core_figure_sketches/circuit_walkthrough_3circuits_key_sketch.png" title="generated by /code/fig_circuit_walkthrough.py"> &gt;
<strong>Figure DEMO <em>(box format)</em>: Applying CLINC to distinguish
a pair of circuits</strong> &gt; &gt; Consider the three-node
identification problem shown in the figure above, in which the
experimenter has identified three hypotheses for the causal structure of
the circuit. These circuit hypotheses, shown as directed graphs in
column 1, can each also be represented by an adjacency matrix of the
form for example, circuit A is represented by an adjacency matrix in
which <span class="math inline">\(w_{01}\)</span>, <span class="math inline">\(w_{20}\)</span>, and <span class="math inline">\(w_{21} \neq 0\)</span>. Note that hypotheses A and
C have direct connections between nodes 0 and 2; while hypothesis B does
not have a direct connection between these nodes, computing the weighted
reachability matrix <span class="math inline">\(\widetilde{W}\)</span>
in circuit B an <em>indirect</em> connection exists through the path 2
<span class="math inline">\(\to\)</span> 1 <span class="math inline">\(\to\)</span> 0 (illustrated in gray in column 2).
&gt; &gt; Because there are direct or indirect connections between each
pair of nodes, passive observation of each hypothesized circuit would
reveal that each pair of nodes is correlated (column 3). These three
hypotheses are therefore difficult to distinguish<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> for
an experimentalist who performs only passive observation, but can be
distinguished through stimulation. &gt; &gt; Column 4 shows the impact
on observed correlations of performing <em>open-loop</em> control on
node 1. In hypothesis A, node 1 is not a driver of other nodes, so
open-loop stimulation at this site will not increase the correlation
between the signal observed at node 1 and other nodes. The path from
node 1 to 0 in hypotheses B and C, meanwhile, causes the open-loop
stimulation at node 1 to <em>increase</em> the observed correlation
between nodes 1 and 0. An experimenter can thus distinguish between
hypothesis A and the other two hypotheses by appling open-loop control
and observing the resulting pattern of correlations (column 4). However,
this pattern of open-loop stimulation would not allow the experimenter
to distinguish between hypotheses B and C. &gt; &gt;
<em>Closed-loop</em> control (columns 5 and 6) can provide the
experimenter with even more inferential power. Column 5 shows the
resulting adjacency matrix when this closed-loop control is applied to
node 1. In each hypothesis, the impact of this closed-loop control is to
remove the impact of other nodes on node 1, because when perfect
closed-loop is applied the activity of node 1 is completely independent
of other nodes. (These severed connections are depicted in column 5 by
dashed lines.) In hypothesis B, this also results in the elimation of
the indirect connection from node 2 to node 1. The application of
closed-loop control at node 1 thus results in a different observed
correlation structure in each of the three circuit hypotheses (column
6). This means that the experimenter can therefore distinguish between
these circuit hypotheses by applying closed-loop control &#x2013; a task not
possible with passive observation or open-loop control.</p>
<p>SECTION: Theory / Prediction</p>
<p><img src="figures/core_figure_sketches/methods_overview_pipeline_sketch.png">
&gt; <strong>Figure OVERVIEW:</strong> &#x2026;</p>
<p>SUB SECTION: Predicting correlation structure (theory)</p>
<p>A linear-Gaussian circuit can be described by 1) the variance of the
gaussian private (independent) noise at each node, and 2) the weight of
the linear relationships between each pair of connected nodes. Let <span class="math inline">\(s \in \mathbb{R}^p\)</span> denote the variance of
each of the <span class="math inline">\(p\)</span> nodes in the circuit,
and <span class="math inline">\(W \in \mathbb{R}^{p \times p}\)</span>
denote the matrix of connection strengths such that <span class="math display">\[W_{ij} = \text{strength of $i \to j$
connection}.\]</span></p>
<p>Under passive observation, the squared correlation coefficient can
thus be written as <span class="math display">\[
\begin{aligned}
    r^2(i,j) &amp;= \frac{\Sigma_{ij}}{\Sigma_{ii} \Sigma_{jj}} \\
    &amp;= \frac{\left( \sum_{k=1}^p \widetilde{W}_{ik}
\widetilde{W}_{jk} s_k \right)^2}{\left(\sum_{k=1}^p
\widetilde{W}_{ik}^2 s_k\right)\left(\sum_{k=1}^p \widetilde{W}_{jk}^2
s_k\right)}.
\end{aligned}
\]</span></p>
<p>This framework also allows us to predict the impact of open- and
closed-loop control on the pairwise correlations we expect to observe.
To model the application of open-loop control on node <span class="math inline">\(c\)</span>, we add an arbitrary amount of private
variance to <span class="math inline">\(s_c\)</span>: <span class="math inline">\(s_c \leftarrow s_c + s_c^{(OL)}\)</span>. To model
the application of closed-loop control on node <span class="math inline">\(c\)</span>, we first sever inputs to node <span class="math inline">\(c\)</span> by setting <span class="math inline">\(W_{k,c} = 0\)</span> for <span class="math inline">\(k = 1, \dots p\)</span>, and then set the private
variance of node <span class="math inline">\(c\)</span> by setting <span class="math inline">\(s_c\)</span> to any arbitrary value. Because <span class="math inline">\(c\)</span>&#x2019;s inputs have been severed, this
private noise will become exactly node <span class="math inline">\(c\)</span>&#x2019;s output variance.</p>
<p>SECTION: Simulation Methods</p>
<p>SUB SECTION: Modeling network structure and dynamics</p>
<p>SUB SECTION: Stochastic network dynamics</p>
<p>The first approach is accomplished with a network of nodes with
gaussian noise sources, linear interactions, and linear dynamics. The
second approach is achieved with a network of nodes consisting of
populations of leaky integrate-and-fire (LIF) neurons. These differ from
the simpler case in their nonlinear-outputs, arising from inclusion of a
spiking threshold. Interactions between neurons happen through spiking
synapses, meaning information is passed between neurons sparsely in
time<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p><em>Neuron dynamics:</em> <span class="math display">\[
\frac{dV}{dt} = \frac{V_0 + I - V}{\tau_m} + \sigma_m \sqrt{\tau_m}
\xi(t)
\]</span></p>
<p>SUB SECTION: Time-resolvable interactions</p>
<p>Additionally we study two domains of interactions between
populations; contemporaneous and delay-resolvable connections. These
domains represent the relative timescales of measurement versus
timescale of synaptic delay.</p>
<p> doesnt work with </p>
<blockquote>
<p>correlation across positive and negative lags between two outputs</p>
</blockquote>
<p>In the delay-resolvable domain, directionality of connections may be
inferred even under passive observations by looking at temporal
precedence - whether the past of one signal is more strongly correlated
with future lags of another signal <em>(i.e.&#xA0;cross-correlation)</em>. In
the contemporaneous domain, network influences act within the time of a
single sample<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> so this temporal precedence clue is
lost (although directionality can still be inferred in the presence of
intervention).</p>
<p>The following work is presented with the linear-Gaussian and
contemporaneous domains as the default for simplicity and
conciseness.</p>
<p>SUB SECTION: Code implementation</p>
<p>Software for data generation, analysis, and plotting is available at
https://github.com/awillats/clinc. Both linear-gaussian and spiking
networks are simulated with code built from the <a href="https://elifesciences.org/articles/47314">Brian2</a> spiking
neural network simulator. This allows for highly modular code with
easily interchanged neuron models and standardized output preprocessing
and plotting. It was necessary to write an additional custom extension
to Brian2 in order to capture delayed linear-gaussian interactions,
available at <a href="https://github.com/awillats/brian_delayed_gaussian">brian_delayed_gaussian</a>.
With this added functionality, it is possible to compare the equivalent
network parameters only changing linear-gaussian versus spiking dynamics
and inspect differences solely due to spiking.</p>
<p><em>see <a href="_network_parameters_table.md">_network_parameters_table.md</a> for
list of relevant parameters</em></p>
<p>SUB SECTION: Implementing interventions</p>
<p><img src="figures/core_figure_sketches/figure1_sketch.png"></p>
<p>To study the effect of various interventions we simulated inputs to
nodes in a network. In the <strong>passive setting</strong>, nodes
receive additive drive from <em>private</em> Gaussian noise sources
common to all neurons within a node, but independent across nodes. The
variance of this noise is specified by <span class="math inline">\(\sigma_m \sqrt{\tau_m}\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p><span class="math display">\[
\frac{dV}{dt} = \frac{V_0 + I - V}{\tau_m} + \sigma_m \sqrt{\tau_m}
\xi(t)
\]</span></p>
<p>To emulate <strong>open-loop intervention</strong> we simulated
current injection from an external source. This is intended to represent
experiments involving stimulation from microelectrodes or optogenetics
<em>(albeit simplifying away any impact of actuator dynamics)</em>. By
default, open-loop intervention is specified as white noise sampled at
each timestep from Gaussian distribution with mean and variance <span class="math inline">\(\mu_{intv.}\)</span> and <span class="math inline">\(\sigma^2_{intv.}\)</span><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p><span class="math display">\[
I_{open-loop} \sim \mathcal{N}(\mu_{intv.},\,\sigma^{2}_{intv.})\\
\]</span> Ignoring the effect of signal means in the linear-Gaussian
setting: <span class="math display">\[
X_k = f(\sigma^2_m, \sigma^{2}_{intv.})
\]</span> <code>per-node indexing needs resolving here also</code></p>
<p>Ideal <strong>closed-loop control</strong> is able to overwrite the
output of a node, setting it precisely to the specified target.
<code>making up notation as I go here, needs tightening up:</code> <span class="math display">\[
\begin{aligned}
T &amp;\sim \mathcal{N}(\mu_{intv.},\,\sigma^{2}_{intv.}) \\
I_{closed-loop} &amp;= f(X, T)  \\
X_k | CL_{k} &amp;\approx T
\end{aligned}
\]</span> Note that in this setting, the <em>output</em> of a node <span class="math inline">\(X_k\)</span> under closed-loop control is
identical to the target, therefore <span class="math display">\[
X_k | CL_{k} = f(\sigma^{2}_{intv.}) \perp \sigma^2_m
\]</span> In practice, near-ideal control is only possible with very
fast measurement and computation relative to the network&#x2019;s intrinsic
dynamics, such as in the case of dynamic clamp<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. To
demonstrate a broader class of closed-loop interventions (such as those
achievable with extracellular recording and stimulation), imperfect
&#x201C;partial&#x201D; control is simulated by linearly interpolating the output of
each node between the target <span class="math inline">\(T\)</span> and
the uncontrolled output based on a control effectiveness parameter <span class="math inline">\(\gamma\)</span></p>
<p><span class="math display">\[
X | CL_{k, \gamma} = \gamma T + (1-\gamma) X
\]</span></p>
<p>In the full discrete-time simulation, closed-loop interventions are
instead simulated through a proportional-integral-derivative (PID)
control policy with control efficacy determined functionally by the
strength of controller gains <span class="math inline">\(K = \{k_P, k_I,
k_D\}\)</span> relative to the dynamics of the network.</p>
<p><span class="math display">\[I_{PID} = \text{PID}(X,T|
K)\]</span></p>
<p>Another interesting intervention to study is <strong>open-loop replay
of a closed-loop stimulus</strong>, <em>that is</em> taking a particular
injected current <span class="math inline">\(I_{CL,\,prev}\)</span> used
to drive nodes to a target <span class="math inline">\(T_{prev}\)</span>
and adding it back to the network in a separate trial.</p>
<p>Because the instantiation of noise in the network will be different
from trial to trial, this &#x201C;replay&#x201D; stimulus will no longer adapt
sample-by-sample (therefore it should be considered open-loop) and the
node&#x2019;s output cannot be expected to match the target precisely, however
the statistics of externally applied inputs will be the same. In effect,
the comparison between closed-loop and open-loop replay conditions
reveals the specific effect of feedback intervention while controlling
for any confounds from input statistics.</p>
<p>SUB SECTION: Extracting circuit estimates</p>
<blockquote>
<p><em>refer to methods overview figure</em></p>
</blockquote>
<p>While a broad range of techniques<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> exist for inferring
functional relationships from observational data,
<code>(for the majority of this work)</code> we choose to focus on
simple bivariate correlation as a measure of dependence in the
linear-Gaussian network. The impact of intervention on this metric is
analytically tractable <em>(see <a href="methods1_predicting_correlation.md">methods1_predicting_correlation.md</a>)</em>,
and can be thought of as a prototype<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> for more sophisticated
measures of dependence such as time-lagged cross-correlations, bivariate
and multivariate transfer entropy.</p>
<p>We implement a naive comparison strategy to estimate the circuit
adjacency from emprical correlations; Thresholded empirical correlation
matrices are compared to correlation matrices predicted from each
circuit in a hypothesis set. Any hypothesized cirucits which are
predicted to have a similar correlation structure as is observed
(i.e.&#xA0;corr. mats equal after thresholding) are marked as &#x201C;plausible
circuits.&#x201D;<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> If only one circuit amongst the
hypothesis set is a plausible match, this is considered to be the
estimated circuit. The threshold for &#x201C;binarizing&#x201D; the empirical
correlation matrix is treated as a hyperparameter to be swept at the
time of analysis.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<p>SUB SECTION: Information-theoretic measures of hypothesis
ambiguity</p>
<p><em>see <a href="_steps_of_inference.md">_steps_of_inference.md</a>
for entropy writeup</em></p>
<p>SECTION: Results</p>
<p>SUB SECTION: Impact of intervention on estimation performance</p>
<p>SUB SECTION: Intervening provides (categorical) improvements in
inference power beyond passive observation</p>
<p><a href="_steps_of_inference.md">Methods: Procedure for choosing
&amp; applying intervention</a></p>

<p>Next, we apply (steps 1-3 of) this circuit search procedure to a
collection of closely related hypotheses for 3 interacting nodes<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> to illustrate the impact of
intervention. &#x1F6A7;
<code>most of the story in the figure caption for now</code> &#x1F6A7;</p>
<p> <img src="figures/core_figure_sketches/circuit_entropy_sketch.png"></p>
<blockquote>
<p><strong>Figure DISAMBIG: Interventions narrow the set of hypotheses
consistent with observed correlations</strong> <em>source: <a href="https://docs.google.com/drawings/d/1CBp1MhOW7OGNuBvo7OkIuzqnq8kmN8EEX_AkFuKpVtM/edit">google
drawing</a></em> <strong>(A)</strong> Directed adjacency matrices
represent the true and hypothesized causal circuit structure
<strong>(B)</strong> Directed reachability matrices represent the direct
<em>(black)</em> and indirect <em>(grey)</em> influences in a network.
Notably, different adjacency matrices can have equivalent reachability
matrices making distinguishing between similar causal structures
difficult, even with open-loop control. <strong>(C)</strong>
Correlations between pairs of nodes. Under passive observation, the
direction of influence is difficult to ascertain. In densely connected
networks, many distinct ground-truth causal structures result in similar
&#x201C;all correlated with all&#x201D; patterns providing little information about
the true structure. <strong>(D-F)</strong> The impact of open-loop
intervention at each of the nodes in the network is illustrated by
modifications to the passive correlation pattern. Thick orange<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> edges denote correlations which
increase above their baseline value with high variance open-loop input.
Thin blue<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> edges denote correlations which
decrease, often as a result of increased connection-independent &#x201C;noise&#x201D;
variance in one of the participating nodes. Grey edges are unaffected by
intervention at that location. A given hypotheses set (A) will result in
an &#x201C;intervention-specific fingerprint&#x201D;, that is a distribution of
frequencies for observing patterns of modified correlations <em>(across
a single row within D-F)</em>. If this fingerprint contains many
examples of the same pattern of correlation (such as
<strong>B</strong>), many hypotheses correspond to the same observation,
and that experiment contributes low information to distinguish between
structures. A maximally informative intervention would produce a unique
pattern of correlation for each member of the hypothesis set.
:construction:<code>caption too long</code></p>
</blockquote>
<p><strong>Why does closed-loop control provide a categorical
advantage?</strong> Because it severs indirect links
<code>is this redundant with intro?</code>
<code>needs to be backed here up by aggregate results?</code> - this is
especially relevant in recurrently connected networks where the
reachability matrix becomes more dense. - more stuff is connected to
other stuff, so there are more indirect connections, and the resulting
correlations look more similar (more circuits in the equivalence class)
- patterns of correlation become more specific with increasing
intervention strength - more severed links &#x2192; more unique
adjacency-specific patterns of correlation</p>
<blockquote>
<p><strong>Where you intervene</strong><a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>
strongly determines the inference power of your experiment.
<strong>secondary point:</strong> having (binary) prediction helps
capture this relationship</p>
</blockquote>
<p>While a primary advantage of closed-loop interventions for circuit
inference is its ability to functionally lesion indirect connections,
another, more nuanced <code>(quantitative)</code> advantage of
closed-loop control lies in its capacity to bidirectionally control
output variance. While the variance of an open-loop stimulus can be
titrated to adjust the output variance at a node, in general, an
open-loop stimulus cannot reduce this variance below its instrinsic<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> variability. That is, if the system
is linear with gaussian noise,</p>
<p><span class="math display">\[\mathbb{V}_{i}(C|S=\text{open},\sigma^2_S) \geq
\mathbb{V}_{i}(C)\]</span> More specifically, if the open-loop stimulus
is statistically independent from the intrinsic variability<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> <span class="math display">\[\mathbb{V}_{i}(C|S=\text{open},\sigma^2_S) =
\mathbb{V}_{i}(C) + \sigma^2_S\]</span> Applying closed-loop to a linear
gaussian circuit:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) &amp;= \sigma^2_S  \\
\mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) &amp;\perp
\mathbb{V}_{i}(C)
\end{aligned}
\]</span></p>
<p>SUB SECTION: Impact of intervention location and variance on pariwise
correlations</p>
<p><a href="methods1_predicting_correlation.md">related methods</a></p>
<p>We have shown that closed-loop interventions provide more flexible
control over output variance of nodes in a network, and that shared and
independent sources of variance determine pairwise correlations between
node outputs. Together, this suggests closed-loop interventions may
allow us to shape the pattern of correlations with more degrees of </p>
<p>One application of this increased flexibility is to increase
correlations associated with pairs of directly correlated nodes, while
decreasing spurious correlations associated with pairs of nodes without
a direct connection (but perhaps are influenced by a common input, or
are connected only indirectly). While &#x201C;correlation does not imply
causation,&#x201D; intervention may decrease the gap between the two.</p>
<p>Our hypothesis is that this shaping of pairwise correlations will
result in reduced false positive edges in inferred circuits,
&#x201C;unblurring&#x201D; the indirect associations that would otherwise confound
circuit inference. However care must be taken, as this strategy relies
on a hypothesis for the ground truth adjacency and may also result in a
&#x201C;confirmation bias&#x201D; as new spurious correlations can be introduced
through closed-loop intervention.</p>
<p>The impact of intervention on correlations can be summarized through
the co-reachability <span class="math inline">\(\text{CoReach}(i,j|S_k)\)</span>. A useful
distillation of this mapping is to understand the sign of <span class="math inline">\(\frac{dR_{ij}}{dS_k}\)</span>, that is whether
increasing the variance of an intervention at node <span class="math inline">\(k\)</span> increases or decreases the correlation
between nodes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span></p>
<p>In a simulated network A&#x2192;B <a href="#fig-var">(fig.&#xA0;variance)</a> we
demonstrate predicted and emprirical correlations between a pair of
nodes as a function of intervention type, location, and variance. A few
features are present which provide a general intuition for the impact of
intervention location in larger circuits: First, interventions
&#x201C;upstream&#x201D; of a true connection <a href="#fig-var">(lower left,
fig.&#xA0;variance)</a> tend to increase the connection-related variance, and
therefore strengthen the observed correlations. <span class="math display">\[\text{Reach}(S_k&#x2192;i) \neq 0 \\ \text{Reach}(i&#x2192;j)
\neq 0 \\ \frac{dR}{dS_k} &gt; 0\]</span></p>
<p>Second, interventions affecting only the downstream node <a href="#fig-var">(lower right, fig.&#xA0;variance)</a> of a true connection
introduce variance which is independent of the connection A&#x2192;B,
decreasing the observed correlation. <span class="math display">\[\text{Reach}(S_k &#x2192; j) = 0 \\ \text{Reach}(S_k &#x2192;
j) \neq 0 \\ \frac{dR}{dS_k} &lt; 0\]</span></p>
<p>Third, interventions which reach both nodes will tend to increase the
observed correlations <a href="#fig-var">(upper left,
fig.&#xA0;variance)</a>, moreover this can be achieved even if no direct
connection <span class="math inline">\(i&#x2192;j\)</span> exists. <span class="math display">\[\text{Reach}(S_k &#x2192; i) \neq 0 \\ \text{Reach}(S_k
&#x2192; j) \neq 0 \\ \text{Reach}(i &#x2192; j) = 0 \\ \frac{dR}{dS_k} &gt;
0\]</span></p>
<p>Notably, the impact of an intervention which is a &#x201C;common cause&#x201D; for
both nodes depends on the relative weighted reachability between the
source and each of the nodes. Correlations induced by a common cause are
maximized when the input to each node is equal, that is <span class="math inline">\(\widetilde{W}_{S_k&#x2192;i} \approx
\widetilde{W}_{S_k&#x2192;j}\)</span> (upper right * in <a href="#fig-var">fig.&#xA0;variance</a>). If i&#x2192;j are connected <span class="math inline">\(\widetilde{W}_{S_k&#x2192;i} \gg
\widetilde{W}_{S_k&#x2192;j}\)</span> results in an variance-correlation
relationship similar to the &#x201C;upstream source&#x201D; case (increasing source
variance increases correlation <span class="math inline">\(\frac{dR}{dS_k} &gt; 0\)</span>), while <span class="math inline">\(\widetilde{W}_{S_k&#x2192;i} \ll
\widetilde{W}_{S_k&#x2192;j}\)</span> results in a relationship similar to the
&#x201C;downstream source&#x201D; case (<span class="math inline">\(\frac{dR}{dS_k}
&lt; 0\)</span>)<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<p><img src="figures/misc_figure_sketches/quant_r2_prediction_common.png">
<img src="figures/from_code/bidirectional_correlation.png" title="generated by sweep_gaussian_SNR.py"></p>
<blockquote>
<p>&#x1F6A7;(Final figure will be a mix of these two panels, caption will need
updating) <strong>Figure VAR: Location, variance, and type of
intervention shape pairwise correlations</strong>
<strong>(CENTER)</strong> A two-node linear gaussian network is
simulated with a connection from A&#x2192;B. Open-loop interventions
<em>(blue)</em> consist of independent gaussian inputs with a range of
variances <span class="math inline">\(\sigma^2_S\)</span>. Closed-loop
interventions <em>(orange)</em> consist of feedback control with an
independent gaussian target with a range of variances. <em>Incomplete
closed-loop interventions result in node outputs which are a mix of the
control target and network-driven activity</em>. Connections from
sources to nodes are colored by their impact on correlations between A
and B; green denotes <span class="math inline">\(dR/dS &gt; 0\)</span>,
red denotes <span class="math inline">\(dR/dS&lt;0\)</span>.
<strong>(lower left)</strong> Intervention &#x201C;upstream&#x201D; of the connection
A&#x2192;B increases the correlation <span class="math inline">\(r^2(A,B)\)</span>. <strong>(lower right)</strong>
Intervention at the terminal of the connection A&#x2192;B decreases the
correlation <span class="math inline">\(r^2(A,B)\)</span> by adding
connection-independent noise. <strong>(upper left)</strong> Intervention
with shared inputs to both nodes generally increases <span class="math inline">\(r^2(A,B)\)</span>, <em>(even without A&#x2192;B, see
supplement)</em>. <strong>(upper right)</strong> The impact of shared
interventions depends on relative weighted reachability <span class="math inline">\(\text{Reach}(S_k&#x2192;A) /
\text{Reach}(S_k&#x2192;B)\)</span>, with highest correlations when these terms
are matched (see <em>) Closed-loop interventions </em>(orange)*
generally result in larger changes in correlation across <span class="math inline">\(\sigma^2_S\)</span> than the equivalent open-loop
intervention. Closed-loop control at B effectively lesions the
connection A&#x2192;B, resulting in near-zero correlation. <a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
</blockquote>
<p>&#x1F6A7; The change in correlation as a function of changing intervention
variance (<span class="math inline">\(\frac{dr^2_{ij}}{dS}\)</span>) can
therefore be used as an additional indicator of presence/absence and
directionality of the connection between A,B <em>(see <a href="fig-disambig">fig.&#xA0;disambig. D.)</a>)</em> &#x1F6A7;</p>
<p><a href="#fig-var">Fig. variance</a> also demonstrates the relative
dynamic range of correlations achievable under passive, open- and
closed-loop intervention. In the passive case, correlations are
determined by instrinsic properties of the network <span class="math inline">\(\sigma^2_{base}\)</span>. These properties have
influence over the observed correlations in a way that can be difficult
to separate from differences due to the ground-truth circuit. With
open-loop intervention we can observe the impact of increasing variance
at a particular node, but the dynamic range of achievable correlations
is bounded by not being able to reduce variance below its baseline
level. With closed-loop control, the bidirectional control of the output
variance for a node means a much wider range of correlations can be
achieved <a href="#fig-var">(blue v.s. orange in fig.&#xA0;variance)</a>,
resulting in a more sensitive signal reflecting the ground-truth
connectivity.</p>
<p><em>see also <a href="results1B_data_efficiency_and_bias.md">results1B_data_efficiency_and_bias.md</a></em></p>
<p>SECTION: Discussion</p>
<p>SUB SECTION: limitations</p>
<p>The examples explored in this work simplify several key features that
may have relevant contributions to circuit identification in practical
experiments. </p>
<p><code>full observability</code></p>
<p>SUB SECTION: results summary &#x2192; summary of value closed-loop
generally</p>
<p>Closed-loop control has the disadvantages of being more complex to
implement and requires specialized real-time hardware and software,
however it has been shown to have multifaceted usefulness in clinical
and basic science applications. Here we focused on two advantages in
particular; First, the capacity for functional lesioning which
(reversibly) severs inputs to nodes and second, closed-loop control&#x2019;s
capacity to precisely shape variance across nodes. Both of these
advantages facilitate opportunities for closed-loop intervention to
reveal more circuit structure than passive observation or even open-loop
experiments.</p>
<p>SUB SECTION: summary of guidelines for experimentors</p>
<p>In studying the utility of various intervention for circuit inference
we arrived at a few general guidelines which may assist experimental
neuroscientists in designing the right intervention for the quesiton at
hand. First, more ambiguous hypotheses sets require &#x201C;stronger&#x201D;
interventions to distinguish. Open-loop intervention may be sufficient
to determine directionality of functional relationships, but as larger
numbers of similar hypotheses closed-loop intervention reduces the
hypothesis set more efficiently. Second, we find that dense networks
with strong reciprocal connections tend to result in many equivalent
circuit hypotheses, but that well-placed closed-loop control can disrupt
loops and simplify correlation structure to be more identifiable.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> Recurrent loops are a common
feature of neural circuit, and represent key opportunities for
successful closed-loop intervention. The same is true for circuits with
strong indirect correlations</p>
<p><code>hidden confounds</code></p>
<p>SUB SECTION: &quot;funnel out&quot;, future work &#x2192; broad impact</p>
<p><code>sequential experimental design</code></p>
<p><em>see <a href="sketches_and_notation/discussion/limitations_future_work.md">limitations_future_work.md</a></em></p>
<p>SECTION: References</p>
<p><em>see <a href="https://github.com/shd101wyy/markdown-preview-enhanced/blob/master/docs/pandoc-bibliographies-and-citations.md">pandoc
pandoc-citations</a></em></p>
<p>SECTION: Supplement</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>may end up discussing quantitative
advantages such as bidirectional variance (and correlation) control. If
that&#x2019;s a strong focus in the results, should be talked about more in the
abstract also<a href="#fnref1" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn2" role="doc-endnote"><p>These assumptions are typically on
properties such as the types of functional relationships that exist in
circuits, the visibility and structure of confounding relationships, and
noise statistics.<a href="#fnref2" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn3" role="doc-endnote"><p>if citations needed here, could start
by looking for a good high-level reference in either or (Both of these
papers are pretty technical, so likely wouln&#x2019;t be great citations on
their own.)<a href="#fnref3" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn4" role="doc-endnote"><p>saying &#x201C;difficult to distinguish&#x201D;
instead of &#x201C;indistinguishable&#x201D; here since the magnitudes of the
correlations could also be informative with different assumptions<a href="#fnref4" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn5" role="doc-endnote"><p>However, depending on overall firing
rates and population sizes, this sparse spike-based transmission can be
coarse-grained to a firing-rate-based model.<a href="#fnref5" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn6" role="doc-endnote"><p>the effective <span class="math inline">\(\Delta_{sample}\)</span> would be broadened in the
presence of jitter in connection delay, measurement noise, or temporal
smoothing applied post-hoc, leading<a href="#fnref6" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn7" role="doc-endnote"><p>need to triple check indexing w.r.t.
nodes, neurons<a href="#fnref7" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn8" role="doc-endnote"><p>need to resolve differences in
implementation between contemporaneous and voltage simulation cases<a href="#fnref8" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn9" role="doc-endnote"><p>NEED dynamic clamp refs -
http://www.scholarpedia.org/article/Dynamic_clamp<a href="#fnref9" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn10" role="doc-endnote"><p><em>inference techniques mentioned
in the intro&#x2026;</em><a href="#fnref10" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn11" role="doc-endnote"><p>what does &#x201C;prototype&#x201D; mean here?
something like MI and corr are equivalent in the linear-Gaussian case,
&#x2026;<a href="#fnref11" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn12" role="doc-endnote"><p>TODO? formalize notation for this<a href="#fnref12" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn13" role="doc-endnote"><p>not sure how important this is.
would prefer to set this threshold at some ad-hoc value since we&#x2019;re
sweeping other properties. But a more in-depth analysis could look at a
receiver-operator curve with respect to this threshold<a href="#fnref13" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn14" role="doc-endnote"><p>nodes in such a graphical model may
represent populations of neurons, distinct cell-types, different regions
within the brain, or components of a latent variable represented in the
brain.<a href="#fnref14" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn15" role="doc-endnote"><p>will change the color scheme for
final figure. Likely using orange and blue to denote closed and
open-loop interventions. Will also add in indication of severed edges<a href="#fnref15" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn16" role="doc-endnote"><p>will change the color scheme for
final figure. Likely using orange and blue to denote closed and
open-loop interventions. Will also add in indication of severed edges<a href="#fnref16" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn17" role="doc-endnote"><p>Figure VAR shows this pretty well,
perhaps sink this section until after discussing categorical and
quantitative?<a href="#fnref17" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn18" role="doc-endnote"><p>below the level set by added,
independent/&#x201C;private&#x201D; sources<a href="#fnref18" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn19" role="doc-endnote"><p>notably, this is part of the
definition of open-loop intervention<a href="#fnref19" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn20" role="doc-endnote"><p>not 100% sure this is true, the
empirical results are really pointing to dR/dW&lt;0 rather than
dR/dS&lt;0. Also this should really be something like <span class="math inline">\(\frac{d|R|}{dS}\)</span> or <span class="math inline">\(\frac{dr^2}{dS}\)</span> since these effects
decrease the <em>magnitude</em> of correlations. I.e. if <span class="math inline">\(\frac{d|R|}{dS} &lt; 0\)</span> increasing <span class="math inline">\(S\)</span> might move <span class="math inline">\(r\)</span> from <span class="math inline">\(-0.8\)</span> to <span class="math inline">\(-0.2\)</span>, i.e.&#xA0;decrease its magnitude not its
value.<a href="#fnref20" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn21" role="doc-endnote"><p>compare especially to <a href="https://www.frontiersin.org/articles/10.3389/fncom.2020.00045/full">&#x201C;Transfer
Entropy as a Measure of Brain Connectivity&#x201D;</a>, <a href="https://www.jneurosci.org/content/29/33/10234">&#x201C;How Connectivity,
Background Activity, and Synaptic Properties Shape the Cross-Correlation
between Spike Trains&#x201D;</a> Figure 3.<a href="#fnref21" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn22" role="doc-endnote"><p>this corroborates Ila Fiete&#x2019;s paper
on bias as a function of recurrent network strength<a href="#fnref22" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
</ol>
</section>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>