<!DOCTYPE html><html><head>
      <title>Closed-Loop Identifiability in Neural Circuits</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"],"imageFont":null},"root":"file:///Users/adam/.dotfiles/atom/packages/markdown-preview-enhanced/node_modules/@shd101wyy/mume/dependencies/mathjax"});
        </script>
        <script type="text/javascript" async src="file:////Users/adam/.dotfiles/atom/packages/markdown-preview-enhanced/node_modules/@shd101wyy/mume/dependencies/mathjax/MathJax.js" charset="UTF-8"></script>
        
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}@font-face {
  font-family: "Material Icons";
  font-style: normal;
  font-weight: 400;
  src: local("Material Icons"), local("MaterialIcons-Regular"), url("data:application/x-font-woff;charset=utf-8;base64,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") format("woff");
}

.admonition {
  box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 1px 5px 0 rgba(0, 0, 0, .12), 0 3px 1px -2px rgba(0, 0, 0, .2);
  position: relative;
  margin: 1.5625em 0;
  padding: 0 1.2rem;
  border-left: .4rem solid rgba(68, 138, 255, .8);
  border-radius: .2rem;
  background-color: rgba(255, 255, 255, 0.05);
  overflow: auto;
}

.admonition>p {
  margin-top: .8rem;
}

.admonition>.admonition-title {
  margin: 0 -1.2rem;
  padding: .8rem 1.2rem .8rem 3.6rem;
  border-bottom: 1px solid rgba(68, 138, 255, .2);
  background-color: rgba(68, 138, 255, .1);
  font-weight: 700;
}

.admonition>.admonition-title:before {
  position: absolute;
  left: 1.2rem;
  font-size: 1.5rem;
  color: rgba(68, 138, 255, .8);
  content: "\E3C9";
}

.admonition>.admonition-title:before {
  font-family: Material Icons;
  font-style: normal;
  font-variant: normal;
  font-weight: 400;
  line-height: 2rem;
  text-transform: none;
  white-space: nowrap;
  speak: none;
  word-wrap: normal;
  direction: ltr;
}

.admonition.summary,
.admonition.abstract,
.admonition.tldr {
  border-left-color: rgba(0, 176, 255, .8);
}

.admonition.summary>.admonition-title,
.admonition.abstract>.admonition-title,
.admonition.tldr>.admonition-title {
  background-color: rgba(0, 176, 255, .1);
  border-bottom-color: rgba(0, 176, 255, .2);
}

.admonition.summary>.admonition-title:before,
.admonition.abstract>.admonition-title:before,
.admonition.tldr>.admonition-title:before {
  color: rgba(0, 176, 255, 1);
  ;
  content: "\E8D2";
}

.admonition.hint,
.admonition.tip {
  border-left-color: rgba(0, 191, 165, .8);
}

.admonition.hint>.admonition-title,
.admonition.tip>.admonition-title {
  background-color: rgba(0, 191, 165, .1);
  border-bottom-color: rgba(0, 191, 165, .2);
}

.admonition.hint>.admonition-title:before,
.admonition.tip>.admonition-title:before {
  color: rgba(0, 191, 165, 1);
  content: "\E80E";
}

.admonition.info,
.admonition.todo {
  border-left-color: rgba(0, 184, 212, .8);
}

.admonition.info>.admonition-title,
.admonition.todo>.admonition-title {
  background-color: rgba(0, 184, 212, .1);
  border-bottom-color: rgba(0, 184, 212, .2);
}

.admonition.info>.admonition-title:before,
.admonition.todo>.admonition-title:before {
  color: rgba(0, 184, 212, 1);
  ;
  content: "\E88E";
}

.admonition.success,
.admonition.check,
.admonition.done {
  border-left-color: rgba(0, 200, 83, .8);
}

.admonition.success>.admonition-title,
.admonition.check>.admonition-title,
.admonition.done>.admonition-title {
  background-color: rgba(0, 200, 83, .1);
  border-bottom-color: rgba(0, 200, 83, .2);
}

.admonition.success>.admonition-title:before,
.admonition.check>.admonition-title:before,
.admonition.done>.admonition-title:before {
  color: rgba(0, 200, 83, 1);
  ;
  content: "\E876";
}

.admonition.question,
.admonition.help,
.admonition.faq {
  border-left-color: rgba(100, 221, 23, .8);
}

.admonition.question>.admonition-title,
.admonition.help>.admonition-title,
.admonition.faq>.admonition-title {
  background-color: rgba(100, 221, 23, .1);
  border-bottom-color: rgba(100, 221, 23, .2);
}

.admonition.question>.admonition-title:before,
.admonition.help>.admonition-title:before,
.admonition.faq>.admonition-title:before {
  color: rgba(100, 221, 23, 1);
  ;
  content: "\E887";
}

.admonition.warning,
.admonition.attention,
.admonition.caution {
  border-left-color: rgba(255, 145, 0, .8);
}

.admonition.warning>.admonition-title,
.admonition.attention>.admonition-title,
.admonition.caution>.admonition-title {
  background-color: rgba(255, 145, 0, .1);
  border-bottom-color: rgba(255, 145, 0, .2);
}

.admonition.attention>.admonition-title:before {
  color: rgba(255, 145, 0, 1);
  content: "\E417";
}

.admonition.warning>.admonition-title:before,
.admonition.caution>.admonition-title:before {
  color: rgba(255, 145, 0, 1);
  content: "\E002";
}

.admonition.failure,
.admonition.fail,
.admonition.missing {
  border-left-color: rgba(255, 82, 82, .8);
}

.admonition.failure>.admonition-title,
.admonition.fail>.admonition-title,
.admonition.missing>.admonition-title {
  background-color: rgba(255, 82, 82, .1);
  border-bottom-color: rgba(255, 82, 82, .2);
}

.admonition.failure>.admonition-title:before,
.admonition.fail>.admonition-title:before,
.admonition.missing>.admonition-title:before {
  color: rgba(255, 82, 82, 1);
  ;
  content: "\E14C";
}

.admonition.danger,
.admonition.error,
.admonition.bug {
  border-left-color: rgba(255, 23, 68, .8);
}

.admonition.danger>.admonition-title,
.admonition.error>.admonition-title,
.admonition.bug>.admonition-title {
  background-color: rgba(255, 23, 68, .1);
  border-bottom-color: rgba(255, 23, 68, .2);
}

.admonition.danger>.admonition-title:before {
  color: rgba(255, 23, 68, 1);
  content: "\E3E7";
}

.admonition.error>.admonition-title:before {
  color: rgba(255, 23, 68, 1);
  content: "\E14C";
}

.admonition.bug>.admonition-title:before {
  color: rgba(255, 23, 68, 1);
  content: "\E868";
}

.admonition.example,
.admonition.snippet {
  border-left-color: rgba(0, 184, 212, .8);
}

.admonition.example>.admonition-title,
.admonition.snippet>.admonition-title {
  background-color: rgba(0, 184, 212, .1);
  border-bottom-color: rgba(0, 184, 212, .2);
}

.admonition.example>.admonition-title:before,
.admonition.snippet>.admonition-title:before {
  color: rgba(0, 184, 212, 1);
  ;
  content: "\E242";
}

.admonition.quote,
.admonition.cite {
  border-left-color: rgba(158, 158, 158, .8);
}

.admonition.quote>.admonition-title,
.admonition.cite>.admonition-title {
  background-color: rgba(158, 158, 158, .1);
  border-bottom-color: rgba(158, 158, 158, .2);
}

.admonition.quote>.admonition-title:before,
.admonition.cite>.admonition-title:before {
  color: rgba(158, 158, 158, 1);
  ;
  content: "\E244";
}

/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
.markdown-preview.markdown-preview {
  /* Heading numbering, inspired by Typora 
     https://support.typora.io/Auto-Numbering/
    */
  counter-reset: h1;
}
.markdown-preview.markdown-preview h1 {
  counter-reset: h2;
}
.markdown-preview.markdown-preview h2 {
  counter-reset: h3;
}
.markdown-preview.markdown-preview h3 {
  counter-reset: h4;
}
.markdown-preview.markdown-preview h4 {
  counter-reset: h5;
}
.markdown-preview.markdown-preview h5 {
  counter-reset: h6;
}
.markdown-preview.markdown-preview h1:before {
  counter-increment: h1;
  content: counter(h1) ". ";
}
.markdown-preview.markdown-preview h2:before {
  counter-increment: h2;
  content: counter(h1) "." counter(h2) ". ";
}
.markdown-preview.markdown-preview h3:before {
  counter-increment: h3;
  content: counter(h1) "." counter(h2) "." counter(h3) ". ";
}
.markdown-preview.markdown-preview h4:before {
  counter-increment: h4;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) ". ";
}
.markdown-preview.markdown-preview h5:before {
  counter-increment: h5;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) ". ";
}
.markdown-preview.markdown-preview h6:before {
  counter-increment: h6;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) "." counter(h6) ". ";
}
.do-number-sections {
  counter-reset: h1;
}
.do-number-sections h1 {
  counter-reset: h2;
}
.do-number-sections h2 {
  counter-reset: h3;
}
.do-number-sections h3 {
  counter-reset: h4;
}
.do-number-sections h4 {
  counter-reset: h5;
}
.do-number-sections h5 {
  counter-reset: h6;
}
.do-number-sections h1:before {
  counter-increment: h1;
  content: counter(h1) ". ";
}
.do-number-sections h2:before {
  counter-increment: h2;
  content: counter(h1) "." counter(h2) ". ";
}
.do-number-sections h3:before {
  counter-increment: h3;
  content: counter(h1) "." counter(h2) "." counter(h3) ". ";
}
.do-number-sections h4:before {
  counter-increment: h4;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) ". ";
}
.do-number-sections h5:before {
  counter-increment: h5;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) ". ";
}
.do-number-sections h6:before {
  counter-increment: h6;
  content: counter(h1) "." counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) "." counter(h6) ". ";
}

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  " id="hide-todo">
      <style>#hide-todo .admonition {
  color: #0f0;
  display: none;
}
#hide-todo details {
  color: #0f0;
  display: none;
}
#hide-todo mark {
  color: #0f0;
  display: none;
}
</style>
<h1 class="mume-header" id="abstract">Abstract</h1>

<div class="admonition todo">
<p class="admonition-title">3/16: - Mention basic science applications of CL control - Maybe more forecasting idea of shaping correlations? (don&apos;t want reader to be surprised by structure of paper&apos;s argument)</p>
</div>
<p>The necessity of intervention in inferring cause has long been understood in neuroscience. Recent work has highlighted the limitations of passive observation and single-site lesion studies in accurately recovering causal circuit structure. The advent of optogenetics has facilitated increasingly precise forms of intervention including closed-loop control which may help eliminate confounding influences. However, it is not yet clear how best to apply closed-loop control to leverage this increased inferential power. In this paper, we use tools from causal inference, control theory, and neuroscience to show when and how closed-loop interventions can more effectively reveal causal relationships. <code>We also examine the performance of standard network inference procedures in simulated spiking networks under passive, open-loop and closed-loop conditions.</code> We demonstrate a unique capacity of feedback control to distinguish competing circuit hypotheses by disrupting connections which would otherwise result in equivalent patterns of correlation<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. Our results build toward a practical framework to improve design of neuroscience experiments to answer causal questions about neural circuits.</p>
<h1 class="mume-header" id="introduction">Introduction</h1>

<h2 class="mume-header" id="estimating-causal-interactions-in-the-brain">Estimating causal interactions in the brain</h2>

<div class="admonition note">
<p class="admonition-title">70% done</p>
</div>
<div class="admonition todo">
<p class="admonition-title">3/16: - &quot;We first propose...&quot; paragraph (could build out or move or change focus away from the &apos;framework&apos;) - think about condensing and/or moving &quot;Inferring causal interactions from time series&quot; subsection - Maybe add half a paragraph or so in the discussion about how causal inference tools can help above correlation analysis (e.g., PC algorithm)</p>
</div>
<p>Many hypotheses about neural circuits are phrased in terms of causal relationships: &quot;will changes in activity to this region of the brain produce corresponding changes in another region?&quot; Understanding these causal relationships is critical to both scientific understanding and to developing effective therapeutic interventions, which require knowledge of how potential therapies will impact brain activity and patient outcomes.</p>
<p>A range of mathematical and practical challenges make it difficult to determine these causal relationships. In studies that rely only observational data, it is often impossible to determine whether observed patterns of activity are caused by known and controlled inputs, or whether they are instead spurious connections generated by recurrent activity, indirect relationships, or unobserved &quot;confounders.&quot; It is generally understood that moving from experiments involving passive observation to more complex levels of intervention allows experimenters to better tackle challenges to circuit identification. However, while chemical and surgical lesion experiments have historically been employed to remove the influence of possible confounds, they are likely to dramatically disrupt circuits from their typical functions, making conclusions about underlying causal structure drawn from these experiments unlikely to hold in naturalistic settings [@chicharro2012when]. <em>Closed-loop</em> interventions [...] <mark>@Adam: short description of closed-loop in neuro, maybe drawing from text in this collapsable:</mark></p>
<details><summary>Proposal text to draw from:</summary>
<p>For decades, engineers have used feedback control to actuate a system based on measured activity to reduce variability, compensate for imperfect measurements, drive systems to desired set points, and decouple connected systems [...]</p>
<p>There is an increasing interest in using approaches from closed-loop control for neural stimulation to both study complex neural circuits and treat neurologic disorders. Recently, a growing community is developing and applying closed-loop stimulation strategies at the cellular and circuit level (Miranda-Dominguez, Gonia, and Netoff 2010; Santaniello, Burns, et al. 2011; Ching et al. 2013; Iolov, Ditlevsen, and Longtin 2014; Nandi, Kafashan, and Ching 2016; Bolus et al. 2018) to understand the brain (Packer et al. 2015) as well as treat disorders (Santaniello, Fiengo, et al. 2011; Paz et al. 2013; Ehrens, Sritharan, and Sarma 2015; Choi et al. 2016; Yang and Shanechi 2016; Koz&#xE1;k and Ber&#xE9;nyi 2017; Sorokin et al. 2017) The advent of optogenetic stimulation has accelerated the potential for effective closed-loop stimulation by providing actuation strategies that can be more precisely targeted and have minimal recording artifacts compared to conventional microelectrode stimulation (Grosenick, Marshel, and Deisseroth 2015)</p>
<p>Most applications of closed-loop control to neuroscience to date have used &#x201C;activity-guided / responsive / triggered stimulation&#x201D; wherein a predesigned stimulus is delivered in response to a detected event. For example, in (Krook-Magnuson et al. 2013) the authors detect seizure activity from spiking and local field potential features to trigger a pulse-train of inhibitory optogenetic stimulation which interrupts the seizure. While this is an effective approach for many applications, these types of closed-loop experiments should be distinguished from closed-loop with ongoing feedback such as dynamic clamp. In these feedback control approaches parameters of stimulation are adjusted on much faster timescales in response to measured activity. For dynamic clamp experiments, this low-latency ongoing feedback control allows experimenters to deliver currents which mimic virtual ion channels which would be implausible with triggered predesigned stimulation. These approaches provide additional precision in being able to drive activity patterns, but also come with increased algorithmic and hardware demands. For the rest of this document, we will use &#x201C;closed-loop control&#x201D; or &#x201C;feedback control&#x201D; to refer to this second, more specific class of approaches.</p>
<p>While many such new actuation and measurement tools have recently become available for neural systems, we require the development of principled algorithmic tools for designing feedback controllers to use these neural interfaces. Our collaborators have previously demonstrated successful closed-loop optogenetic control (CLOC) in-vitro (Newman et al. 2015) and in-vivo (Bolus et al. 2018) to track naturalistic, time-varying trajectories of firing rate.</p>
<ul>
<li>[ ] Also add citation to \cite{ramot2022closedloop}</li>
</ul>
</details>
<p>Despite the promise of these closed-loop strategies for identifying causal relations in neural circuits, however, it is not yet fully understood <em>when</em> more complex intervention strategies can provide additional inferential power, or <em>how</em> these experiments should be optimally designed. In this paper we demonstrate when and how closed-loop interventions can reveal the causal structure governing neural circuits. Drawing from ideas in causal inference<br>
[@pearl2009causality] [@maathuis2016review] \cite{chis2011structural}, we describe the classes of models that can be distinguished by a given set of input-output experiments, and what experiments are necessary to uniquely determine specific causal relationships.</p>
<p>We first propose a mathematical framework that describes how open- and closed-loop interventions impact observable qualities of neural circuits. Using this framework, experimentalists propose a set of candidate hypotheses describing the potential causal structure of the circuit under study, and then select a series of interventions that best allows them to distinguish between these hypotheses. Using both simple controlled models and in silico models of spiking networks, we explore factors that govern the efficacy of these types of interventions. Guided by the results of this exploration, we present a set of recommendations that can guide the design of open- and closed-loop experiments to better uncover the causal structure underlying neural circuits.</p>
<p><strong>Inferring causal interactions from time series.</strong> A number of strategies have been proposed to detect causal relationships between observed variables. Wiener-Granger (or predictive) causality states that a variable <span class="mathjax-exps">$X$</span> &quot;Granger-causes&quot; <span class="mathjax-exps">$Y$</span> if <span class="mathjax-exps">$X$</span> contains information relevant to <span class="mathjax-exps">$Y$</span> that is not contained in <span class="mathjax-exps">$Y$</span> itself or any other variable \cite{wiener1956theory}. This concept has traditionally been operationalized with vector autoregressive models \cite{granger1969investigating}; the requirement that <em>all</em> potentially causative variables be considered makes these notions of dependence susceptible to unobserved confounders \cite{runge2018causal}.</p>
<p>Our work initially focuses on measures of directional interaction that are based on lagged correlations \cite{melssen1987detection}. These metrics look at the correlation of time series collected from pairs of nodes at various lags and detect peaks at negative time lags. Such peaks could indicate the presence of a direct causal relationship -- but they could also stem from indirect causal links or hidden confounders \cite{dean2016dangers}. In these bivariate correlation methods, it is thus necessary to consider patterns of correlation between many pairs of nodes in order to differentiate between direct, indirect, and confounding relationships \cite{dean2016dangers}. This distinguishes these strategies from some multivariate methods that &quot;control&quot; for the effects of potential confounders. While cross-correlation-based measures are generally limited to detecting linear functional relationships between nodes, their computational feasibility makes them a frequent metric of choice in experimental neuroscience work \cite{knox1981detection} \cite{salinas2001correlated} \cite{garofalo2009evaluation}.</p>
<p>Other techniques detect directional interaction stemming from more general or complex relationships. Information-theoretic methods, which use information-based measures to assess the reduction in entropy knowledge of one variable provides about another, are closely related to Granger causality \cite{schreiber2000measuring} \cite{barnett2009granger}. The <em>transfer entropy</em> <span class="mathjax-exps">$T_{X \to Y}(t) = I(Y_t \colon X_{&lt;t} \mid Y_{&lt;t})$</span> extends this notion to time series by measuring the amount of information present in <span class="mathjax-exps">$Y_t$</span> that is not contained in the past of either <span class="mathjax-exps">$X$</span> or <span class="mathjax-exps">$Y$</span> (denoted <span class="mathjax-exps">$X_{&lt;t}$</span> and <span class="mathjax-exps">$Y_{&lt;t}$</span>) \cite{bossomaier2016transfer}. Using transfer entropy as a measure of causal interaction requires accounting for potential confounding variables; the <em>conditional transfer entropy</em> <span class="mathjax-exps">$T_{X \to Y \mid Z}(t) = I(Y_t \colon X_{&lt;t} \mid Y_{&lt;t}, Z_{&lt;t})$</span> conditions on the past of other variables to account for their potential confounding influence \cite[Sec.~4.2.3]{bossomaier2016transfer}. Conditional transfer entropy can thus be interpreted as the amount of information present in <span class="mathjax-exps">$Y$</span> that is not contained in the past of <span class="mathjax-exps">$X$</span>, the past of <span class="mathjax-exps">$Y$</span>, or the past of other variables <span class="mathjax-exps">$Z$</span>.</p>
<p>To quantify the strength of causal interactions, information-theoretic and transfer-entropy-based methods typically require knowledge of the ground truth causal relationships that exist \cite{janzing2013quantifying} or an ability to perturb the system \cite{ay2008information} \cite{lizier2010differentiating}. In practice, these quantities are typically interpreted as &quot;information transfer,&quot; and a variety of estimation strategies and methods to automatically select the conditioning set (i.e., the variables and time lags that should be conditioned on) are used (e.g., \cite{shorten2021estimating}). Multivariate conditional transfer entropy approaches using various variable selection schemes can differentiate between direct interactions, indirect interactions, and common causes, but their results depend on choices such as the binning strategies used to discretize continuous signals, the specific statistical tests used, and the estimator used to compute transfer entropy \cite{wibral2014directed}. <code>[If we end up making the jump to IDTxl in our results: In our empirical results using transfer-entropy-based notions of directional influence we use the IDTxl toolbox \cite{wollstadt2019idtxl}.]</code> However, despite their mathematical differences, previous work has found that cross-correlation-based metrics and information-based metrics tend to produce qualitatively similar results, with similar patterns of true and false positives \cite{garofalo2009evaluation}.</p>
<h2 class="mume-header" id="interventions-in-neuroscience-causal-inference">Interventions in neuroscience &amp; causal inference</h2>

<div class="admonition note">
<p class="admonition-title">70% done</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- Get language more precise and effective <em>(see writing_tasks)</em></p>
</div>
<p>Data collected from experimental settings can provide more inferential power than observational data alone. For example, consider an experimentalist who is considering multiple causal hypotheses for two nodes under study, <span class="mathjax-exps">$x$</span> and <span class="mathjax-exps">$y$</span>: the hypothesis that <span class="mathjax-exps">$x$</span> is driving <span class="mathjax-exps">$y$</span>, the hypothesis that <span class="mathjax-exps">$y$</span> is driving <span class="mathjax-exps">$x$</span>, or the hypothesis that the two variables are being independently driven by a hidden confounder. Observational data revealing that <span class="mathjax-exps">$x$</span> and <span class="mathjax-exps">$y$</span> produce correlated time-series data is equally consistent with each of these three causal hypotheses, providing the experimentalist with no inferential power. Experimentally manipulating <span class="mathjax-exps">$x$</span> and observing the output of <span class="mathjax-exps">$y$</span>, however, allows the scientist to begin to establish which causal interaction pattern is at work. Consistent with intuition from neuroscience literature, a rich theoretical literature has described the central role of interventions in inferring causal structure from data \cite{pearl2009causality, eberhardt2007interventions}.</p>
<p><img src="figures/core_figure_sketches/figure1_sketch.png" alt></p>
<blockquote>
<p><strong>Figure INTRO:</strong> Examples of the roles interventions have played in neuroscience. (A) <em>Passive observation</em> does not involve stimulating the brain. In this example, passive observational data is used to identify patients suffering from absence seizures. (B) <em>Open-loop stimulation</em> involves recording activity in the brain after perturbing a region with a known input signal. Using systematic <em>open-loop stimulation experiments</em>, Penfield uncovered the spatial organization of how senses and movement are mapped in the cortex \cite{penfield1937somatic} \cite{penfield1950cerebral}. (C) <em>Closed-loop control</em> uses feedback control to precisely specify activity in certain brain regions regardless of activity in other regions. Using closed-loop control, <mark>todo-Adam</mark> \cite<mark>todo-Adam</mark>.</p>
</blockquote>
<p>The inferential power of interventions is depends on <em>where</em> stimulation is applied: interventions on some portions of a system may provide more information about the system&apos;s causal structure than interventions in other areas. And interventions are also more valuable when they more effectively set the state of the system: &quot;perfect&quot; closed-loop control, which completely severs a node&apos;s activity from its inputs, are often more informative than &quot;soft&quot; interventions that only partially control a part of the system \cite{eberhardt2007interventions}.</p>
<p>In experimental neuroscience settings, experimenters are faced with deciding between interventions that differ in both location and effectiveness. For example, stimulation can often only be applied to certain regions of the brain. And while experimenters may be able to exactly manipulate activity in some parts of the brain using closed-loop control, in other locations it may only be possible to apply weaker forms of intervention that perturb a region but do not manipulate its activity exactly to a desired state. In Section [X], we compare the effectiveness of open-loop, closed-loop, and partially-effective closed-loop control.</p>
<p>Although algorithms designed to choose optimal interventions are often designed for simple models with strong assumptions,<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> they provide intuition that can aid practitioners seeking to design real-world experiments that provide as much scientific insight as possible.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> Importantly, the informativeness of interventions is often independent of the algorithm used to infer causal connections, meaning that certain interventions can reveal portions of a circuit&apos;s causal structure that would be impossible for <em>any</em> algorithm to infer from only observational data \cite{das2020systematic} <mark>(&lt;- Matt to Adam: make sure this citation is in the right place)</mark>. We similarly expect the results we demonstrate in this paper to both inform experimentalists and open avenues for further research.</p>
<h2 class="mume-header" id="representations-reachability">Representations &amp; reachability</h2>

<div class="admonition note">
<p class="admonition-title">60% done</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- Rewrite X=XW+E as vector version - Describe what &apos;reachability&apos; is <em>(see writing_tasks)</em></p>
</div>
<p>@ import &quot;/section_content/background_representation_reach.md&quot;</p>
<div class="admonition note">
<p class="admonition-title">70% done</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- Talk about what &apos;reachability&apos; means (total direct+indirect impact) - [Matt:] Rewrite first paragraph to not use notation (place this box before any theory/notation sections) - [Matt:] Set expectation here that we&apos;re talking about linear Gaussian circuits</p>
</div>
<p><img src="figures/core_figure_sketches/circuit_walkthrough_3circuits_key_sketch.png" alt title="generated by /code/fig_circuit_walkthrough.py"></p>
<blockquote>
<p><strong>Figure DEMO <em>(box format)</em>: Applying CLINC to distinguish a pair of circuits</strong></p>
<p>Consider the three-node identification problem shown in the figure above, in which the experimenter has identified three hypotheses for the causal structure of the circuit. These circuit hypotheses, shown as directed graphs in column 1, can each also be represented by an adjacency matrix of the form \ref{eq:adjacency-matrix}: for example, circuit A is represented by an adjacency matrix in which <span class="mathjax-exps">$w_{01}$</span>, <span class="mathjax-exps">$w_{20}$</span>, and <span class="mathjax-exps">$w_{21} \neq 0$</span>. Note that hypotheses A and C have direct connections between nodes 0 and 2; while hypothesis B does not have a direct connection between these nodes, computing the weighted reachability matrix <span class="mathjax-exps">$\widetilde{W}$</span> in circuit B an <em>indirect</em> connection exists through the path 2 <span class="mathjax-exps">$\to$</span> 1 <span class="mathjax-exps">$\to$</span> 0 (illustrated in gray in column 2).</p>
<p>Because there are direct or indirect connections between each pair of nodes, passive observation of each hypothesized circuit would reveal that each pair of nodes is correlated (column 3). These three hypotheses are therefore difficult to distinguish<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> for an experimentalist who performs only passive observation, but can be distinguished through stimulation.</p>
<p>Column 4 shows the impact on observed correlations of performing <em>open-loop</em> control on node 1. In hypothesis A, node 1 is not a driver of other nodes, so open-loop stimulation at this site will not increase the correlation between the signal observed at node 1 and other nodes. The path from node 1 to 0 in hypotheses B and C, meanwhile, causes the open-loop stimulation at node 1 to <em>increase</em> the observed correlation between nodes 1 and 0. An experimenter can thus distinguish between hypothesis A and the other two hypotheses by appling open-loop control and observing the resulting pattern of correlations (column 4). However, this pattern of open-loop stimulation would not allow the experimenter to distinguish between hypotheses B and C.</p>
<p><em>Closed-loop</em> control (columns 5 and 6) can provide the experimenter with even more inferential power. Column 5 shows the resulting adjacency matrix when this closed-loop control is applied to node 1. In each hypothesis, the impact of this closed-loop control is to remove the impact of other nodes on node 1, because when perfect closed-loop is applied the activity of node 1 is completely independent of other nodes. (These severed connections are depicted in column 5 by dashed lines.) In hypothesis B, this also results in the elimation of the indirect connection from node 2 to node 1. The application of closed-loop control at node 1 thus results in a different observed correlation structure in each of the three circuit hypotheses (column 6). This means that the experimenter can therefore distinguish between these circuit hypotheses by applying closed-loop control -- a task not possible with passive observation or open-loop control.</p>
</blockquote>
<details><summary>&#x21AA; figure to do items for @Adam</summary>
<ul>
<li>[ ] @Adam - change labels at top from &quot;B&quot; to &quot;1&quot;</li>
<li>[ ] @Adam - add (A) (B) (C) labels to each row</li>
<li>[ ] @Adam - in legend, change in/direct &quot;edge&quot; to in/direct &quot;connection&quot;</li>
<li>[ ] @Adam - in legend, orange dashed arrow to dark gray</li>
</ul>
</details>
<details><summary>&#x21AA;2,3 circuit versions, straight from code</summary>
<p><img src="code/network_analysis/results/circuit_walkthrough_2circuits.png" alt title="generated by /code/fig_circuit_walkthrough.py"><br>
<img src="code/network_analysis/results/circuit_walkthrough_3circuits.png" alt title="generated by /code/fig_circuit_walkthrough.py"></p>
<blockquote>
<p>3 circuit walkthrough, walkthrough will all intervention locations might be appropriate for the supplement</p>
</blockquote>
</details>
<details><summary>&#x21AA;to do items</summary>
<ul>
<li>[ ] find and include frequent circuit (curto + motif)</li>
<li>[ ] wrap circuits we want in <code>example_circuits.py</code></li>
<li>[ ] alt method of displaying indirect paths?
<ul>
<li><a href="https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.simple_paths.all_simple_paths.html#networkx.algorithms.simple_paths.all_simple_paths">https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.simple_paths.all_simple_paths.html#networkx.algorithms.simple_paths.all_simple_paths</a></li>
</ul>
</li>
</ul>
</details>
<details><summary> &#x21AA;see also</summary>
<p>more inspiration:</p>
<ul>
<li>Combining multiple functional connectivity methods to improve causal inferences</li>
<li>Advancing functional connectivity research from association to causation</li>
<li>Fig1. of &quot;Systematic errors in connectivity&quot;</li>
</ul>
<p><img src="code/network_analysis/results/effect_of_control_horiz.png" alt><br>
<img src="figures/misc_figure_sketches/two_circuit_case_study_mockup.png" alt></p>
<blockquote>
<p>this figure does a great job of:</p>
<ul>
<li>setting up a key</li>
<li>incrementally adding confounds</li>
<li>highlighting severed edges<br>
this figure does NOT</li>
<li>explicitly address mutliple hypotheses</li>
</ul>
</blockquote>
<p><img src="figures/misc_figure_sketches/closed_loop_severs_inputs.png" alt><br>
<strong>Figure 11: Closed-loop control compensates for inputs to a node in simple circuits:</strong> The left column shows a simple circuit and recording and stimulation sites for an open-loop experiment. The right column shows the functional circuit which results from closed-loop control of the output of region A. Generally, assuming perfectly effective control, the impact of other inputs to a controlled node is nullified and therefore crossed off the functional circuit diagram.</p>
<blockquote>
<p>this figure does a great job of:</p>
<ul>
<li>using a minimal version of the key above</li>
<li>showing two competing hypotheses</li>
<li>(throughs latent / common modulation in for fun)</li>
</ul>
</blockquote>
<p><img src="figures/misc_figure_sketches/closed_loop_distinguishes_corticalEI.png" alt><br>
<strong>Figure 12: Closed-loop control allows for two circuit hypotheses to be distinguished.</strong> Two hypothesized circuits for the relationships between pyramidal (Pyr, excitatory), parvalbumin-positive (PV, inhibitory), and somatostain-expressing (Som, inhibitory) cells are shown in the two rows. Dashed lines in the right column represent connections whose effects are compensated for through closed-loop control of the Pyr node. By measuring correlations between recorded regions during closed-loop control it is possible to distinguish which hypothesized circuit better matches the data. Notably in the open-loop intervention, activity in all regions is correlated for both hypothesized circuits leading to ambiguity.</p>
</details>
<details><summary>&#x21AA;more notes</summary>
<p>probably want</p>
<ul>
<li>
<p>two circuits which look clearly different</p>
<ul>
<li>! but which have equivalent reachability</li>
<li>possibly with reciprocal connections</li>
<li>possssibly with common modulation</li>
</ul>
</li>
<li>
<p>do we need to reflect back from set of possible observations to consistent hypotheses?</p>
<ul>
<li>mention markov equivalence classes explicitly?</li>
</ul>
</li>
<li>
<p>intuitive explanation using binary reachability rules</p>
<!-- - consider postponing until we introduce intervention?
- i.e. have one figure that walks through both reachability and impact of intervention -->
</li>
<li>
<p><em>point to the rest of the paper as deepening and generalizing these ideas</em></p>
</li>
<li>
<p><em>(example papers - Advancing functional connectivity research from association to causation, Combining multiple functional connectivity methods to improve causal inferences)</em></p>
</li>
<li>
<p>connect <strong>graded reachability</strong> to ID-SNR</p>
<ul>
<li><span class="mathjax-exps">$\mathrm{IDSNR}_{ij}$</span> measures the strength of signal related to the connection <span class="mathjax-exps">$i&#x2192;j$</span> relative to in the output of node <span class="mathjax-exps">$j$</span></li>
<li>for true, direct connections this quantity increasing means a (true positive) connection will be identified more easily (with high certainty, requiring less data)</li>
<li>for false or indirect connections, this quantity increasing means a false positive connection is more likely to be identified</li>
<li>as a result we want to maximize IDSNR for true links, and minimize it for false/indirect links</li>
</ul>
</li>
</ul>
<p>( see also <code>sketches_and_notation/walkthrough_EI_dissection.md</code> )</p>
</details>
<h1 class="mume-header" id="theory-prediction">Theory / Prediction</h1>

<p><img src="figures/core_figure_sketches/methods_overview_pipeline_sketch.png" alt></p>
<blockquote>
<p><strong>Figure OVERVIEW:</strong> ...</p>
</blockquote>
<h2 class="mume-header" id="predicting-correlation-structure-theory">Predicting correlation structure (theory)</h2>

<p>A linear-Gaussian circuit can be described by 1) the variance of the gaussian private (independent) noise at each node, and 2) the weight of the linear relationships between each pair of connected nodes. Let <span class="mathjax-exps">$s \in \mathbb{R}^p$</span> denote the variance of each of the <span class="mathjax-exps">$p$</span> nodes in the circuit, and <span class="mathjax-exps">$W \in \mathbb{R}^{p \times p}$</span> denote the matrix of connection strengths such that </p><div class="mathjax-exps">$$W_{ij} = \text{strength of $i \to j$ connection}.$$</div><p></p>
<p>Note that <span class="mathjax-exps">$\left[(W^T) s\right]_j$</span> gives the variance at node <span class="mathjax-exps">$j$</span> due to length-1 (direct) connections, and more generally, <span class="mathjax-exps">$\left[ (W^T)^k s \right]_j$</span> gives the variance at node <span class="mathjax-exps">$j$</span> due to length-<span class="mathjax-exps">$k$</span> (indirect) connections. The <em>total</em> variance at node <span class="mathjax-exps">$j$</span> is thus <span class="mathjax-exps">$\left[ \sum_{k=0}^{\infty} (W^T)^k s \right]_j$</span>.</p>
<p>Our goal is to connect private variances and connection strengths to observed pairwise correlations in the circuit. Defining <span class="mathjax-exps">$X \in \mathbb{R}^{p \times n}$</span> as the matrix of <span class="mathjax-exps">$n$</span> observations of each node, we have<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup><br>
</p><div class="mathjax-exps">$$\begin{aligned}     \Sigma &amp;= \mathrm{cov}(X) = \mathbb{E}\left[X X^T\right] \\     &amp;= (I-W^T)^{-1} \mathrm{diag}(s) (I-W^T)^{-T} \\     &amp;= \widetilde{W} \mathrm{diag}(s) \widetilde{W}^T, \end{aligned}$$</div><br>
where <span class="mathjax-exps">$\widetilde{W} = \sum_{k=0}^{\infty} (W)^k$</span> denotes the <em>weighted reachability matrix</em>, whose <span class="mathjax-exps">$(i,j)^\mathrm{th}$</span> entry indicates the total influence of node <span class="mathjax-exps">$i$</span> on node <span class="mathjax-exps">$j$</span> through both direct and indirect connections.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> That is, <span class="mathjax-exps">$\widetilde{W}_{ij}$</span> tells us how much variance at node <span class="mathjax-exps">$j$</span> would result from injecting a unit of private variance at node <span class="mathjax-exps">$i$</span>. We can equivalently write <span class="mathjax-exps">$\Sigma_{ij} = \sum_{k=1}^p \widetilde{W}_{ik} \widetilde{W}_{jk} s_k$</span>.<p></p>
<p>Under passive observation, the squared correlation coefficient can thus be written as<br>
</p><div class="mathjax-exps">$$\begin{aligned}     r^2(i,j) &amp;= \frac{\Sigma_{ij}}{\Sigma_{ii} \Sigma_{jj}} \\     &amp;= \frac{\left( \sum_{k=1}^p \widetilde{W}_{ik} \widetilde{W}_{jk} s_k \right)^2}{\left(\sum_{k=1}^p \widetilde{W}_{ik}^2 s_k\right)\left(\sum_{k=1}^p \widetilde{W}_{jk}^2 s_k\right)}. \end{aligned}$$</div><p></p>
<p>This framework also allows us to predict the impact of open- and closed-loop control on the pairwise correlations we expect to observe. To model the application of open-loop control on node <span class="mathjax-exps">$c$</span>, we add an arbitrary amount of private variance to <span class="mathjax-exps">$s_c$</span>: <span class="mathjax-exps">$s_c \leftarrow s_c + s_c^{(OL)}$</span>. To model the application of closed-loop control on node <span class="mathjax-exps">$c$</span>, we first sever inputs to node <span class="mathjax-exps">$c$</span> by setting <span class="mathjax-exps">$W_{k,c} = 0$</span> for <span class="mathjax-exps">$k = 1, \dots p$</span>, and then set the private variance of node <span class="mathjax-exps">$c$</span> by setting <span class="mathjax-exps">$s_c$</span> to any arbitrary value. Because <span class="mathjax-exps">$c$</span>&apos;s inputs have been severed, this private noise will become exactly node <span class="mathjax-exps">$c$</span>&apos;s output variance.</p>
<div class="admonition todo">
<p class="admonition-title">[Matt:] add table from <code>sketches_and_notation/intro-background/causal_vs_expt.md</code> and modify text above to match</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- Some redundancy with simulation methods; cut and paste anything useful in 4.2 and put into 3.1 / 3.2</p>
</div>
<h1 class="mume-header" id="simulation-methods">Simulation Methods</h1>

<div class="admonition todo">
<p class="admonition-title">- reorganize / split sections</p>
</div>
<p>@ import &quot;methods0_0_overview.md&quot;</p>
<h2 class="mume-header" id="modeling-network-structure-and-dynamics">Modeling network structure and dynamics</h2>

<div class="admonition note">
<p class="admonition-title">70% done</p>
</div>
<details><summary>&#x21AA;to do</summary>
<ul>
<li>[~] read e.g.</li>
<li>[ ] discuss networks - adj &#x2705;</li>
<li>discuss 2 key dimensions of complexity
<ul>
<li>linear-gaussian v.s. spiking (LIF - Poisson?) &#x1F4AB;</li>
<li>contemporaneous v.s. delayed connections &#x1F4AB;</li>
</ul>
</li>
<li>[ ] discuss brian implementation (supplement) &#x1F4AB;</li>
</ul>
</details>
<p>We sought to understand both general principles (abstracted across particulars of network implementation) as well as some practical considerations introduced by dealing with spikes and synapses.</p>
<h3 class="mume-header" id="stochastic-network-dynamics">Stochastic network dynamics</h3>

<p>The first approach is accomplished with a network of nodes with gaussian noise sources, linear interactions, and linear dynamics. The second approach is achieved with a network of nodes consisting of populations of leaky integrate-and-fire (LIF) neurons. These differ from the simpler case in their nonlinear-outputs, arising from inclusion of a spiking threshold. Interactions between neurons happen through spiking synapses, meaning information is passed between neurons sparsely in time<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>.</p>
<p><em>Neuron dynamics:</em><br>
</p><div class="mathjax-exps">\[\frac{dV}{dt} = \frac{V_0 + I - V}{\tau_m} + \sigma_m \sqrt{\tau_m} \xi(t)\]</div><p></p>
<h3 class="mume-header" id="time-resolvable-interactions">Time-resolvable interactions</h3>

<p>Additionally we study two domains of interactions between populations; contemporaneous and delay-resolvable connections. These domains represent the relative timescales of measurement versus timescale of synaptic delay.</p>
<p><mark>DANGER doesnt work with pandoc</mark></p>
<blockquote>
<p>correlation across positive and negative lags between two outputs</p>
</blockquote>
<p>In the delay-resolvable domain, directionality of connections may be inferred even under passive observations by looking at temporal precedence - whether the past of one signal is more strongly correlated with future lags of another signal <em>(i.e. cross-correlation)</em>. In the contemporaneous domain, network influences act within the time of a single sample<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> so this temporal precedence clue is lost (although directionality can still be inferred in the presence of intervention).</p>
<p>The following work is presented with the linear-Gaussian and contemporaneous domains as the default for simplicity and conciseness.</p>
<div class="admonition note">
<p class="admonition-title">talk about the extension to time-resolvable, spiking if it ends up being included</p>
</div>
<details><summary>&#x21AA;concept figures</summary>
<p><img src="figures/whiteboard/concept_time_resolved.png" alt><br>
<img src="figures/whiteboard/concept_open_loop_contemporaneous.png" alt></p>
</details>
<h3 class="mume-header" id="code-implementation">Code implementation</h3>

<p>Software for data generation, analysis, and plotting is available at <a href="https://github.com/awillats/clinc">https://github.com/awillats/clinc</a>.<br>
Both linear-gaussian and spiking networks are simulated with code built from the <a href="https://elifesciences.org/articles/47314">Brian2</a> spiking neural network simulator. This allows for highly modular code with easily interchanged neuron models and standardized output preprocessing and plotting. It was necessary to write an additional custom extension to Brian2 in order to capture delayed linear-gaussian interactions, available at <a href="https://github.com/awillats/brian_delayed_gaussian">brian_delayed_gaussian</a>. With this added functionality, it is possible to compare the equivalent network parameters only changing linear-gaussian versus spiking dynamics and inspect differences solely due to spiking.</p>
<div class="admonition note">
<p class="admonition-title">talk about parameter choices and ranges?</p>
</div>
<p><em>see <a href="_network_parameters_table.md">_network_parameters_table.md</a> for list of relevant parameters</em></p>
<h2 class="mume-header" id="implementing-interventions">Implementing interventions</h2>

<div class="admonition note">
<p class="admonition-title">70% done</p>
</div>
<div class="admonition note">
<p class="admonition-title">assumed: effect of interventions on theory already address</p>
</div>
<p><img src="figures/core_figure_sketches/figure1_sketch.png" alt></p>
<p>To study the effect of various interventions we simulated inputs to nodes in a network. In the <strong>passive setting</strong>, nodes receive additive drive from <em>private</em> Gaussian noise sources common to all neurons within a node, but independent across nodes. The variance of this noise is specified by <span class="mathjax-exps">$\sigma_m \sqrt{\tau_m}$</span>.<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></p>
<p></p><div class="mathjax-exps">\[\frac{dV}{dt} = \frac{V_0 + I - V}{\tau_m} + \sigma_m \sqrt{\tau_m} \xi(t)\]</div><p></p>
<p>To emulate <strong>open-loop intervention</strong> we simulated current injection from an external source. This is intended to represent experiments involving stimulation from microelectrodes or optogenetics <em>(albeit simplifying away any impact of actuator dynamics)</em>. By default, open-loop intervention is specified as white noise sampled at each timestep from Gaussian distribution with mean and variance <span class="mathjax-exps">$\mu_{intv.}$</span> and <span class="mathjax-exps">$\sigma^2_{intv.}$</span><sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup></p>
<p></p><div class="mathjax-exps">\[I_{open-loop} \sim \mathcal{N}(\mu_{intv.},\,\sigma^{2}_{intv.})\\\]</div><br>
Ignoring the effect of signal means in the linear-Gaussian setting:<br>
<div class="mathjax-exps">\[X_k = f(\sigma^2_m, \sigma^{2}_{intv.})\]</div><br>
<code>per-node indexing needs resolving here also</code><p></p>
<p>Ideal <strong>closed-loop control</strong> is able to overwrite the output of a node, setting it precisely to the specified target.<br>
<code>making up notation as I go here, needs tightening up:</code><br>
</p><div class="mathjax-exps">\[\begin{aligned} T &amp;\sim \mathcal{N}(\mu_{intv.},\,\sigma^{2}_{intv.}) \\ I_{closed-loop} &amp;= f(X, T)  \\ X_k | CL_{k} &amp;\approx T \end{aligned}\]</div><br>
Note that in this setting, the <em>output</em> of a node <span class="mathjax-exps">$X_k$</span> under closed-loop control is identical to the target, therefore<br>
<div class="mathjax-exps">\[X_k | CL_{k} = f(\sigma^{2}_{intv.}) \perp \sigma^2_m\]</div><br>
In practice, near-ideal control is only possible with very fast measurement and computation relative to the network&apos;s intrinsic dynamics, such as in the case of dynamic clamp<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>. To demonstrate a broader class of closed-loop interventions (such as those achievable with extracellular recording and stimulation), imperfect &quot;partial&quot; control is simulated by linearly interpolating the output of each node between the target <span class="mathjax-exps">$T$</span> and the uncontrolled output based on a control effectiveness parameter <span class="mathjax-exps">$\gamma$</span><p></p>
<p></p><div class="mathjax-exps">\[X | CL_{k, \gamma} = \gamma T + (1-\gamma) X\]</div><p></p>
<p>In the full discrete-time simulation, closed-loop interventions are instead simulated through a proportional-integral-derivative (PID) control policy with control efficacy determined functionally by the strength of controller gains <span class="mathjax-exps">$K = \{k_P, k_I, k_D\}$</span> relative to the dynamics of the network.</p>
<p></p><div class="mathjax-exps">\[I_{PID} = \text{PID}(X,T| K)\]</div><p></p>
<p>Another interesting intervention to study is <strong>open-loop replay of a closed-loop stimulus</strong>, <em>that is</em> taking a particular injected current <span class="mathjax-exps">$I_{CL,\,prev}$</span> used to drive nodes to a target <span class="mathjax-exps">$T_{prev}$</span> and adding it back to the network in a separate trial.</p>
<p>Because the instantiation of noise in the network will be different from trial to trial, this &quot;replay&quot; stimulus will no longer adapt sample-by-sample (therefore it should be considered open-loop) and the node&apos;s output cannot be expected to match the target precisely, however the statistics of externally applied inputs will be the same. In effect, the comparison between closed-loop and open-loop replay conditions reveals the specific effect of feedback intervention while controlling for any confounds from input statistics.</p>
<h2 class="mume-header" id="extracting-circuit-estimates">Extracting circuit estimates</h2>

<div class="admonition note">
<p class="admonition-title">10% done</p>
</div>
<blockquote>
<p><em>refer to methods overview figure</em></p>
</blockquote>
<p>While a broad range of techniques<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup> exist for inferring functional relationships from observational data, <code>(for the majority of this work)</code> we choose to focus on simple bivariate correlation as a measure of dependence in the linear-Gaussian network. The impact of intervention on this metric is analytically tractable <em>(see <a href="methods1_predicting_correlation.md">methods1_predicting_correlation.md</a>)</em>, and can be thought of as a prototype<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup> for more sophisticated measures of dependence such as time-lagged cross-correlations, bivariate and multivariate transfer entropy.</p>
<p>We implement a naive comparison strategy to estimate the circuit adjacency from emprical correlations; Thresholded empirical correlation matrices are compared to correlation matrices predicted from each circuit in a hypothesis set. Any hypothesized cirucits which are predicted to have a similar correlation structure as is observed (i.e. corr. mats equal after thresholding) are marked as &quot;plausible circuits.&quot;<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup> If only one circuit amongst the hypothesis set is a plausible match, this is considered to be the estimated circuit. The threshold for &quot;binarizing&quot; the empirical correlation matrix is treated as a hyperparameter to be swept at the time of analysis.<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup></p>
<h2 class="mume-header" id="information-theoretic-measures-of-hypothesis-ambiguity">Information-theoretic measures of hypothesis ambiguity</h2>

<div class="admonition note">
<p class="admonition-title">10% done</p>
</div>
<p><em>see <a href="_steps_of_inference.md">_steps_of_inference.md</a> for entropy writeup</em></p>
<h1 class="mume-header" id="results">Results</h1>

<div class="admonition note">
<p class="admonition-title">overall, 60% done</p>
</div>
<h2 class="mume-header" id="impact-of-intervention-on-estimation-performance">Impact of intervention on estimation performance</h2>

<h3 class="mume-header" id="intervening-provides-categorical-improvements-in-inference-power-beyond-passive-observation">Intervening provides (categorical) improvements in inference power beyond passive observation</h3>

<div class="admonition note">
<p class="admonition-title">Application to demo set, entropy over hypotheses - 50% done</p>
</div>
<details><summary>&#x21AA;notes, see also </summary>
<details><summary>going to assume these have already been discussed</summary>
<ul>
<li>predicting correlation</li>
<li>measuring dependence</li>
<li>markov equivalence</li>
</ul>
</details>
<p><a href="_steps_of_inference.md">Methods: Procedure for choosing &amp; applying intervention</a></p>
</details>
<p>Next, we apply (steps 1-3 of) this circuit search procedure to a collection of closely related hypotheses for 3 interacting nodes<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> to illustrate the impact of intervention. &#x1F6A7; <code>most of the story in the figure caption for now</code> &#x1F6A7;</p>
<p><a id="fig-disambig" href></a><br>
<img src="figures/core_figure_sketches/circuit_entropy_sketch.png" alt></p>
<blockquote>
<p><strong>Figure DISAMBIG: Interventions narrow the set of hypotheses consistent with observed correlations</strong><br>
<em>source: <a href="https://docs.google.com/drawings/d/1CBp1MhOW7OGNuBvo7OkIuzqnq8kmN8EEX_AkFuKpVtM/edit">google drawing</a></em><br>
<strong>(A)</strong> Directed adjacency matrices represent the true and hypothesized causal circuit structure<br>
<strong>(B)</strong> Directed reachability matrices represent the direct <em>(black)</em> and indirect <em>(grey)</em> influences in a network. Notably, different adjacency matrices can have equivalent reachability matrices making distinguishing between similar causal structures difficult, even with open-loop control.<br>
<strong>(C)</strong> Correlations between pairs of nodes. Under passive observation, the direction of influence is difficult to ascertain. In densely connected networks, many distinct ground-truth causal structures result in similar &quot;all correlated with all&quot; patterns providing little information about the true structure.<br>
<strong>(D-F)</strong> The impact of open-loop intervention at each of the nodes in the network is illustrated by modifications to the passive correlation pattern. Thick orange<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup> edges denote correlations which increase above their baseline value with high variance open-loop input. Thin blue<sup class="footnote-ref"><a href="#fn17" id="fnref17:1">[17:1]</a></sup> edges denote correlations which decrease, often as a result of increased connection-independent &quot;noise&quot; variance in one of the participating nodes. Grey edges are unaffected by intervention at that location.<br>
A given hypotheses set (A) will result in an &quot;intervention-specific fingerprint&quot;, that is a distribution of frequencies for observing patterns of modified correlations <em>(across a single row within D-F)</em>. If this fingerprint contains many examples of the same pattern of correlation (such as <strong>B</strong>), many hypotheses correspond to the same observation, and that experiment contributes low information to distinguish between structures. A maximally informative intervention would produce a unique pattern of correlation for each member of the hypothesis set.<br>
&#x1F6A7;<code>caption too long</code></p>
</blockquote>
<div class="admonition note">
<p class="admonition-title">Explain why closed-loop helps - link severing - 5% done</p>
</div>
<p><strong>Why does closed-loop control provide a categorical advantage?</strong> Because it severs indirect links<br>
<code>is this redundant with intro?</code><br>
<code>needs to be backed here up by aggregate results?</code></p>
<ul>
<li>this is especially relevant in recurrently connected networks where the reachability matrix becomes more dense.</li>
<li>more stuff is connected to other stuff, so there are more indirect connections, and the resulting correlations look more similar (more circuits in the equivalence class)</li>
<li>patterns of correlation become more specific with increasing intervention strength
<ul>
<li>more severed links &#x2192; more unique adjacency-specific patterns of correlation</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Where you intervene</strong><sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup> strongly determines the inference power of your experiment.<br>
<strong>secondary point:</strong> having (binary) prediction helps capture this relationship</p>
</blockquote>
<div class="admonition note">
<p class="admonition-title">Quantitative impact of closed-loop - 70% done</p>
</div>
<h3 class="mume-header" id="stronger-intervention-shapes-correlation-resulting-in-more-data-efficient-inference-with-less-bias">Stronger intervention shapes correlation, resulting in more data-efficient inference with less bias</h3>

<div class="admonition note">
<p class="admonition-title">Explain why closed-loop helps - bidirectional variance control - 60% done</p>
</div>
<p>While a primary advantage of closed-loop interventions for circuit inference is its ability to functionally lesion indirect connections, another, more nuanced <code>(quantitative)</code> advantage of closed-loop control lies in its capacity to bidirectionally control output variance. While the variance of an open-loop stimulus can be titrated to adjust the output variance at a node, in general, an open-loop stimulus cannot reduce this variance below its instrinsic<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> variability. That is, if the system is linear with gaussian noise,</p>
<div class="admonition todo">
<p class="admonition-title">- this is very closely related to 4.2 implementing interventions, description of impact of intervention on variance should perhaps be moved there... or the supplement?</p>
</div>
<p></p><div class="mathjax-exps">$$\mathbb{V}_{i}(C|S=\text{open},\sigma^2_S) \geq \mathbb{V}_{i}(C)$$</div><br>
More specifically, if the open-loop stimulus is statistically independent from the intrinsic variability<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup><br>
<div class="mathjax-exps">$$\mathbb{V}_{i}(C|S=\text{open},\sigma^2_S) = \mathbb{V}_{i}(C) + \sigma^2_S$$</div><br>
Applying closed-loop to a linear gaussian circuit:<p></p>
<p></p><div class="mathjax-exps">\[\begin{aligned} \mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) &amp;= \sigma^2_S  \\ \mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) &amp;\perp \mathbb{V}_{i}(C) \end{aligned}\]</div><p></p>
<details><summary> &#x21AA; Firing rates couple mean and variance </summary> 
<p>In neural circuits, we&apos;re often interested in firing rates, which are non-negative. This particular output nonlinearity means that the linear gaussian assumptions do not hold, especially in the presence of strong inhibitory inputs. In this setting, firing rate variability is coupled to its mean rate; Under a homoeneous-rate Poisson assumption, mean firing rate and firing rate variability would be proportional. With inhibitory inputs, open-loop stimulus can drive firing rates low enough to reduce their variability. Here, feedback control still provides an advantage in being able to control the mean and variance of firing rates independently<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup></p>
<p></p><div class="mathjax-exps">\[\begin{aligned} \mu^{out}_i &amp;= f(\mu^{in}_i, \mathbb{V}^{in}_i)\\ \mathbb{V}^{out}_{i}(C) &amp;= f(\mu^{out}_i, \mathbb{V}^{in}_i) \end{aligned}\]</div><p></p>
</details>
<details><summary> &#x21AA; Notes on imperfect control </summary> 
<p><code>Ideal control</code><br>
</p><div class="mathjax-exps">\[\mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) = \sigma^2_S\]</div><br>
<code>Imperfect control</code> - intuitively feedback control is counteracting / subtracting disturbance due to unobserved sources, including intrinsic variability. We could summarize the effectiveness of closed-loop disturbance rejection with a scalar <span class="mathjax-exps">$0\leq\gamma\leq1$</span><br>
<div class="mathjax-exps">\[\mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) = \mathbb{V}_{i}(C) - \gamma\mathbb{V}_{i}(C) + \sigma^2_S \\ \mathbb{V}_{i}(C|S=\text{closed},\sigma^2_S) = (1-\gamma) \mathbb{V}_{i}(C) + \sigma^2_S\]</div><p></p>
</details>
<div class="admonition note">
<p class="admonition-title">reference <a href="#fig-var">figvar</a> to empricially show this bidirectional control of output variance?</p>
</div>
<h4 class="mume-header" id="impact-of-intervention-location-and-variance-on-pairwise-correlations">Impact of intervention location and variance on pairwise correlations</h4>

<p><a href="methods1_predicting_correlation.md">related methods</a></p>
<div class="admonition todo">
<p class="admonition-title">- again, feels very backgroundy / discussiony ... where to put this?</p>
</div>
<p>We have shown that closed-loop interventions provide more flexible control over output variance of nodes in a network, and that shared and independent sources of variance determine pairwise correlations between node outputs. Together, this suggests closed-loop interventions may allow us to shape the pattern of correlations with more degrees of freedom<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup> <code>[why do we want to?...]</code></p>
<p>One application of this increased flexibility is to increase correlations associated with pairs of directly correlated nodes, while decreasing spurious correlations associated with pairs of nodes without a direct connection (but perhaps are influenced by a common input, or are connected only indirectly). While &quot;correlation does not imply causation,&quot; intervention may decrease the gap between the two.</p>
<p>Our hypothesis is that this shaping of pairwise correlations will result in reduced false positive edges in inferred circuits, &quot;unblurring&quot; the indirect associations that would otherwise confound circuit inference. However care must be taken, as this strategy relies on a hypothesis for the ground truth adjacency and may also result in a &quot;confirmation bias&quot; as new spurious correlations can be introduced through closed-loop intervention.</p>
<p>The impact of intervention on correlations can be summarized through the co-reachability <span class="mathjax-exps">$\text{CoReach}(i,j|S_k)$</span>. A useful distillation of this mapping is to understand the sign of <span class="mathjax-exps">$\frac{dR_{ij}}{dS_k}$</span>, that is whether increasing the variance of an intervention at node <span class="mathjax-exps">$k$</span> increases or decreases the correlation between nodes <span class="mathjax-exps">$i$</span> and <span class="mathjax-exps">$j$</span></p>
<p>In a simulated network A&#x2192;B <a href="#fig-var">(fig. variance)</a> we demonstrate predicted and emprirical correlations between a pair of nodes as a function of intervention type, location, and variance. A few features are present which provide a general intuition for the impact of intervention location in larger circuits: First, interventions &quot;upstream&quot; of a true connection <a href="#fig-var">(lower left, fig. variance)</a> tend to increase the connection-related variance, and therefore strengthen the observed correlations.<br>
</p><div class="mathjax-exps">$$\text{Reach}(S_k&#x2192;i) \neq 0 \\ \text{Reach}(i&#x2192;j) \neq 0 \\ \frac{dR}{dS_k} &gt; 0$$</div><p></p>
<p>Second, interventions affecting only the downstream node <a href="#fig-var">(lower right, fig. variance)</a> of a true connection introduce variance which is independent of the connection A&#x2192;B, decreasing the observed correlation.<br>
</p><div class="mathjax-exps">$$\text{Reach}(S_k &#x2192; j) = 0 \\ \text{Reach}(S_k &#x2192; j) \neq 0 \\ \frac{dR}{dS_k} &lt; 0$$</div><p></p>
<p>Third, interventions which reach both nodes will tend to increase the observed correlations <a href="#fig-var">(upper left, fig. variance)</a>, moreover this can be achieved even if no direct connection <span class="mathjax-exps">$i&#x2192;j$</span> exists.<br>
</p><div class="mathjax-exps">$$\text{Reach}(S_k &#x2192; i) \neq 0 \\ \text{Reach}(S_k &#x2192; j) \neq 0 \\ \text{Reach}(i &#x2192; j) = 0 \\ \frac{dR}{dS_k} &gt; 0$$</div><p></p>
<p>Notably, the impact of an intervention which is a &quot;common cause&quot; for both nodes depends on the relative weighted reachability between the source and each of the nodes. Correlations induced by a common cause are maximized when the input to each node is equal, that is <span class="mathjax-exps">$\widetilde{W}_{S_k&#x2192;i} \approx \widetilde{W}_{S_k&#x2192;j}$</span> (upper right * in <a href="#fig-var">fig. variance</a>). If i&#x2192;j are connected <span class="mathjax-exps">$\widetilde{W}_{S_k&#x2192;i} \gg \widetilde{W}_{S_k&#x2192;j}$</span> results in an variance-correlation relationship similar to the &quot;upstream source&quot; case (increasing source variance increases correlation <span class="mathjax-exps">$\frac{dR}{dS_k} &gt; 0$</span>),<br>
while <span class="mathjax-exps">$\widetilde{W}_{S_k&#x2192;i} \ll \widetilde{W}_{S_k&#x2192;j}$</span> results in a relationship similar to the &quot;downstream source&quot; case (<span class="mathjax-exps">$\frac{dR}{dS_k} &lt; 0$</span>)<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup></p>
<p><a id="fig-predict" href></a></p>
<p><img src="figures/misc_figure_sketches/quant_r2_prediction_common.png" alt><br>
<img src="figures/from_code/bidirectional_correlation.png" alt title="generated by sweep_gaussian_SNR.py"></p>
<blockquote>
<p>&#x1F6A7;(Final figure will be a mix of these two panels, caption will need updating) <strong>Figure VAR: Location, variance, and type of intervention shape pairwise correlations</strong><br>
<strong>(CENTER)</strong> A two-node linear gaussian network is simulated with a connection from A&#x2192;B. Open-loop interventions <em>(blue)</em> consist of independent gaussian inputs with a range of variances <span class="mathjax-exps">$\sigma^2_S$</span>. Closed-loop interventions <em>(orange)</em> consist of feedback control with an independent gaussian target with a range of variances. <em>Incomplete closed-loop interventions result in node outputs which are a mix of the control target and network-driven activity</em>. Connections from sources to nodes are colored by their impact on correlations between A and B; green denotes <span class="mathjax-exps">$dR/dS &gt; 0$</span>, red denotes <span class="mathjax-exps">$dR/dS&lt;0$</span>.<br>
<strong>(lower left)</strong> Intervention &quot;upstream&quot; of the connection A&#x2192;B increases the correlation <span class="mathjax-exps">$r^2(A,B)$</span>.<br>
<strong>(lower right)</strong> Intervention at the terminal of the connection A&#x2192;B decreases the correlation <span class="mathjax-exps">$r^2(A,B)$</span> by adding connection-independent noise.<br>
<strong>(upper left)</strong> Intervention with shared inputs to both nodes generally increases <span class="mathjax-exps">$r^2(A,B)$</span>, <em>(even without A&#x2192;B, see supplement)</em>.<br>
<strong>(upper right)</strong> The impact of shared interventions depends on relative weighted reachability <span class="mathjax-exps">$\text{Reach}(S_k&#x2192;A) / \text{Reach}(S_k&#x2192;B)$</span>, with highest correlations when these terms are matched (see *)<br>
Closed-loop interventions <em>(orange)</em> generally result in larger changes in correlation across <span class="mathjax-exps">$\sigma^2_S$</span> than the equivalent open-loop intervention. Closed-loop control at B effectively lesions the connection A&#x2192;B, resulting in near-zero correlation.<br>
<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup></p>
</blockquote>
<details><summary>&#x21AA; additional notes:</summary>
<ul>
<li>contextualize increasing correlation is sometimes good, sometimes bad!</li>
<li>having (quantitative) prediction helps capture this relationship</li>
<li><strong>(incidental) subfigure PREDICT: Comparing predicted and empirical correlation, identification performance</strong></li>
</ul>
</details>
<p>&#x1F6A7;<br>
The change in correlation as a function of changing intervention variance (<span class="mathjax-exps">$\frac{dr^2_{ij}}{dS}$</span>) can therefore be used as an additional indicator of presence/absence and directionality of the connection between A,B <em>(see <a href="fig-disambig">fig. disambig. D.)</a>)</em><br>
&#x1F6A7;</p>
<p><a href="#fig-var">Fig. variance</a> also demonstrates the relative dynamic range of correlations achievable under passive, open- and closed-loop intervention. In the passive case, correlations are determined by instrinsic properties of the network <span class="mathjax-exps">$\sigma^2_{base}$</span>. These properties have influence over the observed correlations in a way that can be difficult to separate from differences due to the ground-truth circuit. With open-loop intervention we can observe the impact of increasing variance at a particular node, but the dynamic range of achievable correlations is bounded by not being able to reduce variance below its baseline level. With closed-loop control, the bidirectional control of the output variance for a node means a much wider range of correlations can be achieved <a href="#fig-var">(blue v.s. orange in fig. variance)</a>, resulting in a more sensitive signal reflecting the ground-truth connectivity.</p>
<p><em>see also <a href="results1B_data_efficiency_and_bias.md">results1B_data_efficiency_and_bias.md</a></em></p>
<div class="admonition todo">
<p class="admonition-title">- comaprison signs in rows of DISAMBIG figure</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- merge from &quot;box style&quot; where entrire story is in caption, to having something in body of results text</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- write &quot;explain why CL is better&quot; section, ? exile it to discussion section?</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- connect DISAMBIG caption to quantitative variance explanation section</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- collapse figvar - do we need to make shared input point here? or is discussion fine?</p>
</div>
<div class="admonition todo">
<p class="admonition-title">- dR/dS needs to mention R as r^2 corr</p>
</div>
<details><summary>&#x21AA;Notes from matt</summary>
<ul>
<li>[super minor] First part of fig DISAMBIG: subsections (A) through (C) work really well</li>
<li>[super minor] in caption for (D-F): &quot;modifications to the passive correlation pattern&quot; is a bit confusing in the context of open-loop intervention</li>
<li>[super minor] also in caption for (D-F): really like &quot;intervention-specific fingerprint&quot; terminology. The last sentence of the (D-F) caption really hits the message home, possible to emphasize that this is the take-home message earlier?</li>
<li>[narrative/organization] fig DISAMBIG feels really example-y, more like a proof of concept than &apos;results.&apos; The writing in Sec 5.1.1 also has this flavor, like it could be in a methods section. (The plot in the top right feels much more results-ey.) Not necessarily a bad thing, maybe just a consideration for thinking about article vs perspective flavor.</li>
<li>[missing] Section 5.1.2.1: what are the definitions of S_k, CoReach(i,j|S_k), and R_{ij}?</li>
<li>[narrative] Section 5.1.2.1: the narrative here really works for me, but it&apos;s a little unclear whether this is more of a &apos;result&apos; or a &apos;recipe&apos; -- the figures here also feel more example/proof-of-concept-ey, and the math here helps ground things in</li>
<li>[missing] discussion of partial closed-loop control?</li>
</ul>
</details>  
<h1 class="mume-header" id="discussion">Discussion</h1>

<h3 class="mume-header" id="limitations">limitations</h3>

<p>The examples explored in this work simplify several key features that may have relevant contributions to circuit identification in practical experiments. [...]</p>
<p><code>full observability</code></p>
<h3 class="mume-header" id="results-summary-summary-of-value-closed-loop-generally">results summary &#x2192; summary of value closed-loop generally</h3>

<p>Closed-loop control has the disadvantages of being more complex to implement and requires specialized real-time hardware and software, however it has been shown to have multifaceted usefulness in clinical and basic science applications. Here we focused on two advantages in particular; First, the capacity for functional lesioning which (reversibly) severs inputs to nodes and second, closed-loop control&apos;s capacity to precisely shape variance across nodes. Both of these advantages facilitate opportunities for closed-loop intervention to reveal more circuit structure than passive observation or even open-loop experiments.</p>
<h3 class="mume-header" id="summary-of-guidelines-for-experimenters">summary of guidelines for experimenters</h3>

<p>In studying the utility of various intervention for circuit inference we arrived at a few general guidelines which may assist experimental neuroscientists in designing the right intervention for the quesiton at hand.<br>
First, more ambiguous hypotheses sets require &quot;stronger&quot; interventions to distinguish. Open-loop intervention may be sufficient to determine directionality of functional relationships, but as larger numbers of similar hypotheses [...] closed-loop intervention reduces the hypothesis set more efficiently.<br>
Second, we find that dense networks with strong reciprocal connections tend to result in many equivalent circuit hypotheses, but that well-placed closed-loop control can disrupt loops and simplify correlation structure to be more identifiable.<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup> Recurrent loops are a common feature of neural circuit, and represent key opportunities for successful closed-loop intervention. The same is true for circuits with strong indirect correlations</p>
<p><code>hidden confounds</code></p>
<h3 class="mume-header" id="funnel-out-future-work-broad-impact">&quot;funnel out&quot;, future work &#x2192; broad impact</h3>

<p><code>sequential experimental design</code></p>
<p><em>see <a href="sketches_and_notation/discussion/limitations_future_work.md">limitations_future_work.md</a></em></p>
<h1 class="mume-header" id="references">References</h1>

<p><em>see <a href="https://github.com/shd101wyy/markdown-preview-enhanced/blob/master/docs/pandoc-bibliographies-and-citations.md">pandoc pandoc-citations</a></em></p>
<h1 class="mume-header" id="supplement">Supplement</h1>

<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>may end up discussing quantitative advantages such as bidirectional variance (and correlation) control. If that&apos;s a strong focus in the results, should be talked about more in the abstract also <a href="#fnref1" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn2" class="footnote-item"><p>These assumptions are typically on properties such as the types of functional relationships that exist in circuits, the visibility and structure of confounding relationships, and noise statistics. <a href="#fnref2" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn3" class="footnote-item"><p>if citations needed here, could start by looking for a good high-level reference in either \cite{ghassami2018budgeted} or \cite{yang2018characterizing}. (Both of these papers are pretty technical, so likely wouln&apos;t be great citations on their own.) <a href="#fnref3" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn4" class="footnote-item"><p>saying &quot;difficult to distinguish&quot; instead of &quot;indistinguishable&quot; here since the magnitudes of the correlations could also be informative with different assumptions <a href="#fnref4" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn5" class="footnote-item"><p>To see this, denote by <span class="mathjax-exps">$E \in \mathbb{R}^{p \times n}$</span> the matrix of <span class="mathjax-exps">$n$</span> private noise observations for each node. Note that <span class="mathjax-exps">$X = W^T X + E$</span>, so <span class="mathjax-exps">$X = E(I-W^T)^{-1}$</span>. The covariance matrix <span class="mathjax-exps">$\Sigma = \mathrm{cov}(X) = \mathbb{E}\left[X X^T\right]$</span> can then be written as <span class="mathjax-exps">$\Sigma = \mathbb{E}\left[ (I-W^T)^{-1} E E^T (I-W^T)^{-1} \right] = (I-W^T)^{-1} \mathrm{cov}(E) (I-W^T)^{-T} = (I-W^T)^{-1} \mathrm{diag}(s) (I-W^T)^{-T}$</span>. <a href="#fnref5" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn6" class="footnote-item"><p>We can use <span class="mathjax-exps">$p-1$</span> as an upper limit on the sum <span class="mathjax-exps">$\widetilde{W} = \sum_{k=0}^{\infty} W^k$</span> when there are no recurrent connections. <a href="#fnref6" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn7" class="footnote-item"><p>However, depending on overall firing rates and population sizes, this sparse spike-based transmission can be coarse-grained to a firing-rate-based model. <a href="#fnref7" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn8" class="footnote-item"><p>the effective <span class="mathjax-exps">$\Delta_{sample}$</span> would be broadened in the presence of jitter in connection delay, measurement noise, or temporal smoothing applied post-hoc, leading <a href="#fnref8" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn9" class="footnote-item"><p>need to triple check indexing w.r.t. nodes, neurons <a href="#fnref9" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn10" class="footnote-item"><p>need to resolve differences in implementation between contemporaneous and voltage simulation cases <a href="#fnref10" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn11" class="footnote-item"><p>NEED dynamic clamp refs - <a href="http://www.scholarpedia.org/article/Dynamic_clamp">http://www.scholarpedia.org/article/Dynamic_clamp</a> <a href="#fnref11" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn12" class="footnote-item"><p><em>inference techniques mentioned in the intro...</em> <a href="#fnref12" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn13" class="footnote-item"><p>what does &quot;prototype&quot; mean here? something like MI and corr are equivalent in the linear-Gaussian case, ... <a href="#fnref13" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn14" class="footnote-item"><p>TODO? formalize notation for this <a href="#fnref14" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn15" class="footnote-item"><p>not sure how important this is. would prefer to set this threshold at some ad-hoc value since we&apos;re sweeping other properties. But a more in-depth analysis could look at a receiver-operator curve with respect to this threshold <a href="#fnref15" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn16" class="footnote-item"><p>nodes in such a graphical model may represent populations of neurons, distinct cell-types, different regions within the brain, or components of a latent variable represented in the brain. <a href="#fnref16" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn17" class="footnote-item"><p>will change the color scheme for final figure. Likely using orange and blue to denote closed and open-loop interventions. Will also add in indication of severed edges <a href="#fnref17" class="footnote-backref">&#x21A9;&#xFE0E;</a> <a href="#fnref17:1" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn18" class="footnote-item"><p>Figure VAR shows this pretty well, perhaps sink this section until after discussing categorical and quantitative? <a href="#fnref18" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn19" class="footnote-item"><p>below the level set by added, independent/&quot;private&quot; sources <a href="#fnref19" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn20" class="footnote-item"><p>notably, this is part of the definition of open-loop intervention <a href="#fnref20" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn21" class="footnote-item"><p>practically, this requires very fast feedback to achieve fully independent control over mean and variance. In the case of firing rates, I suspect <span class="mathjax-exps">$\mu \leq \alpha\mathbb{V}$</span>, so variances can be reduced, but for very low firing rates, there&apos;s still an upper limit on what the variance can be. <a href="#fnref21" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn22" class="footnote-item"><p>need a more specific way of stating this. I mean degrees of freedom in the sense that mean and variance can be controlled independent of each other. And also, that the range of achievable correlation coefficients is wider for closed-loop than open-loop (where instrinsic variability constrains the minimum output variance) <a href="#fnref22" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn23" class="footnote-item"><p>not 100% sure this is true, the empirical results are really pointing to dR/dW&lt;0 rather than dR/dS&lt;0. Also this should really be something like <span class="mathjax-exps">$\frac{d|R|}{dS}$</span> or <span class="mathjax-exps">$\frac{dr^2}{dS}$</span> since these effects decrease the <em>magnitude</em> of correlations. I.e. if <span class="mathjax-exps">$\frac{d|R|}{dS} &lt; 0$</span> increasing <span class="mathjax-exps">$S$</span> might move <span class="mathjax-exps">$r$</span> from <span class="mathjax-exps">$-0.8$</span> to <span class="mathjax-exps">$-0.2$</span>, i.e. decrease its magnitude not its value. <a href="#fnref23" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn24" class="footnote-item"><p>compare especially to <a href="https://www.frontiersin.org/articles/10.3389/fncom.2020.00045/full">&quot;Transfer Entropy as a Measure of Brain Connectivity&quot;</a>, <a href="https://www.jneurosci.org/content/29/33/10234">&quot;How Connectivity, Background Activity, and Synaptic Properties Shape the Cross-Correlation between Spike Trains&quot;</a> Figure 3. <a href="#fnref24" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
<li id="fn25" class="footnote-item"><p>this corroborates Ila Fiete&apos;s paper on bias as a function of recurrent network strength <a href="#fnref25" class="footnote-backref">&#x21A9;&#xFE0E;</a></p>
</li>
</ol>
</section>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>